{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "567972bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback\n",
      "Files exist? train=True test=True sample=True\n"
     ]
    }
   ],
   "source": [
    "# Imports & quick checks\n",
    "import os, sys, json, math, warnings, gc, time, random\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# Try XGBoost for meta\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"xgboost not installed; meta-XGB will be skipped.\")\n",
    "\n",
    "# Try LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"lightgbm not installed; LightGBM will be skipped.\")\n",
    "\n",
    "# Try CatBoost\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    CB_AVAILABLE = False\n",
    "    print(\"catboost not installed; CatBoost will be skipped.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_BASE = 42\n",
    "np.random.seed(RANDOM_BASE)\n",
    "random.seed(RANDOM_BASE)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'Data'\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH = DATA_DIR / 'test.csv'\n",
    "SAMPLE_SUB_PATH = DATA_DIR / 'sample_submission.csv'\n",
    "SUB_DIR = ROOT / 'submissions'\n",
    "SUB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f'ROOT: {ROOT}')\n",
    "print(f'Files exist? train={TRAIN_PATH.exists()} test={TEST_PATH.exists()} sample={SAMPLE_SUB_PATH.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "980a8c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected ID_COL= id  TARGET= loan_paid_back\n",
      "(593994, 13) train shape\n",
      "(254569, 12) test shape\n",
      "(593994, 13) train shape\n",
      "(254569, 12) test shape\n"
     ]
    }
   ],
   "source": [
    "# Config & target/id detection\n",
    "TARGET_CANDIDATES = ['target','TARGET','label','Label','default','is_default','loan_status','loan_repaid']\n",
    "ID_CANDIDATES = ['id','ID','loan_id','Loan_ID']\n",
    "\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    cols = df.columns.tolist()\n",
    "    id_col = None\n",
    "    for c in ID_CANDIDATES:\n",
    "        if c in cols:\n",
    "            id_col = c\n",
    "            break\n",
    "    \n",
    "    target_col = None\n",
    "    for c in TARGET_CANDIDATES:\n",
    "        if c in cols:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        # Heuristic: last column if binary-like\n",
    "        last = cols[-1]\n",
    "        if df[last].dropna().isin([0,1]).mean() > 0.9:\n",
    "            target_col = last\n",
    "    return id_col, target_col\n",
    "\n",
    "# Peek few rows to detect columns\n",
    "preview = pd.read_csv(TRAIN_PATH, nrows=100)\n",
    "ID_COL, TARGET = detect_columns(preview)\n",
    "print('Detected ID_COL=', ID_COL, ' TARGET=', TARGET)\n",
    "assert TARGET is not None, 'Target column not detected; please set TARGET manually.'\n",
    "\n",
    "# Load full data\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH) if TEST_PATH.exists() else None\n",
    "print(train.shape, 'train shape')\n",
    "if test is not None:\n",
    "    print(test.shape, 'test shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 5 numeric, 6 categorical\n",
      "Created grade_subgrade_ordinal: 1-30 mapping\n",
      "Created is_unemployed binary flag\n",
      "Target encoded (smoothed): gender\n",
      "Target encoded (smoothed): gender\n",
      "Target encoded (smoothed): marital_status\n",
      "Target encoded (smoothed): marital_status\n",
      "Target encoded (smoothed): education_level\n",
      "Target encoded (smoothed): education_level\n",
      "Target encoded (smoothed): employment_status\n",
      "Target encoded (smoothed): employment_status\n",
      "Target encoded (smoothed): loan_purpose\n",
      "Target encoded (smoothed): loan_purpose\n",
      "Target encoded (smoothed): grade_subgrade\n",
      "Created log transform: annual_income_log\n",
      "Created log transform: loan_amount_log\n",
      "Created log transform: debt_to_income_ratio_log\n",
      "Created log_income_to_log_loan ratio\n",
      "Created risk_ratio feature (interest_rate / credit_score)\n",
      "Created pti_proxy feature ((loan_amount * interest_rate) / annual_income)\n",
      "Target encoded (smoothed): grade_subgrade\n",
      "Created log transform: annual_income_log\n",
      "Created log transform: loan_amount_log\n",
      "Created log transform: debt_to_income_ratio_log\n",
      "Created log_income_to_log_loan ratio\n",
      "Created risk_ratio feature (interest_rate / credit_score)\n",
      "Created pti_proxy feature ((loan_amount * interest_rate) / annual_income)\n",
      "Engineered: 43 numeric, 6 categorical\n",
      "Feature count: 49 (from 11)\n",
      "\n",
      "ðŸŽ¯ KNN Feature Engineering (Orthogonal Diversity)\n",
      "  Using 34 numeric features for KNN\n",
      "Engineered: 43 numeric, 6 categorical\n",
      "Feature count: 49 (from 11)\n",
      "\n",
      "ðŸŽ¯ KNN Feature Engineering (Orthogonal Diversity)\n",
      "  Using 34 numeric features for KNN\n",
      "  âœ“ Created knn_risk (avg target of 10 neighbors) and knn_dist (avg distance)\n",
      "  knn_risk range: [0.000, 1.000]\n",
      "  âœ“ Created knn_risk (avg target of 10 neighbors) and knn_dist (avg distance)\n",
      "  knn_risk range: [0.000, 1.000]\n",
      "After KNN: 45 numeric, 6 categorical\n",
      "After KNN: 45 numeric, 6 categorical\n"
     ]
    }
   ],
   "source": [
    "â‰ˆ# WINNING Feature Engineering Strategy (s5e11 reference)\n",
    "\n",
    "y = train[TARGET].astype(int)\n",
    "X = train.drop(columns=[TARGET] + ([ID_COL] if ID_COL else []))\n",
    "X_test = None\n",
    "if 'test' in globals() and test is not None:\n",
    "    X_test = test.drop(columns=[ID_COL] if ID_COL else [])\n",
    "\n",
    "num_cols_orig = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols_orig]\n",
    "\n",
    "print(f'Original: {len(num_cols_orig)} numeric, {len(cat_cols)} categorical')\n",
    "\n",
    "# 1. LOAD ORIGINAL DATASET (The Stabilizer)\n",
    "print('\\nðŸ“š Loading original reference dataset...')\n",
    "orig = None\n",
    "orig_paths = [\n",
    "    DATA_DIR / 'loan_dataset_20000.csv',\n",
    "    ROOT / 'loan_dataset_20000.csv',\n",
    "    Path('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n",
    "]\n",
    "\n",
    "for path in orig_paths:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            orig = pd.read_csv(path)\n",
    "            print(f'  âœ“ Loaded original dataset from: {path}')\n",
    "            print(f'  Shape: {orig.shape}')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'  âš ï¸  Failed to load {path}: {e}')\n",
    "\n",
    "if orig is None:\n",
    "    print('  âš ï¸  Original dataset not found. Using train-based target encoding instead.')\n",
    "    print('  Tip: Place loan_dataset_20000.csv in the Data/ folder for better stability.')\n",
    "    use_orig = False\n",
    "else:\n",
    "    use_orig = True\n",
    "    # Detect target column in original dataset\n",
    "    orig_target = None\n",
    "    for t in ['loan_paid_back', 'target', 'TARGET', 'loan_status', 'loan_repaid']:\n",
    "        if t in orig.columns:\n",
    "            orig_target = t\n",
    "            break\n",
    "    if orig_target is None:\n",
    "        print('  âš ï¸  Target column not found in original dataset')\n",
    "        use_orig = False\n",
    "    else:\n",
    "        print(f'  âœ“ Using target column: {orig_target}')\n",
    "\n",
    "# 0. ORDINAL ENCODING for grade_subgrade (explicit rank mapping)\n",
    "GRADE_SUBGRADE_MAP = {}\n",
    "grades = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "rank = 1\n",
    "for grade in grades:\n",
    "    for subgrade in range(1, 6):\n",
    "        GRADE_SUBGRADE_MAP[f'{grade}{subgrade}'] = rank\n",
    "        rank += 1\n",
    "\n",
    "if 'grade_subgrade' in X.columns:\n",
    "    X['grade_subgrade_ordinal'] = X['grade_subgrade'].map(GRADE_SUBGRADE_MAP).fillna(0).astype(int)\n",
    "    if X_test is not None and 'grade_subgrade' in X_test.columns:\n",
    "        X_test['grade_subgrade_ordinal'] = X_test['grade_subgrade'].map(GRADE_SUBGRADE_MAP).fillna(0).astype(int)\n",
    "    print(f'Created grade_subgrade_ordinal: 1-30 mapping')\n",
    "\n",
    "# 0b. Binary flag for unemployment\n",
    "if 'employment_status' in X.columns:\n",
    "    X['is_unemployed'] = (X['employment_status'] == 'Unemployed').astype(int)\n",
    "    if X_test is not None and 'employment_status' in X_test.columns:\n",
    "        X_test['is_unemployed'] = (X_test['employment_status'] == 'Unemployed').astype(int)\n",
    "    print(f'Created is_unemployed binary flag')\n",
    "\n",
    "# 1. TARGET ENCODING for categorical features (10-fold CV to prevent leakage) with smoothing\n",
    "from sklearn.model_selection import KFold\n",
    "TARGET_ENCODED = {}\n",
    "\n",
    "global_mean = y.mean()\n",
    "smoothing_k = 50  # smoothing strength per instructions\n",
    "\n",
    "for cat in cat_cols[:]:  # Encode all categoricals\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    X[f'{cat}_target_enc'] = 0.0\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        # Compute smoothed stats on training fold only\n",
    "        stats_fold = train.iloc[tr_idx].groupby(cat)[TARGET].agg(['mean', 'count'])\n",
    "        stats_fold['smooth'] = (stats_fold['mean'] * stats_fold['count'] + global_mean * smoothing_k) / (stats_fold['count'] + smoothing_k)\n",
    "        mapping_fold = stats_fold['smooth']\n",
    "        X.loc[X.index[va_idx], f'{cat}_target_enc'] = X.iloc[va_idx][cat].map(mapping_fold).fillna(global_mean)\n",
    "    # For test, use full train stats (smoothed)\n",
    "    if X_test is not None:\n",
    "        stats_full = train.groupby(cat)[TARGET].agg(['mean', 'count'])\n",
    "        stats_full['smooth'] = (stats_full['mean'] * stats_full['count'] + global_mean * smoothing_k) / (stats_full['count'] + smoothing_k)\n",
    "        mapping_full = stats_full['smooth']\n",
    "        X_test[f'{cat}_target_enc'] = X_test[cat].map(mapping_full).fillna(global_mean)\n",
    "    TARGET_ENCODED[cat] = f'{cat}_target_enc'\n",
    "    print(f'Target encoded (smoothed): {cat}')\n",
    "\n",
    "# 2. LOG TRANSFORMS for skewed features (address heavy skew)\n",
    "LOG_COLS = ['annual_income', 'loan_amount', 'debt_to_income_ratio']\n",
    "for col in LOG_COLS:\n",
    "    if col in num_cols_orig:\n",
    "        X[f'{col}_log'] = np.log1p(X[col].clip(lower=0))\n",
    "        if X_test is not None:\n",
    "            X_test[f'{col}_log'] = np.log1p(X_test[col].clip(lower=0))\n",
    "        print(f'Created log transform: {col}_log')\n",
    "\n",
    "# 2b. Log income to log loan ratio\n",
    "if 'annual_income' in num_cols_orig and 'loan_amount' in num_cols_orig:\n",
    "    X['log_income_to_log_loan'] = np.log1p(X['annual_income'].clip(lower=0)) / (np.log1p(X['loan_amount'].clip(lower=0)) + 1e-6)\n",
    "    if X_test is not None:\n",
    "        X_test['log_income_to_log_loan'] = np.log1p(X_test['annual_income'].clip(lower=0)) / (np.log1p(X_test['loan_amount'].clip(lower=0)) + 1e-6)\n",
    "    print('Created log_income_to_log_loan ratio')\n",
    "\n",
    "# 3. INTERACTION FEATURES (ratios + products + polynomials)\n",
    "IMPORTANT_PAIRS = [\n",
    "    ('loan_amount', 'annual_income'),\n",
    "    ('loan_amount', 'credit_score'),\n",
    "    ('debt_to_income_ratio', 'credit_score'),\n",
    "    ('annual_income', 'credit_score'),\n",
    "    ('interest_rate', 'loan_amount'),\n",
    "]\n",
    "\n",
    "for c1, c2 in IMPORTANT_PAIRS:\n",
    "    if c1 in num_cols_orig and c2 in num_cols_orig:\n",
    "        # Ratio\n",
    "        X[f'{c1}_div_{c2}'] = X[c1] / (X[c2] + 1e-6)\n",
    "        if X_test is not None:\n",
    "            X_test[f'{c1}_div_{c2}'] = X_test[c1] / (X_test[c2] + 1e-6)\n",
    "        # Product\n",
    "        X[f'{c1}_x_{c2}'] = X[c1] * X[c2]\n",
    "        if X_test is not None:\n",
    "            X_test[f'{c1}_x_{c2}'] = X_test[c1] * X_test[c2]\n",
    "        # Difference\n",
    "        X[f'{c1}_minus_{c2}'] = X[c1] - X[c2]\n",
    "        if X_test is not None:\n",
    "            X_test[f'{c1}_minus_{c2}'] = X_test[c1] - X_test[c2]\n",
    "\n",
    "# Risk Ratio: Interest Rate relative to Credit Score (High Rate + Low Score = Extreme Risk)\n",
    "if 'interest_rate' in num_cols_orig and 'credit_score' in num_cols_orig:\n",
    "    X['risk_ratio'] = X['interest_rate'] / (X['credit_score'] + 1e-6)\n",
    "    if X_test is not None:\n",
    "        X_test['risk_ratio'] = X_test['interest_rate'] / (X_test['credit_score'] + 1e-6)\n",
    "    print('Created risk_ratio feature (interest_rate / credit_score)')\n",
    "\n",
    "# Estimated Monthly Payment / Monthly Income (Payment-to-Income proxy)\n",
    "if 'loan_amount' in num_cols_orig and 'interest_rate' in num_cols_orig and 'annual_income' in num_cols_orig:\n",
    "    X['pti_proxy'] = (X['loan_amount'] * X['interest_rate']) / (X['annual_income'] + 1)\n",
    "    if X_test is not None:\n",
    "        X_test['pti_proxy'] = (X_test['loan_amount'] * X_test['interest_rate']) / (X_test['annual_income'] + 1)\n",
    "    print('Created pti_proxy feature ((loan_amount * interest_rate) / annual_income)')\n",
    "\n",
    "# 4. POLYNOMIAL FEATURES (square key predictors)\n",
    "for col in ['credit_score', 'annual_income', 'loan_amount'][:]:\n",
    "    if col in num_cols_orig:\n",
    "        X[f'{col}_squared'] = X[col] ** 2\n",
    "        X[f'{col}_sqrt'] = np.sqrt(X[col].clip(lower=0))\n",
    "        if X_test is not None:\n",
    "            X_test[f'{col}_squared'] = X_test[col] ** 2\n",
    "            X_test[f'{col}_sqrt'] = np.sqrt(X_test[col].clip(lower=0))\n",
    "\n",
    "# 5. BINNING FEATURES (discretize continuous)\n",
    "for col in ['credit_score', 'annual_income', 'loan_amount'][:]:\n",
    "    if col in num_cols_orig:\n",
    "        X[f'{col}_bin'] = pd.qcut(X[col], q=10, labels=False, duplicates='drop')\n",
    "        if X_test is not None:\n",
    "            # Use train quantiles for test\n",
    "            quantiles = X[col].quantile(np.linspace(0, 1, 11)).unique()\n",
    "            X_test[f'{col}_bin'] = pd.cut(X_test[col], bins=quantiles, labels=False, include_lowest=True).fillna(5)\n",
    "\n",
    "num_cols = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "print(f'Engineered: {len(num_cols)} numeric, {len(cat_cols)} categorical')\n",
    "print(f'Feature count: {X.shape[1]} (from {len(num_cols_orig) + len(cat_cols)})')\n",
    "\n",
    "# Global fill for Sklearn models\n",
    "X = X.fillna(0)\n",
    "if X_test is not None:\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "# 6. KNN INJECTION: Local manifold structure (Magic Feature for loan prediction)\n",
    "print('\\nðŸŽ¯ KNN Feature Engineering (Orthogonal Diversity)')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Select numeric features for KNN (exclude categorical and target-encoded)\n",
    "knn_feature_cols = [c for c in num_cols if '_target_enc' not in c and '_bin' not in c]\n",
    "print(f'  Using {len(knn_feature_cols)} numeric features for KNN')\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_knn_scaled = scaler.fit_transform(X[knn_feature_cols])\n",
    "if X_test is not None:\n",
    "    X_test_knn_scaled = scaler.transform(X_test[knn_feature_cols])\n",
    "\n",
    "# Fit KNN on training data\n",
    "knn = NearestNeighbors(n_neighbors=11, n_jobs=-1)  # 11 to exclude self\n",
    "knn.fit(X_knn_scaled)\n",
    "\n",
    "# Get neighbors and distances for train\n",
    "distances_train, indices_train = knn.kneighbors(X_knn_scaled)\n",
    "# Exclude self (first neighbor) and take mean of remaining 10\n",
    "knn_risk_train = np.array([y.iloc[indices_train[i, 1:]].mean() for i in range(len(indices_train))])\n",
    "X['knn_risk'] = knn_risk_train\n",
    "X['knn_dist'] = distances_train[:, 1:].mean(axis=1)\n",
    "\n",
    "if X_test is not None:\n",
    "    # Get neighbors and distances for test\n",
    "    distances_test, indices_test = knn.kneighbors(X_test_knn_scaled)\n",
    "    # For test, use all 10 neighbors (no self to exclude)\n",
    "    knn_risk_test = np.array([y.iloc[indices_test[i, :10]].mean() for i in range(len(indices_test))])\n",
    "    X_test['knn_risk'] = knn_risk_test\n",
    "    X_test['knn_dist'] = distances_test[:, :10].mean(axis=1)\n",
    "\n",
    "print(f'  âœ“ Created knn_risk (avg target of 10 neighbors) and knn_dist (avg distance)')\n",
    "print(f'  knn_risk range: [{X[\"knn_risk\"].min():.3f}, {X[\"knn_risk\"].max():.3f}]')\n",
    "\n",
    "# Update numeric columns list\n",
    "num_cols = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "print(f'After KNN: {len(num_cols)} numeric, {len(cat_cols)} categorical')\n",
    "\n",
    "# Simplified preprocessing\n",
    "numeric_tf = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_tf, num_cols),\n",
    "    ('cat', categorical_tf, cat_cols)\n",
    "])\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == 'xgb' and XGB_AVAILABLE:\n",
    "        return 'xgb_raw'\n",
    "    elif name == 'lgb' and LGB_AVAILABLE:\n",
    "        return 'lgb_raw'\n",
    "    elif name == 'cb' and CB_AVAILABLE:\n",
    "        return 'cb_raw'\n",
    "    else:\n",
    "        raise ValueError(f'Model {name} not available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8dbee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV, metrics, threshold sweep, and isotonic calibration utils\n",
    "def get_cv(n_splits=5, seed=42):\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "def threshold_sweep(y_true, prob, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best = {'threshold': None, 'f1': -1, 'precision': None, 'recall': None}\n",
    "    for t in thresholds:\n",
    "        pred = (prob >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred)\n",
    "        if f1 > best['f1']:\n",
    "            # compute precision & recall via confusion matrix\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec = tp / (tp + fn + 1e-9)\n",
    "            best = {'threshold': float(t), 'f1': float(f1), 'precision': float(prec), 'recall': float(rec)}\n",
    "    return best\n",
    "\n",
    "def fit_isotonic(y_true, prob):\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(prob, y_true)\n",
    "    return iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "476f51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress tracking ready âœ“\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Š Performance tracking utilities\n",
    "\n",
    "def print_progress_bar(current, target=0.93, width=50):\n",
    "    \"\"\"Visual progress bar toward 93% AUC\"\"\"\n",
    "    min_val = 0.90\n",
    "    max_val = 0.94\n",
    "    progress = (current - min_val) / (max_val - min_val)\n",
    "    filled = int(width * progress)\n",
    "    bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)\n",
    "    pct = current * 100\n",
    "    target_pct = target * 100\n",
    "    \n",
    "    print(f'\\nðŸ“Š Progress to {target_pct:.1f}%:')\n",
    "    print(f'[{bar}] {pct:.3f}%')\n",
    "    \n",
    "    if current >= target:\n",
    "        print('ðŸŽ‰ TARGET ACHIEVED! ðŸŽ‰')\n",
    "    else:\n",
    "        gap = (target - current) * 100\n",
    "        print(f'Gap: {gap:.3f} pp')\n",
    "\n",
    "def compare_techniques(base_auc, l2_auc, l3_auc, cal_auc):\n",
    "    \"\"\"Show incremental gains from each technique\"\"\"\n",
    "    print(f'\\nðŸ“ˆ TECHNIQUE BREAKDOWN:')\n",
    "    print(f'   Base (L1):       {base_auc:.5f}')\n",
    "    print(f'   + L2 Meta:       {l2_auc:.5f}  (+{(l2_auc-base_auc)*100:.3f} pp)')\n",
    "    print(f'   + L3 Pseudo:     {l3_auc:.5f}  (+{(l3_auc-l2_auc)*100:.3f} pp)')\n",
    "    print(f'   + Calibration:   {cal_auc:.5f}  (+{(cal_auc-l3_auc)*100:.3f} pp)')\n",
    "    print(f'   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "    print(f'   TOTAL GAIN:      +{(cal_auc-base_auc)*100:.3f} pp')\n",
    "    \n",
    "print('Progress tracking ready âœ“')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c434c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Base Models: EXTREME tuning for 93%+ (3 diverse boosters + MLP for orthogonal diversity)\n",
    "\n",
    "def train_base_models(X, y, X_test=None, seed=42, n_splits=5):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    cv = get_cv(n_splits=n_splits, seed=seed)\n",
    "    \n",
    "    # Prepare scaled data for MLP (neural nets need normalized inputs)\n",
    "    num_cols_for_mlp = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "    scaler_mlp = StandardScaler()\n",
    "    X_scaled = scaler_mlp.fit_transform(X[num_cols_for_mlp])\n",
    "    X_test_scaled = None\n",
    "    if X_test is not None:\n",
    "        X_test_scaled = scaler_mlp.transform(X_test[num_cols_for_mlp])\n",
    "    \n",
    "    # Use ALL available gradient boosters\n",
    "    base_models_config = []\n",
    "    \n",
    "    if XGB_AVAILABLE:\n",
    "        base_models_config.append(('xgb', {\n",
    "            'n_estimators': 1500, 'learning_rate': 0.015, 'max_depth': 8,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "            'reg_lambda': 3.0, 'reg_alpha': 0.8, 'min_child_weight': 2,\n",
    "            'tree_method': 'hist', 'n_jobs': -1,\n",
    "            'enable_categorical': True,\n",
    "        }))\n",
    "    \n",
    "    if LGB_AVAILABLE:\n",
    "        base_models_config.append(('lgb', {\n",
    "            'n_estimators': 1500, 'learning_rate': 0.015, 'max_depth': 10, 'num_leaves': 255,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "            'reg_lambda': 3.0, 'reg_alpha': 0.6, 'min_child_samples': 20,\n",
    "            'verbose': -1, 'n_jobs': -1, 'force_col_wise': True\n",
    "        }))\n",
    "    \n",
    "    if CB_AVAILABLE:\n",
    "        base_models_config.append(('cb', {\n",
    "            'iterations': 1500, 'learning_rate': 0.015, 'depth': 8,\n",
    "            'l2_leaf_reg': 5, 'border_count': 254, 'min_data_in_leaf': 10,\n",
    "            'verbose': 0, 'thread_count': -1\n",
    "        }))\n",
    "\n",
    "    # Add Random Forest (Bagging) for diversity\n",
    "    base_models_config.append(('rf', {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_leaf': 5,\n",
    "        'max_features': 'sqrt',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': seed\n",
    "    }))\n",
    "    \n",
    "    # Add MLP (Neural Network) for ORTHOGONAL DIVERSITY (smooth decision boundary)\n",
    "    base_models_config.append(('mlp', {\n",
    "        'hidden_layer_sizes': (256, 128),\n",
    "        'early_stopping': True,\n",
    "        'max_iter': 300,\n",
    "        'random_state': seed,\n",
    "        'verbose': False\n",
    "    }))\n",
    "    \n",
    "    if not base_models_config:\n",
    "        raise ValueError('No gradient boosters available! Install XGBoost, LightGBM, or CatBoost.')\n",
    "    \n",
    "    base_names = [name for name, _ in base_models_config]\n",
    "    print(f'ðŸš€ Training {len(base_names)} L1 base models: {base_names}')\n",
    "    \n",
    "    oof = np.zeros((len(X), len(base_names)))\n",
    "    test_preds = np.zeros((len(X_test), len(base_names))) if X_test is not None else None\n",
    "    aucs = {name: [] for name in base_names}\n",
    "\n",
    "    for j, (name, params) in enumerate(base_models_config):\n",
    "        fold_idx = 0\n",
    "        for tr_idx, va_idx in cv.split(X, y):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            if name == 'xgb':\n",
    "                # Cast object/bin columns to categorical for XGBoost\n",
    "                cat_features_xgb = [c for c in X.columns if X[c].dtype == 'object' or '_bin' in c]\n",
    "                X_tr_local = X_tr.copy()\n",
    "                X_va_local = X_va.copy()\n",
    "                for c in cat_features_xgb:\n",
    "                    if c in X_tr_local.columns:\n",
    "                        X_tr_local[c] = X_tr_local[c].astype('category')\n",
    "                    if c in X_va_local.columns:\n",
    "                        X_va_local[c] = X_va_local[c].astype('category')\n",
    "                model = xgb.XGBClassifier(random_state=seed+fold_idx, **params)\n",
    "                model.fit(X_tr_local, y_tr)\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "            elif name == 'lgb':\n",
    "                # LightGBM requires category dtype for categorical features (not object)\n",
    "                cat_obj_cols = [c for c in X.columns if X[c].dtype == 'object']\n",
    "                X_tr_local = X_tr.copy()\n",
    "                X_va_local = X_va.copy()\n",
    "                for c in cat_obj_cols:\n",
    "                    if c in X_tr_local.columns:\n",
    "                        X_tr_local[c] = X_tr_local[c].astype('category')\n",
    "                    if c in X_va_local.columns:\n",
    "                        X_va_local[c] = X_va_local[c].astype('category')\n",
    "                model = lgb.LGBMClassifier(random_state=seed+fold_idx, **params)\n",
    "                model.fit(X_tr_local, y_tr, categorical_feature=cat_obj_cols if cat_obj_cols else 'auto')\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "            elif name == 'cb':\n",
    "                # CatBoost requires str/int categorical features (not float/numeric bins)\n",
    "                cat_features_cb = [c for c in X.columns if X[c].dtype == 'object' or '_bin' in c]\n",
    "                X_tr_local = X_tr.copy()\n",
    "                X_va_local = X_va.copy()\n",
    "                # Convert bin columns (float) to string for CatBoost\n",
    "                for c in cat_features_cb:\n",
    "                    if '_bin' in c and c in X_tr_local.columns:\n",
    "                        X_tr_local[c] = X_tr_local[c].fillna(-1).astype(int).astype(str)\n",
    "                    if '_bin' in c and c in X_va_local.columns:\n",
    "                        X_va_local[c] = X_va_local[c].fillna(-1).astype(int).astype(str)\n",
    "                model = cb.CatBoostClassifier(\n",
    "                    random_seed=seed+fold_idx, \n",
    "                    cat_features=cat_features_cb if cat_features_cb else None,\n",
    "                    **params\n",
    "                )\n",
    "                model.fit(X_tr_local, y_tr)\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "            elif name == 'rf':\n",
    "                # RandomForest on numeric features only\n",
    "                num_cols_rf = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "                X_tr_local = X_tr[num_cols_rf]\n",
    "                X_va_local = X_va[num_cols_rf]\n",
    "                model = RandomForestClassifier(**params)\n",
    "                model.fit(X_tr_local, y_tr)\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "            elif name == 'mlp':\n",
    "                # MLP on SCALED numeric features (critical for neural networks)\n",
    "                X_tr_local = X_scaled[tr_idx]\n",
    "                X_va_local = X_scaled[va_idx]\n",
    "                model = MLPClassifier(**params)\n",
    "                model.fit(X_tr_local, y_tr)\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "            \n",
    "            oof[va_idx, j] = p\n",
    "            auc = roc_auc_score(y_va, p)\n",
    "            aucs[name].append(auc)\n",
    "            \n",
    "            if X_test is not None:\n",
    "                if name == 'xgb':\n",
    "                    X_test_local = X_test.copy()\n",
    "                    for c in cat_features_xgb:\n",
    "                        if c in X_test_local.columns:\n",
    "                            X_test_local[c] = X_test_local[c].astype('category')\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / cv.get_n_splits()\n",
    "                elif name == 'lgb':\n",
    "                    X_test_local = X_test.copy()\n",
    "                    for c in cat_obj_cols:\n",
    "                        if c in X_test_local.columns:\n",
    "                            X_test_local[c] = X_test_local[c].astype('category')\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / cv.get_n_splits()\n",
    "                elif name == 'cb':\n",
    "                    X_test_local = X_test.copy()\n",
    "                    for c in cat_features_cb:\n",
    "                        if '_bin' in c and c in X_test_local.columns:\n",
    "                            X_test_local[c] = X_test_local[c].fillna(-1).astype(int).astype(str)\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / cv.get_n_splits()\n",
    "                elif name == 'rf':\n",
    "                    num_cols_rf = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "                    X_test_local = X_test[num_cols_rf]\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / cv.get_n_splits()\n",
    "                elif name == 'mlp':\n",
    "                    # MLP on scaled test data\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_scaled)[:,1] / cv.get_n_splits()\n",
    "            fold_idx += 1\n",
    "        \n",
    "        mean_auc = np.mean(aucs[name])\n",
    "        print(f\"  âœ“ {name}: {np.round(aucs[name], 5)} â†’ mean {mean_auc:.5f}\")\n",
    "    \n",
    "    return oof, test_preds, aucs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45f7657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage & drift utilities ready.\n"
     ]
    }
   ],
   "source": [
    "# Leakage audit utilities\n",
    "# 1. Single-feature AUC to flag suspicious leak features.\n",
    "# 2. Temporal leakage heuristic: columns that look like aggregates (e.g., total_*, avg_*) might encode future info.\n",
    "# 3. Type casting helpers.\n",
    "\n",
    "import re\n",
    "\n",
    "LEAK_MAX_FEATURES = 40  # cap evaluation for speed\n",
    "\n",
    "def single_feature_auc_scan(df: pd.DataFrame, y: pd.Series, max_features=LEAK_MAX_FEATURES):\n",
    "    aucs = []\n",
    "    for col in df.columns[:max_features]:\n",
    "        try:\n",
    "            if df[col].nunique() < 2:\n",
    "                continue\n",
    "            vals = df[col].fillna(df[col].median() if df[col].dtype != 'O' else 'missing')\n",
    "            # For categorical -> encode label frequency\n",
    "            if vals.dtype == 'O':\n",
    "                mapping = vals.value_counts(normalize=True).to_dict()\n",
    "                enc = vals.map(mapping).astype(float)\n",
    "            else:\n",
    "                enc = vals.astype(float)\n",
    "            score = roc_auc_score(y, enc) if len(np.unique(enc)) > 1 else 0.5\n",
    "            aucs.append((col, score))\n",
    "        except Exception:\n",
    "            continue\n",
    "    aucs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return aucs\n",
    "\n",
    "AGG_PATTERNS = [r'^total_', r'^sum_', r'^avg_', r'^mean_', r'^max_', r'^min_']\n",
    "\n",
    "def looks_leaky(colname: str) -> bool:\n",
    "    for pat in AGG_PATTERNS:\n",
    "        if re.search(pat, colname):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# KS & PSI drift checks between train/test\n",
    "\n",
    "def ks_stat(train_col, test_col):\n",
    "    # dropna\n",
    "    a = pd.Series(train_col).dropna()\n",
    "    b = pd.Series(test_col).dropna()\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        stat, pval = stats.ks_2samp(a, b)\n",
    "        return stat\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# Population Stability Index for binned values\n",
    "\n",
    "def psi(train_col, test_col, buckets=10):\n",
    "    a = pd.Series(train_col).dropna()\n",
    "    b = pd.Series(test_col).dropna()\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return 0.0\n",
    "    quantiles = np.linspace(0, 1, buckets + 1)\n",
    "    cuts = a.quantile(quantiles).unique()\n",
    "    a_bins = pd.cut(a, bins=np.unique(cuts), include_lowest=True)\n",
    "    b_bins = pd.cut(b, bins=np.unique(cuts), include_lowest=True)\n",
    "    a_dist = a_bins.value_counts(normalize=True)\n",
    "    b_dist = b_bins.value_counts(normalize=True)\n",
    "    psi_val = 0.0\n",
    "    for idx in a_dist.index:\n",
    "        expected = a_dist.get(idx, 1e-6)\n",
    "        actual = b_dist.get(idx, 1e-6)\n",
    "        if expected > 0 and actual > 0:\n",
    "            psi_val += (actual - expected) * math.log(actual / expected)\n",
    "    return psi_val\n",
    "\n",
    "DRIFT_REPORT_LIMIT = 40\n",
    "\n",
    "def drift_report(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    rows = []\n",
    "    shared = [c for c in train_df.columns if c in test_df.columns]\n",
    "    for col in shared[:DRIFT_REPORT_LIMIT]:\n",
    "        try:\n",
    "            k = ks_stat(train_df[col], test_df[col])\n",
    "            p = psi(train_df[col], test_df[col])\n",
    "            rows.append({'feature': col, 'ks': k, 'psi': p})\n",
    "        except Exception:\n",
    "            continue\n",
    "    rep = pd.DataFrame(rows)\n",
    "    if not rep.empty:\n",
    "        rep.sort_values(['ks','psi'], ascending=False, inplace=True)\n",
    "    return rep\n",
    "\n",
    "BOOL_LIKE = ['y','n','yes','no','true','false']\n",
    "\n",
    "def cast_types(df: pd.DataFrame):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            # bool-like\n",
    "            low = df[c].str.lower()\n",
    "            if low.isin(BOOL_LIKE).mean() > 0.9:\n",
    "                df[c] = low.map({'y':1,'yes':1,'true':1,'n':0,'no':0,'false':0}).astype('Int8')\n",
    "    return df\n",
    "\n",
    "print('Leakage & drift utilities ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8264e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No leakage features flagged by simple scan.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>ks</th>\n",
       "      <th>psi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interest_rate</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debt_to_income_ratio</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annual_income</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit_score</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loan_amount</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature        ks       psi\n",
       "4         interest_rate  0.002596  0.000062\n",
       "1  debt_to_income_ratio  0.002063  0.000047\n",
       "0         annual_income  0.001902  0.000039\n",
       "2          credit_score  0.001877  0.000026\n",
       "3           loan_amount  0.001703  0.000044"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing audits complete.\n"
     ]
    }
   ],
   "source": [
    "# Apply type casting, leakage audit, and drift checks\n",
    "# Must run after data load (Cell 3)\n",
    "\n",
    "assert 'train' in globals(), 'Run the data load cell first.'\n",
    "\n",
    "# 1) Type casting\n",
    "if 'ID_COL' in globals() and ID_COL:\n",
    "    train[ID_COL] = train[ID_COL].astype(str)\n",
    "    if 'test' in globals() and test is not None and ID_COL in test.columns:\n",
    "        test[ID_COL] = test[ID_COL].astype(str)\n",
    "\n",
    "train = cast_types(train)\n",
    "if 'test' in globals() and test is not None:\n",
    "    test = cast_types(test)\n",
    "\n",
    "# 2) Leakage audit (simple, top-N features)\n",
    "feat_cols = [c for c in train.columns if c not in [TARGET] + ([ID_COL] if ID_COL else [])]\n",
    "scan_df = train[feat_cols].copy()\n",
    "scan_aucs = single_feature_auc_scan(scan_df, train[TARGET], max_features=min(LEAK_MAX_FEATURES, len(feat_cols)))\n",
    "leaky = [c for (c, auc) in scan_aucs if auc >= 0.92 or auc <= 0.08 or looks_leaky(c)]\n",
    "\n",
    "if len(leaky) > 0:\n",
    "    print('Dropping suspicious leakage features:', leaky)\n",
    "    train.drop(columns=[c for c in leaky if c in train.columns], inplace=True)\n",
    "    if 'test' in globals() and test is not None:\n",
    "        test.drop(columns=[c for c in leaky if c in test.columns], inplace=True)\n",
    "else:\n",
    "    print('No leakage features flagged by simple scan.')\n",
    "\n",
    "# 3) Drift check (requires test)\n",
    "if 'test' in globals() and test is not None:\n",
    "    tr_common = train.drop(columns=[TARGET] + ([ID_COL] if ID_COL else []), errors='ignore')\n",
    "    te_common = test.drop(columns=[ID_COL] if ID_COL else [], errors='ignore')\n",
    "    rep = drift_report(tr_common, te_common)\n",
    "    display(rep.head(12))\n",
    "    # Drop worst offenders by relaxed rule\n",
    "    drop_drift = rep[(rep['ks'] >= 0.3) | (rep['psi'] >= 0.3)]['feature'].tolist()\n",
    "    if drop_drift:\n",
    "        print('Dropping drift-heavy features:', drop_drift)\n",
    "        train.drop(columns=[c for c in drop_drift if c in train.columns], inplace=True)\n",
    "        test.drop(columns=[c for c in drop_drift if c in test.columns], inplace=True)\n",
    "else:\n",
    "    print('Test set not available; skipping drift check.')\n",
    "\n",
    "print('Preprocessing audits complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24602756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Meta: HILL CLIMBING optimizer for optimal ensemble weights (replaces XGBoost)\n",
    "\n",
    "def train_meta_l2(oof_feats, y, test_feats=None, seed=42, X_orig=None, X_test_orig=None):\n",
    "    \"\"\"L2 Hill Climbing Meta-Learner: Find optimal weights to maximize AUC\"\"\"\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    print(f'  ðŸ“Š L2 Hill Climbing on {oof_feats.shape[1]} base model predictions')\n",
    "    \n",
    "    # Objective function: Minimize negative AUC (to maximize AUC)\n",
    "    def objective(weights):\n",
    "        # Weighted average of base model predictions\n",
    "        weighted_pred = np.dot(oof_feats, weights)\n",
    "        return -roc_auc_score(y, weighted_pred)\n",
    "    \n",
    "    # Constraints: weights must sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    \n",
    "    # Bounds: all weights must be non-negative\n",
    "    bounds = [(0, 1) for _ in range(oof_feats.shape[1])]\n",
    "    \n",
    "    # Initial guess: equal weights\n",
    "    initial_weights = np.ones(oof_feats.shape[1]) / oof_feats.shape[1]\n",
    "    \n",
    "    # Run optimization\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_weights,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'maxiter': 1000, 'ftol': 1e-9}\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        optimal_weights = result.x\n",
    "        oof_meta = np.dot(oof_feats, optimal_weights)\n",
    "        meta_auc = roc_auc_score(y, oof_meta)\n",
    "        \n",
    "        print(f'  âœ“ Hill Climbing optimized weights: {np.round(optimal_weights, 4)}')\n",
    "        print(f'  âœ“ L2 Hill Climbing Meta AUC: {meta_auc:.5f}')\n",
    "        \n",
    "        # Apply same weights to test predictions\n",
    "        test_meta = None\n",
    "        if test_feats is not None:\n",
    "            test_meta = np.dot(test_feats, optimal_weights)\n",
    "        \n",
    "        return oof_meta, test_meta\n",
    "    else:\n",
    "        print(f'  âš ï¸  Optimization failed, using equal weights')\n",
    "        # Fallback to equal weights\n",
    "        oof_meta = oof_feats.mean(axis=1)\n",
    "        test_meta = test_feats.mean(axis=1) if test_feats is not None else None\n",
    "        meta_auc = roc_auc_score(y, oof_meta)\n",
    "        print(f'  âœ“ L2 Equal Weights Meta AUC: {meta_auc:.5f}')\n",
    "        return oof_meta, test_meta\n",
    "\n",
    "\n",
    "def train_meta_l3_with_pseudo(oof_l2, y, test_l2, X, X_test, seed=42):\n",
    "    \"\"\"L3 Meta + PSEUDO-LABELING for final push to 93%+\"\"\"\n",
    "    \n",
    "    # PSEUDO-LABELING: Use high-confidence test predictions\n",
    "    if test_l2 is not None and X_test is not None:\n",
    "        # Select high-confidence test samples (â‰¥0.90 or â‰¤0.10)\n",
    "        high_conf_mask = (test_l2 > 0.90) | (test_l2 < 0.10)\n",
    "        pseudo_labels = (test_l2 > 0.5).astype(int)\n",
    "        \n",
    "        n_pseudo = high_conf_mask.sum()\n",
    "        if n_pseudo > 0:\n",
    "            print(f'  ðŸŽ­ Pseudo-labeling: {n_pseudo} high-confidence test samples')\n",
    "            \n",
    "            # Combine train + pseudo-labeled test\n",
    "            X_combined = pd.concat([X, X_test.iloc[high_conf_mask]], axis=0, ignore_index=True)\n",
    "            y_combined = pd.concat([y, pd.Series(pseudo_labels[high_conf_mask])], axis=0, ignore_index=True)\n",
    "            oof_combined = np.concatenate([oof_l2, test_l2[high_conf_mask]])\n",
    "            \n",
    "            # Retrain L3 on combined data\n",
    "            cv = get_cv(n_splits=5, seed=seed)\n",
    "            oof_l3 = np.zeros(len(oof_combined))\n",
    "            \n",
    "            for fold, (tr_idx, va_idx) in enumerate(cv.split(oof_combined, y_combined)):\n",
    "                X_tr, X_va = oof_combined[tr_idx].reshape(-1, 1), oof_combined[va_idx].reshape(-1, 1)\n",
    "                y_tr, y_va = y_combined.iloc[tr_idx], y_combined.iloc[va_idx]\n",
    "                \n",
    "                clf = LogisticRegression(max_iter=5000, C=0.5)\n",
    "                clf.fit(X_tr, y_tr)\n",
    "                oof_l3[va_idx] = clf.predict_proba(X_va)[:,1]\n",
    "            \n",
    "            # Extract only original train predictions\n",
    "            oof_l3_train = oof_l3[:len(y)]\n",
    "            auc_l3 = roc_auc_score(y, oof_l3_train)\n",
    "            print(f'  âœ“ L3 Meta + Pseudo AUC: {auc_l3:.5f}')\n",
    "            \n",
    "            return oof_l3_train\n",
    "    \n",
    "    # Fallback: simple L3 without pseudo-labeling\n",
    "    return oof_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ffafe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training orchestrator: EXTREME pipeline for 93%+\n",
    "\n",
    "def run_training_extreme(seeds=[42, 43], target_auc=0.93):\n",
    "    \"\"\"\n",
    "    Multi-level stacking + pseudo-labeling pipeline\n",
    "    L1 (base) â†’ L2 (fast linear) â†’ L3 (pseudo) â†’ Calibration\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    best = None\n",
    "    \n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n{'='*70}\\nðŸš€ SEED {seed} ({i+1}/{len(seeds)}) â€” Targeting 93%+ AUC\\n{'='*70}\")\n",
    "        \n",
    "        # L1: Base models (XGB + LGB + CB)\n",
    "        print('\\n[L1] Training base models...')\n",
    "        oof_l1, test_l1, base_aucs = train_base_models(X, y, X_test, seed=seed, n_splits=7)\n",
    "        \n",
    "        # L2: Fast Linear Meta model with grade ordinal\n",
    "        print('\\n[L2] Training fast linear meta...')\n",
    "        oof_l2, test_l2 = train_meta_l2(oof_l1, y, test_l1, seed=seed, X_orig=X, X_test_orig=X_test)\n",
    "        \n",
    "        # L3: Pseudo-labeling (if available)\n",
    "        print('\\n[L3] Pseudo-labeling...')\n",
    "        oof_l3 = train_meta_l3_with_pseudo(oof_l2, y, test_l2, X, X_test, seed=seed)\n",
    "        \n",
    "        # Isotonic calibration\n",
    "        print('\\n[CAL] Calibrating predictions...')\n",
    "        iso = fit_isotonic(y.values, oof_l3)\n",
    "        oof_cal = iso.predict(oof_l3)\n",
    "        auc_cal = roc_auc_score(y, oof_cal)\n",
    "        test_cal = iso.predict(test_l2) if test_l2 is not None else None\n",
    "        \n",
    "        # Threshold optimization\n",
    "        best_thr = threshold_sweep(y.values, oof_cal)\n",
    "        \n",
    "        print(f'\\n{\"=\"*70}')\n",
    "        print(f'ðŸŽ¯ FINAL AUC (calibrated): {auc_cal:.5f}')\n",
    "        print(f'ðŸ“Š Best threshold: {best_thr[\"threshold\"]:.3f} (F1={best_thr[\"f1\"]:.4f})')\n",
    "        print(f'{\"=\"*70}')\n",
    "        \n",
    "        record = {\n",
    "            'seed': seed,\n",
    "            'auc_l2': roc_auc_score(y, oof_l2),\n",
    "            'auc_l3': roc_auc_score(y, oof_l3),\n",
    "            'auc_cal': auc_cal,\n",
    "            'best_thr': best_thr,\n",
    "        }\n",
    "        results.append(record)\n",
    "        \n",
    "        if (best is None) or (auc_cal > best['auc_cal']):\n",
    "            best = {\n",
    "                **record,\n",
    "                'oof_cal': oof_cal,\n",
    "                'test_cal': test_cal,\n",
    "                'base_aucs': base_aucs\n",
    "            }\n",
    "            print(f'âœ¨ NEW BEST: {auc_cal:.5f}')\n",
    "        \n",
    "        if auc_cal >= target_auc:\n",
    "            print(f'\\nðŸ† BREAKTHROUGH! Hit {target_auc:.1%} target!')\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(results), best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b96babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ TARGET: Break 93% AUC barrier\n",
      "ðŸ“ˆ Strategy: L1â†’L2â†’L3 stacking + pseudo-labeling + calibration\n",
      "â±ï¸  ETA: ~10-15 minutes with full optimization\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ SEED 42 (1/5) â€” Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "ðŸš€ Training 5 L1 base models: ['xgb', 'lgb', 'cb', 'rf', 'mlp']\n",
      "ðŸš€ Training 5 L1 base models: ['xgb', 'lgb', 'cb', 'rf', 'mlp']\n",
      "  âœ“ xgb: [0.92197 0.92046 0.92024 0.91901 0.9185  0.92053 0.91986] â†’ mean 0.92008\n",
      "  âœ“ xgb: [0.92197 0.92046 0.92024 0.91901 0.9185  0.92053 0.91986] â†’ mean 0.92008\n",
      "  âœ“ lgb: [0.92272 0.9223  0.92116 0.92017 0.91999 0.92162 0.92055] â†’ mean 0.92122\n",
      "  âœ“ cb: [0.92051 0.91938 0.91914 0.91812 0.91734 0.91967 0.91886] â†’ mean 0.91900\n",
      "  âœ“ rf: [0.91535 0.91403 0.91379 0.91348 0.9125  0.91465 0.91347] â†’ mean 0.91390\n",
      "  âœ“ mlp: [0.91423 0.91269 0.91005 0.9117  0.90823 0.91313 0.91236] â†’ mean 0.91177\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  ðŸ“Š L2 Hill Climbing on 5 base model predictions\n",
      "  âœ“ Hill Climbing optimized weights: [0.2328 0.3771 0.2633 0.     0.1269]\n",
      "  âœ“ L2 Hill Climbing Meta AUC: 0.92060\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  ðŸŽ­ Pseudo-labeling: 175974 high-confidence test samples\n",
      "  âœ“ L3 Meta + Pseudo AUC: 0.92060\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FINAL AUC (calibrated): 0.92071\n",
      "ðŸ“Š Best threshold: 0.450 (F1=0.9431)\n",
      "======================================================================\n",
      "âœ¨ NEW BEST: 0.92071\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ SEED 43 (2/5) â€” Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "ðŸš€ Training 5 L1 base models: ['xgb', 'lgb', 'cb', 'rf', 'mlp']\n",
      "  âœ“ xgb: [0.92096 0.92143 0.91878 0.91736 0.92129 0.92138 0.91889] â†’ mean 0.92001\n",
      "  âœ“ lgb: [0.92191 0.92254 0.92035 0.91825 0.92252 0.92294 0.92012] â†’ mean 0.92123\n",
      "  âœ“ cb: [0.92006 0.92064 0.91735 0.91595 0.92025 0.92041 0.91787] â†’ mean 0.91893\n",
      "  âœ“ rf: [0.91519 0.91565 0.91203 0.91089 0.91548 0.91531 0.91276] â†’ mean 0.91390\n",
      "  âœ“ mlp: [0.91222 0.91275 0.91032 0.9095  0.91323 0.91274 0.91081] â†’ mean 0.91165\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  ðŸ“Š L2 Hill Climbing on 5 base model predictions\n",
      "  âœ“ Hill Climbing optimized weights: [0.4397 0.4538 0.1007 0.0058 0.    ]\n",
      "  âœ“ L2 Hill Climbing Meta AUC: 0.92111\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  ðŸŽ­ Pseudo-labeling: 177638 high-confidence test samples\n",
      "  âœ“ L3 Meta + Pseudo AUC: 0.92110\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FINAL AUC (calibrated): 0.92121\n",
      "ðŸ“Š Best threshold: 0.500 (F1=0.9430)\n",
      "======================================================================\n",
      "âœ¨ NEW BEST: 0.92121\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ SEED 44 (3/5) â€” Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "ðŸš€ Training 5 L1 base models: ['xgb', 'lgb', 'cb', 'rf', 'mlp']\n",
      "  âœ“ xgb: [0.92006 0.91964 0.92066 0.92089 0.92028 0.92059 0.91824] â†’ mean 0.92005\n",
      "  âœ“ lgb: [0.92107 0.92095 0.92198 0.92216 0.92157 0.92159 0.91937] â†’ mean 0.92124\n",
      "  âœ“ cb: [0.91873 0.91907 0.91995 0.92042 0.91932 0.91941 0.91711] â†’ mean 0.91914\n",
      "  âœ“ rf: [0.91389 0.91367 0.9146  0.91523 0.91459 0.91411 0.91154] â†’ mean 0.91395\n",
      "  âœ“ mlp: [0.9113  0.91166 0.91299 0.91312 0.91271 0.91153 0.91013] â†’ mean 0.91192\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  ðŸ“Š L2 Hill Climbing on 5 base model predictions\n",
      "  âœ“ Hill Climbing optimized weights: [0.2581 0.2302 0.3798 0.1319 0.    ]\n",
      "  âœ“ L2 Hill Climbing Meta AUC: 0.92037\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  ðŸŽ­ Pseudo-labeling: 175964 high-confidence test samples\n",
      "  âœ“ L3 Meta + Pseudo AUC: 0.92036\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FINAL AUC (calibrated): 0.92047\n",
      "ðŸ“Š Best threshold: 0.500 (F1=0.9431)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ SEED 45 (4/5) â€” Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "ðŸš€ Training 5 L1 base models: ['xgb', 'lgb', 'cb', 'rf', 'mlp']\n",
      "  âœ“ xgb: [0.91964 0.91854 0.91982 0.91973 0.92143 0.92055 0.92042] â†’ mean 0.92002\n",
      "  âœ“ lgb: [0.92084 0.9198  0.92086 0.92092 0.92241 0.92164 0.92192] â†’ mean 0.92120\n",
      "  âœ“ cb: [0.91903 0.91742 0.91901 0.91879 0.92045 0.91958 0.91978] â†’ mean 0.91915\n",
      "  âœ“ rf: [0.91387 0.91162 0.91361 0.9139  0.91545 0.91438 0.91438] â†’ mean 0.91389\n",
      "  âœ“ mlp: [0.91146 0.90889 0.91177 0.91048 0.91342 0.91305 0.91347] â†’ mean 0.91179\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  ðŸ“Š L2 Hill Climbing on 5 base model predictions\n",
      "  âœ“ Hill Climbing optimized weights: [0.3371 0.4578 0.1185 0.057  0.0296]\n",
      "  âœ“ L2 Hill Climbing Meta AUC: 0.92095\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  ðŸŽ­ Pseudo-labeling: 177159 high-confidence test samples\n",
      "  âœ“ L3 Meta + Pseudo AUC: 0.92094\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FINAL AUC (calibrated): 0.92104\n",
      "ðŸ“Š Best threshold: 0.500 (F1=0.9430)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ SEED 46 (5/5) â€” Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "ðŸš€ Training 5 L1 base models: ['xgb', 'lgb', 'cb', 'rf', 'mlp']\n",
      "  âœ“ xgb: [0.92098 0.91997 0.92246 0.91925 0.91838 0.92066 0.91878] â†’ mean 0.92007\n",
      "  âœ“ lgb: [0.92234 0.92138 0.92303 0.92033 0.91954 0.922   0.91965] â†’ mean 0.92118\n",
      "  âœ“ cb: [0.91978 0.9187  0.92152 0.91862 0.91755 0.92016 0.91791] â†’ mean 0.91918\n",
      "  âœ“ rf: [0.91491 0.91343 0.91617 0.91301 0.91231 0.91495 0.91243] â†’ mean 0.91389\n",
      "  âœ“ mlp: [0.91296 0.91181 0.91473 0.911   0.90951 0.91315 0.90948] â†’ mean 0.91181\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  ðŸ“Š L2 Hill Climbing on 5 base model predictions\n",
      "  âœ“ Hill Climbing optimized weights: [0.1888 0.1168 0.6567 0.     0.0377]\n",
      "  âœ“ L2 Hill Climbing Meta AUC: 0.92011\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  ðŸŽ­ Pseudo-labeling: 175466 high-confidence test samples\n",
      "  âœ“ L3 Meta + Pseudo AUC: 0.92010\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FINAL AUC (calibrated): 0.92021\n",
      "ðŸ“Š Best threshold: 0.450 (F1=0.9430)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š RESULTS SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>auc_l2</th>\n",
       "      <th>auc_l3</th>\n",
       "      <th>auc_cal</th>\n",
       "      <th>best_thr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>0.920604</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>0.920708</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>0.921109</td>\n",
       "      <td>0.921105</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>{'threshold': 0.49999999999999994, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>0.920370</td>\n",
       "      <td>0.920363</td>\n",
       "      <td>0.920471</td>\n",
       "      <td>{'threshold': 0.49999999999999994, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.920943</td>\n",
       "      <td>0.921042</td>\n",
       "      <td>{'threshold': 0.49999999999999994, 'f1': 0.942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>0.920105</td>\n",
       "      <td>0.920104</td>\n",
       "      <td>0.920209</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.942...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed    auc_l2    auc_l3   auc_cal  \\\n",
       "0    42  0.920604  0.920597  0.920708   \n",
       "1    43  0.921109  0.921105  0.921208   \n",
       "2    44  0.920370  0.920363  0.920471   \n",
       "3    45  0.920950  0.920943  0.921042   \n",
       "4    46  0.920105  0.920104  0.920209   \n",
       "\n",
       "                                            best_thr  \n",
       "0  {'threshold': 0.44999999999999996, 'f1': 0.943...  \n",
       "1  {'threshold': 0.49999999999999994, 'f1': 0.943...  \n",
       "2  {'threshold': 0.49999999999999994, 'f1': 0.943...  \n",
       "3  {'threshold': 0.49999999999999994, 'f1': 0.942...  \n",
       "4  {'threshold': 0.44999999999999996, 'f1': 0.942...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† BEST RESULT:\n",
      "   Seed: 43\n",
      "   L2 AUC: 0.92111\n",
      "   L3 AUC: 0.92110\n",
      "   Calibrated AUC: 0.92121\n",
      "   Threshold: 0.500\n",
      "   F1 Score: 0.9430\n",
      "\n",
      "ðŸ“ Gap to 93%: 0.00879 (0.879 pp)\n",
      "ðŸ’¡ Next steps: Add more seeds, try neural blend, or ensemble with other models\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ EXECUTE: Extreme training for 93%+ AUC\n",
    "\n",
    "print('ðŸŽ¯ TARGET: Break 93% AUC barrier')\n",
    "print('ðŸ“ˆ Strategy: L1â†’L2â†’L3 stacking + pseudo-labeling + calibration')\n",
    "print('â±ï¸  ETA: ~10-15 minutes with full optimization\\n')\n",
    "\n",
    "SEEDS = [42, 43, 44, 45, 46]  # Increased seeds for stability and performance\n",
    "results_df, best = run_training_extreme(seeds=SEEDS, target_auc=0.93)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸ“Š RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "display(results_df)\n",
    "\n",
    "print(f'\\nðŸ† BEST RESULT:')\n",
    "print(f'   Seed: {best[\"seed\"]}')\n",
    "print(f'   L2 AUC: {best[\"auc_l2\"]:.5f}')\n",
    "print(f'   L3 AUC: {best[\"auc_l3\"]:.5f}')\n",
    "print(f'   Calibrated AUC: {best[\"auc_cal\"]:.5f}')\n",
    "print(f'   Threshold: {best[\"best_thr\"][\"threshold\"]:.3f}')\n",
    "print(f'   F1 Score: {best[\"best_thr\"][\"f1\"]:.4f}')\n",
    "\n",
    "if best['auc_cal'] >= 0.93:\n",
    "    print(f'\\nðŸŽ‰ðŸŽ‰ï¿½ BREAKTHROUGH ACHIEVED! {best[\"auc_cal\"]:.5f} >= 93% ðŸŽ‰ðŸŽ‰ðŸŽ‰')\n",
    "else:\n",
    "    gap = 0.93 - best['auc_cal']\n",
    "    print(f'\\nðŸ“ Gap to 93%: {gap:.5f} ({gap*100:.3f} pp)')\n",
    "    print('ðŸ’¡ Next steps: Add more seeds, try neural blend, or ensemble with other models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "368e872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† SUBMISSION SAVED!\n",
      "   File: EXTREME_93pct_AUC092121_20251120_011834.csv\n",
      "   AUC: 0.92121\n",
      "   Threshold: 0.500\n",
      "   Samples: 254,569\n"
     ]
    }
   ],
   "source": [
    "# ðŸ… Build WINNING submission\n",
    "\n",
    "if 'test' in globals() and test is not None and SAMPLE_SUB_PATH.exists() and best is not None:\n",
    "    sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "    sub_id_col = sub.columns[0]\n",
    "    sub_target_col = sub.columns[1] if len(sub.columns) > 1 else (TARGET if TARGET is not None else 'target')\n",
    "    \n",
    "    if 'ID_COL' in globals() and ID_COL and sub_id_col != ID_COL and ID_COL in test.columns:\n",
    "        sub[sub_id_col] = test[ID_COL].values\n",
    "    \n",
    "    preds = best['test_cal'] if best.get('test_cal') is not None else None\n",
    "    \n",
    "    if preds is not None:\n",
    "        sub[sub_target_col] = preds\n",
    "        timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "        auc_str = f\"{best['auc_cal']:.5f}\".replace('.', '')\n",
    "        out_path = SUB_DIR / f'EXTREME_93pct_AUC{auc_str}_{timestamp}.csv'\n",
    "        sub.to_csv(out_path, index=False)\n",
    "        \n",
    "        print(f'\\nðŸ† SUBMISSION SAVED!')\n",
    "        print(f'   File: {out_path.name}')\n",
    "        print(f'   AUC: {best[\"auc_cal\"]:.5f}')\n",
    "        print(f'   Threshold: {best[\"best_thr\"][\"threshold\"]:.3f}')\n",
    "        print(f'   Samples: {len(sub):,}')\n",
    "        \n",
    "        if best['auc_cal'] >= 0.93:\n",
    "            print(f'\\nðŸŽŠ FIRST TO BREAK 93%! Submit this ASAP! ðŸŽŠ')\n",
    "    else:\n",
    "        print('âš ï¸  No test predictions available.')\n",
    "else:\n",
    "    print('âš ï¸  Submission not created (missing test data or best result).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163bd18",
   "metadata": {},
   "source": [
    "Stuff we tryed so far \n",
    "\n",
    ". Architecture & Model Stacking\n",
    "L1 Base Layer: Started with XGBoost, LightGBM, and CatBoost.\n",
    "Tried: Adding Logistic Regression for diversity (Removed later due to complexity/low gain).\n",
    "Current State: XGB, LGB, CB with 1500 estimators and learning_rate=0.015.\n",
    "L2 Meta Layer:\n",
    "Tried: Fast Linear Stacking (Logistic/Ridge) to save time. Result: Performance dropped (0.917).\n",
    "Restored: XGBoost Meta-Learner. We switched back to a shallow XGBoost (Depth 4) to capture non-linearities between model predictions.\n",
    "L3 Pseudo-Labeling:\n",
    "Implemented: Using high-confidence test predictions (top/bottom 10%) to retrain the ensemble.\n",
    "Calibration:\n",
    "Implemented: Isotonic Regression to recalibrate probabilities before the final submission.\n",
    "2. Feature Engineering\n",
    "Target Encoding: Applied smoothed target encoding to all categorical features (10-fold CV to prevent leakage).\n",
    "Interaction Pairs: Created standard ratios and products between key columns (Loan Amount, Income, etc.).\n",
    "Visual Insight Fixes:\n",
    "Ordinal Encoding: Mapped grade_subgrade explicitly (A1=1 ... F5=35) because the plots showed a perfect monotonic trend.\n",
    "Unemployment Flag: Created is_unemployed binary feature based on the specific drop-off seen in the plots.\n",
    "Log Transforms: Applied np.log1p to Income, Loan Amount, and DTI to handle the massive right-skew seen in histograms.\n",
    "Risk Features:\n",
    "Risk Ratio: Added Interest Rate / Credit Score (High rate + Low score = Extreme risk).\n",
    "PTI Proxy: (Proposed) (Loan * Rate) / Income to approximate payment burden.\n",
    "3. Hyperparameter Tuning & Correction\n",
    "Class Weights (The Failed Experiment):\n",
    "Action: We enabled scale_pos_weight and is_unbalance to handle the 80/20 split.\n",
    "Result: Score dropped to 0.917. Class weights distorted the ranking probabilities required for AUC.\n",
    "Fix: Removed all class weights to let the model learn the natural probability distribution.\n",
    "Estimator Count:\n",
    "Action: Reduced to 800 for speed.\n",
    "Correction: Bumped back to 1500 because the model was underfitting the complex feature set.\n",
    "4. Current Status\n",
    "Best Score: 0.92020 (Leaderboard).\n",
    "Current Issue: Generalization Gap. Local CV is higher (~0.9215) than LB (0.9202). The boosting models are correlated and slightly overfitting.\n",
    "Immediate Next Step: Introduce Bagging (Random Forest) to the L1 layer to reduce variance and close the gap to 0.93."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
