{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4a0969",
   "metadata": {},
   "source": [
    "# Loan Payback — Meta-Boosted XGBoost (Copilot Edition)\n",
    "\n",
    "This notebook builds a **two–stage imbalance-aware boosted model** with advanced feature engineering:\n",
    "\n",
    "## Key Features:\n",
    "1. **Auto Feature Engineering**: Log transforms, ratios, normalized credit scores\n",
    "2. **Imbalance Handling**: Automatic `scale_pos_weight` calculation\n",
    "3. **Stage 1**: Strong XGBoost with deeper trees and careful regularization\n",
    "4. **Stage 2 (Meta Boost)**: Boosts over Stage-1 logits using `base_margin`\n",
    "5. **5-Fold Stratified CV** with threshold optimization\n",
    "6. **Automatic model selection** based on OOF AUC\n",
    "\n",
    "## Target: 0.925+ AUC\n",
    "Follow the tuning guide at the end to iteratively improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "464cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Imports & basic configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_DIR = Path(\"Data\")\n",
    "# If running locally, you can override DATA_DIR, e.g.:\n",
    "# DATA_DIR = Path(\"/mnt/data\") / \"loan-payback\"\n",
    "\n",
    "def log(msg: str):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cb6aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:23] Using train: train.csv\n",
      "[23:16:23] Using test : test.csv\n",
      "[23:16:24] Train shape: (593994, 13)\n",
      "[23:16:24] Test  shape: (254569, 12)\n",
      "[23:16:24] Detected target column: loan_paid_back\n",
      "[23:16:24] Detected id column: id\n",
      "[23:16:24] Class distribution: 119500 negatives, 474494 positives\n",
      "[23:16:24] scale_pos_weight = 0.2518\n",
      "[23:16:24] Number of features: 11\n",
      "[23:16:24] Train shape: (593994, 13)\n",
      "[23:16:24] Test  shape: (254569, 12)\n",
      "[23:16:24] Detected target column: loan_paid_back\n",
      "[23:16:24] Detected id column: id\n",
      "[23:16:24] Class distribution: 119500 negatives, 474494 positives\n",
      "[23:16:24] scale_pos_weight = 0.2518\n",
      "[23:16:24] Number of features: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Data loading and automatic target / id detection\n",
    "\n",
    "train_path = None\n",
    "test_path = None\n",
    "\n",
    "# Heuristic: pick first train/test-looking CSVs\n",
    "csv_files = sorted(list(DATA_DIR.glob(\"*.csv\")))\n",
    "for p in csv_files:\n",
    "    name = p.name.lower()\n",
    "    if \"train\" in name and train_path is None:\n",
    "        train_path = p\n",
    "    if \"test\" in name and test_path is None and \"train\" not in name:\n",
    "        test_path = p\n",
    "\n",
    "if train_path is None or test_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not detect train/test CSVs inside {DATA_DIR}. \"\n",
    "        \"Please set train_path and test_path manually.\"\n",
    "    )\n",
    "\n",
    "log(f\"Using train: {train_path.name}\")\n",
    "log(f\"Using test : {test_path.name}\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "log(f\"Train shape: {train_df.shape}\")\n",
    "log(f\"Test  shape: {test_df.shape}\")\n",
    "\n",
    "def detect_target(train_df: pd.DataFrame, test_df: pd.DataFrame) -> str:\n",
    "    diff = list(set(train_df.columns) - set(test_df.columns))\n",
    "    # Prefer a binary label\n",
    "    candidates = []\n",
    "    for c in diff:\n",
    "        if train_df[c].nunique() <= 3:\n",
    "            candidates.append(c)\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    if len(diff) == 1:\n",
    "        return diff[0]\n",
    "    for name in [\"loan_paid_back\", \"target\", \"label\", \"is_default\", \"default\", \"paid\"]:\n",
    "        if name in train_df.columns and name not in test_df.columns:\n",
    "            return name\n",
    "    raise ValueError(f\"Could not detect target. Diff columns: {diff}\")\n",
    "\n",
    "target_col = detect_target(train_df, test_df)\n",
    "log(f\"Detected target column: {target_col}\")\n",
    "\n",
    "# Simple ID detection: column whose values are unique in train and test\n",
    "id_col = None\n",
    "for col in train_df.columns:\n",
    "    if col == target_col:\n",
    "        continue\n",
    "    if col in test_df.columns:\n",
    "        if train_df[col].is_unique and test_df[col].is_unique:\n",
    "            id_col = col\n",
    "            break\n",
    "\n",
    "log(f\"Detected id column: {id_col}\")\n",
    "\n",
    "y = train_df[target_col].astype(int).values\n",
    "\n",
    "# Compute class imbalance for scale_pos_weight\n",
    "n_pos = (y == 1).sum()\n",
    "n_neg = (y == 0).sum()\n",
    "scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "log(f\"Class distribution: {n_neg} negatives, {n_pos} positives\")\n",
    "log(f\"scale_pos_weight = {scale_pos_weight:.4f}\")\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in [target_col, id_col]]\n",
    "X = train_df[feature_cols].copy()\n",
    "X_test = test_df[feature_cols].copy()\n",
    "\n",
    "log(f\"Number of features: {len(feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7be81abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:24] Numeric features    : 5\n",
      "[23:16:24] Categorical features: 6\n",
      "[23:16:24] Fitting preprocessing on full training data...\n",
      "[23:16:25] Processed X shape      : (593994, 60)\n",
      "[23:16:25] Processed X_test shape : (254569, 60)\n",
      "[23:16:25] Processed X shape      : (593994, 60)\n",
      "[23:16:25] Processed X_test shape : (254569, 60)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Preprocessing: numeric + categorical pipelines\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "log(f\"Numeric features    : {len(numeric_cols)}\")\n",
    "log(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "numeric_transformer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on full training data, then transform train & test\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "\n",
    "dummy_model = SklearnPipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "])\n",
    "\n",
    "log(\"Fitting preprocessing on full training data...\")\n",
    "dummy_model.fit(X)\n",
    "\n",
    "X_proc = dummy_model.transform(X)\n",
    "X_test_proc = dummy_model.transform(X_test)\n",
    "\n",
    "log(f\"Processed X shape      : {X_proc.shape}\")\n",
    "log(f\"Processed X_test shape : {X_test_proc.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8b36608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:25] Engineering features for train and test...\n",
      "[23:16:25] ✓ Created loan_amount_log from loan_amount\n",
      "[23:16:25] ✓ Created annual_income_log from annual_income\n",
      "[23:16:25] ✓ Created income_per_loan\n",
      "[23:16:25] ✓ Created dti_x_rate\n",
      "[23:16:25] ✓ Created credit_score_norm from credit_score\n",
      "[23:16:25] ✓ Created loan_amount_log from loan_amount\n",
      "[23:16:25] ✓ Created annual_income_log from annual_income\n",
      "[23:16:25] ✓ Created income_per_loan\n",
      "[23:16:25] ✓ Created dti_x_rate\n",
      "[23:16:25] ✓ Created credit_score_norm from credit_score\n",
      "[23:16:25] Features after engineering: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Feature Engineering: Add powerful derived features\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Automatically creates advanced features if the base columns exist.\n",
    "    Returns a copy with new features added.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Map common column name variations (case-insensitive)\n",
    "    col_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        col_map[col_lower] = col\n",
    "    \n",
    "    # Log transforms for skewed amounts\n",
    "    if \"loan_amount\" in col_map or \"loan_amnt\" in col_map:\n",
    "        loan_col = col_map.get(\"loan_amount\", col_map.get(\"loan_amnt\"))\n",
    "        df[\"loan_amount_log\"] = np.log1p(df[loan_col])\n",
    "        log(f\"✓ Created loan_amount_log from {loan_col}\")\n",
    "    \n",
    "    if \"annual_income\" in col_map or \"annual_inc\" in col_map:\n",
    "        income_col = col_map.get(\"annual_income\", col_map.get(\"annual_inc\"))\n",
    "        df[\"annual_income_log\"] = np.log1p(df[income_col])\n",
    "        log(f\"✓ Created annual_income_log from {income_col}\")\n",
    "    \n",
    "    # Income per loan ratio\n",
    "    loan_col = col_map.get(\"loan_amount\", col_map.get(\"loan_amnt\"))\n",
    "    income_col = col_map.get(\"annual_income\", col_map.get(\"annual_inc\"))\n",
    "    if loan_col and income_col:\n",
    "        df[\"income_per_loan\"] = df[income_col] / (df[loan_col] + 1)\n",
    "        log(f\"✓ Created income_per_loan\")\n",
    "    \n",
    "    # DTI × Interest Rate interaction\n",
    "    dti_col = col_map.get(\"debt_to_income_ratio\", col_map.get(\"dti\"))\n",
    "    rate_col = col_map.get(\"interest_rate\", col_map.get(\"int_rate\"))\n",
    "    if dti_col and rate_col:\n",
    "        df[\"dti_x_rate\"] = df[dti_col] * df[rate_col]\n",
    "        log(f\"✓ Created dti_x_rate\")\n",
    "    \n",
    "    # Normalized credit score\n",
    "    credit_col = col_map.get(\"credit_score\", col_map.get(\"fico_range_low\"))\n",
    "    if credit_col:\n",
    "        mean_credit = df[credit_col].mean()\n",
    "        std_credit = df[credit_col].std()\n",
    "        if std_credit > 0:\n",
    "            df[\"credit_score_norm\"] = (df[credit_col] - mean_credit) / std_credit\n",
    "            log(f\"✓ Created credit_score_norm from {credit_col}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "log(\"Engineering features for train and test...\")\n",
    "X = engineer_features(X)\n",
    "X_test = engineer_features(X_test)\n",
    "\n",
    "log(f\"Features after engineering: {X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed116d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:25] Training Stage 1 model (imbalance-aware XGBoost)...\n",
      "[23:16:25] Params: max_depth=5, min_child_weight=20, eta=0.035, scale_pos_weight=0.252\n",
      "[23:16:25] Fold 1/5\n",
      "[0]\ttrain-auc:0.90254\tvalid-auc:0.90304\n",
      "[0]\ttrain-auc:0.90254\tvalid-auc:0.90304\n",
      "[200]\ttrain-auc:0.91698\tvalid-auc:0.91762\n",
      "[200]\ttrain-auc:0.91698\tvalid-auc:0.91762\n",
      "[400]\ttrain-auc:0.92000\tvalid-auc:0.91963\n",
      "[400]\ttrain-auc:0.92000\tvalid-auc:0.91963\n",
      "[600]\ttrain-auc:0.92239\tvalid-auc:0.92088\n",
      "[600]\ttrain-auc:0.92239\tvalid-auc:0.92088\n",
      "[800]\ttrain-auc:0.92448\tvalid-auc:0.92186\n",
      "[800]\ttrain-auc:0.92448\tvalid-auc:0.92186\n",
      "[1000]\ttrain-auc:0.92601\tvalid-auc:0.92240\n",
      "[1000]\ttrain-auc:0.92601\tvalid-auc:0.92240\n",
      "[1200]\ttrain-auc:0.92723\tvalid-auc:0.92263\n",
      "[1200]\ttrain-auc:0.92723\tvalid-auc:0.92263\n",
      "[1400]\ttrain-auc:0.92831\tvalid-auc:0.92284\n",
      "[1400]\ttrain-auc:0.92831\tvalid-auc:0.92284\n",
      "[1600]\ttrain-auc:0.92929\tvalid-auc:0.92294\n",
      "[1600]\ttrain-auc:0.92929\tvalid-auc:0.92294\n",
      "[1800]\ttrain-auc:0.93018\tvalid-auc:0.92299\n",
      "[1800]\ttrain-auc:0.93018\tvalid-auc:0.92299\n",
      "[1999]\ttrain-auc:0.93099\tvalid-auc:0.92300\n",
      "[1999]\ttrain-auc:0.93099\tvalid-auc:0.92300\n",
      "[23:18:21] Fold 2/5\n",
      "[23:18:21] Fold 2/5\n",
      "[0]\ttrain-auc:0.90264\tvalid-auc:0.90265\n",
      "[0]\ttrain-auc:0.90264\tvalid-auc:0.90265\n",
      "[200]\ttrain-auc:0.91719\tvalid-auc:0.91692\n",
      "[200]\ttrain-auc:0.91719\tvalid-auc:0.91692\n",
      "[400]\ttrain-auc:0.91999\tvalid-auc:0.91876\n",
      "[400]\ttrain-auc:0.91999\tvalid-auc:0.91876\n",
      "[600]\ttrain-auc:0.92253\tvalid-auc:0.92027\n",
      "[600]\ttrain-auc:0.92253\tvalid-auc:0.92027\n",
      "[800]\ttrain-auc:0.92436\tvalid-auc:0.92123\n",
      "[800]\ttrain-auc:0.92436\tvalid-auc:0.92123\n",
      "[1000]\ttrain-auc:0.92590\tvalid-auc:0.92184\n",
      "[1000]\ttrain-auc:0.92590\tvalid-auc:0.92184\n",
      "[1200]\ttrain-auc:0.92715\tvalid-auc:0.92222\n",
      "[1200]\ttrain-auc:0.92715\tvalid-auc:0.92222\n",
      "[1400]\ttrain-auc:0.92820\tvalid-auc:0.92239\n",
      "[1400]\ttrain-auc:0.92820\tvalid-auc:0.92239\n",
      "[1600]\ttrain-auc:0.92915\tvalid-auc:0.92257\n",
      "[1600]\ttrain-auc:0.92915\tvalid-auc:0.92257\n",
      "[1800]\ttrain-auc:0.93003\tvalid-auc:0.92266\n",
      "[1800]\ttrain-auc:0.93003\tvalid-auc:0.92266\n",
      "[1999]\ttrain-auc:0.93084\tvalid-auc:0.92274\n",
      "[1999]\ttrain-auc:0.93084\tvalid-auc:0.92274\n",
      "[23:20:15] Fold 3/5\n",
      "[0]\ttrain-auc:0.90273\tvalid-auc:0.90206\n",
      "[23:20:15] Fold 3/5\n",
      "[0]\ttrain-auc:0.90273\tvalid-auc:0.90206\n",
      "[200]\ttrain-auc:0.91755\tvalid-auc:0.91555\n",
      "[200]\ttrain-auc:0.91755\tvalid-auc:0.91555\n",
      "[400]\ttrain-auc:0.92038\tvalid-auc:0.91714\n",
      "[400]\ttrain-auc:0.92038\tvalid-auc:0.91714\n",
      "[600]\ttrain-auc:0.92284\tvalid-auc:0.91849\n",
      "[600]\ttrain-auc:0.92284\tvalid-auc:0.91849\n",
      "[800]\ttrain-auc:0.92480\tvalid-auc:0.91949\n",
      "[800]\ttrain-auc:0.92480\tvalid-auc:0.91949\n",
      "[1000]\ttrain-auc:0.92631\tvalid-auc:0.92008\n",
      "[1000]\ttrain-auc:0.92631\tvalid-auc:0.92008\n",
      "[1200]\ttrain-auc:0.92767\tvalid-auc:0.92049\n",
      "[1200]\ttrain-auc:0.92767\tvalid-auc:0.92049\n",
      "[1400]\ttrain-auc:0.92874\tvalid-auc:0.92069\n",
      "[1400]\ttrain-auc:0.92874\tvalid-auc:0.92069\n",
      "[1600]\ttrain-auc:0.92968\tvalid-auc:0.92079\n",
      "[1600]\ttrain-auc:0.92968\tvalid-auc:0.92079\n",
      "[1800]\ttrain-auc:0.93061\tvalid-auc:0.92088\n",
      "[1800]\ttrain-auc:0.93061\tvalid-auc:0.92088\n",
      "[1999]\ttrain-auc:0.93139\tvalid-auc:0.92094\n",
      "[1999]\ttrain-auc:0.93139\tvalid-auc:0.92094\n",
      "[23:22:09] Fold 4/5\n",
      "[0]\ttrain-auc:0.90261\tvalid-auc:0.90246\n",
      "[23:22:09] Fold 4/5\n",
      "[0]\ttrain-auc:0.90261\tvalid-auc:0.90246\n",
      "[200]\ttrain-auc:0.91744\tvalid-auc:0.91648\n",
      "[200]\ttrain-auc:0.91744\tvalid-auc:0.91648\n",
      "[400]\ttrain-auc:0.92048\tvalid-auc:0.91835\n",
      "[400]\ttrain-auc:0.92048\tvalid-auc:0.91835\n",
      "[600]\ttrain-auc:0.92266\tvalid-auc:0.91939\n",
      "[600]\ttrain-auc:0.92266\tvalid-auc:0.91939\n",
      "[800]\ttrain-auc:0.92455\tvalid-auc:0.92029\n",
      "[800]\ttrain-auc:0.92455\tvalid-auc:0.92029\n",
      "[1000]\ttrain-auc:0.92607\tvalid-auc:0.92081\n",
      "[1000]\ttrain-auc:0.92607\tvalid-auc:0.92081\n",
      "[1200]\ttrain-auc:0.92735\tvalid-auc:0.92120\n",
      "[1200]\ttrain-auc:0.92735\tvalid-auc:0.92120\n",
      "[1400]\ttrain-auc:0.92846\tvalid-auc:0.92146\n",
      "[1400]\ttrain-auc:0.92846\tvalid-auc:0.92146\n",
      "[1600]\ttrain-auc:0.92942\tvalid-auc:0.92159\n",
      "[1600]\ttrain-auc:0.92942\tvalid-auc:0.92159\n",
      "[1800]\ttrain-auc:0.93034\tvalid-auc:0.92162\n",
      "[1800]\ttrain-auc:0.93034\tvalid-auc:0.92162\n",
      "[1864]\ttrain-auc:0.93059\tvalid-auc:0.92163\n",
      "[1864]\ttrain-auc:0.93059\tvalid-auc:0.92163\n",
      "[23:23:56] Fold 5/5\n",
      "[0]\ttrain-auc:0.90289\tvalid-auc:0.90273\n",
      "[23:23:56] Fold 5/5\n",
      "[0]\ttrain-auc:0.90289\tvalid-auc:0.90273\n",
      "[200]\ttrain-auc:0.91742\tvalid-auc:0.91604\n",
      "[200]\ttrain-auc:0.91742\tvalid-auc:0.91604\n",
      "[400]\ttrain-auc:0.92043\tvalid-auc:0.91808\n",
      "[400]\ttrain-auc:0.92043\tvalid-auc:0.91808\n",
      "[600]\ttrain-auc:0.92273\tvalid-auc:0.91941\n",
      "[600]\ttrain-auc:0.92273\tvalid-auc:0.91941\n",
      "[800]\ttrain-auc:0.92448\tvalid-auc:0.92017\n",
      "[800]\ttrain-auc:0.92448\tvalid-auc:0.92017\n",
      "[1000]\ttrain-auc:0.92590\tvalid-auc:0.92068\n",
      "[1000]\ttrain-auc:0.92590\tvalid-auc:0.92068\n",
      "[1200]\ttrain-auc:0.92719\tvalid-auc:0.92109\n",
      "[1200]\ttrain-auc:0.92719\tvalid-auc:0.92109\n",
      "[1400]\ttrain-auc:0.92826\tvalid-auc:0.92132\n",
      "[1400]\ttrain-auc:0.92826\tvalid-auc:0.92132\n",
      "[1600]\ttrain-auc:0.92920\tvalid-auc:0.92146\n",
      "[1600]\ttrain-auc:0.92920\tvalid-auc:0.92146\n",
      "[1800]\ttrain-auc:0.93005\tvalid-auc:0.92153\n",
      "[1800]\ttrain-auc:0.93005\tvalid-auc:0.92153\n",
      "[1999]\ttrain-auc:0.93087\tvalid-auc:0.92157\n",
      "[1999]\ttrain-auc:0.93087\tvalid-auc:0.92157\n",
      "[23:25:52] Stage 1 OOF ROC-AUC: 0.92197\n",
      "[23:25:52] Stage 1 OOF ROC-AUC: 0.92197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5) Stage 1: Imbalance-Aware XGBoost with Stronger Regularization\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "oof_pred_stage1 = np.zeros(X_proc.shape[0])\n",
    "test_pred_stage1_folds = []\n",
    "\n",
    "params_stage1 = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 20,\n",
    "    \"learning_rate\": 0.035,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "log(\"Training Stage 1 model (imbalance-aware XGBoost)...\")\n",
    "log(f\"Params: max_depth={params_stage1['max_depth']}, \"\n",
    "    f\"min_child_weight={params_stage1['min_child_weight']}, \"\n",
    "    f\"eta={params_stage1['learning_rate']:.3f}, \"\n",
    "    f\"scale_pos_weight={params_stage1['scale_pos_weight']:.3f}\")\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_proc, y), 1):\n",
    "    log(f\"Fold {fold}/{n_splits}\")\n",
    "    X_tr, X_val = X_proc[tr_idx], X_proc[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test_proc)\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params_stage1,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200,\n",
    "    )\n",
    "\n",
    "    oof_pred_stage1[val_idx] = booster.predict(dval, iteration_range=(0, booster.best_iteration + 1))\n",
    "    test_pred_stage1_folds.append(\n",
    "        booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1))\n",
    "    )\n",
    "\n",
    "auc_stage1 = roc_auc_score(y, oof_pred_stage1)\n",
    "log(f\"Stage 1 OOF ROC-AUC: {auc_stage1:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "139ec92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:25:52] Computing Stage 1 logits for OOF and test...\n",
      "[23:25:52] Training Stage 2 meta model with base_margin (logit boosting)...\n",
      "[23:25:52] Params: max_depth=4, min_child_weight=10, eta=0.030\n",
      "[23:25:52] Stage 2 Fold 1/5\n",
      "[0]\ttrain-auc:0.92172\tvalid-auc:0.92300\n",
      "[0]\ttrain-auc:0.92172\tvalid-auc:0.92300\n",
      "[100]\ttrain-auc:0.92208\tvalid-auc:0.92276\n",
      "[100]\ttrain-auc:0.92208\tvalid-auc:0.92276\n",
      "[23:25:58] Stage 2 Fold 2/5\n",
      "[0]\ttrain-auc:0.92179\tvalid-auc:0.92274\n",
      "[23:25:58] Stage 2 Fold 2/5\n",
      "[0]\ttrain-auc:0.92179\tvalid-auc:0.92274\n",
      "[100]\ttrain-auc:0.92207\tvalid-auc:0.92253\n",
      "[100]\ttrain-auc:0.92207\tvalid-auc:0.92253\n",
      "[23:26:04] Stage 2 Fold 3/5\n",
      "[0]\ttrain-auc:0.92224\tvalid-auc:0.92093\n",
      "[23:26:04] Stage 2 Fold 3/5\n",
      "[0]\ttrain-auc:0.92224\tvalid-auc:0.92093\n",
      "[100]\ttrain-auc:0.92254\tvalid-auc:0.92068\n",
      "[100]\ttrain-auc:0.92254\tvalid-auc:0.92068\n",
      "[23:26:10] Stage 2 Fold 4/5\n",
      "[0]\ttrain-auc:0.92206\tvalid-auc:0.92163\n",
      "[23:26:10] Stage 2 Fold 4/5\n",
      "[0]\ttrain-auc:0.92206\tvalid-auc:0.92163\n",
      "[99]\ttrain-auc:0.92237\tvalid-auc:0.92136\n",
      "[99]\ttrain-auc:0.92237\tvalid-auc:0.92136\n",
      "[23:26:16] Stage 2 Fold 5/5\n",
      "[0]\ttrain-auc:0.92208\tvalid-auc:0.92157\n",
      "[23:26:16] Stage 2 Fold 5/5\n",
      "[0]\ttrain-auc:0.92208\tvalid-auc:0.92157\n",
      "[100]\ttrain-auc:0.92242\tvalid-auc:0.92134\n",
      "[100]\ttrain-auc:0.92242\tvalid-auc:0.92134\n",
      "[23:26:21] Stage 2 OOF ROC-AUC: 0.92197\n",
      "[23:26:21] Stage 2 OOF ROC-AUC: 0.92197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Stage 2: Meta XGBoost boosting over Stage‑1 logits (base_margin trick)\n",
    "\n",
    "def prob_to_logit(p: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "log(\"Computing Stage 1 logits for OOF and test...\")\n",
    "\n",
    "logits_oof_stage1 = prob_to_logit(oof_pred_stage1)\n",
    "test_pred_stage1_folds = np.vstack(test_pred_stage1_folds)  # (n_splits, n_test)\n",
    "logits_test_stage1_folds = prob_to_logit(test_pred_stage1_folds)\n",
    "\n",
    "oof_pred_stage2 = np.zeros(X_proc.shape[0])\n",
    "test_pred_stage2_folds = []\n",
    "\n",
    "params_stage2 = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_lambda\": 1.5,\n",
    "    \"reg_alpha\": 0.2,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": RANDOM_STATE + 1,\n",
    "}\n",
    "\n",
    "log(\"Training Stage 2 meta model with base_margin (logit boosting)...\")\n",
    "log(f\"Params: max_depth={params_stage2['max_depth']}, \"\n",
    "    f\"min_child_weight={params_stage2['min_child_weight']}, \"\n",
    "    f\"eta={params_stage2['learning_rate']:.3f}\")\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_proc, y), 1):\n",
    "    log(f\"Stage 2 Fold {fold}/{n_splits}\")\n",
    "    X_tr, X_val = X_proc[tr_idx], X_proc[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test_proc)\n",
    "\n",
    "    # base_margin = Stage‑1 logits\n",
    "    dtrain.set_base_margin(logits_oof_stage1[tr_idx])\n",
    "    dval.set_base_margin(logits_oof_stage1[val_idx])\n",
    "    dtest.set_base_margin(logits_test_stage1_folds[fold - 1])\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n",
    "\n",
    "    booster_meta = xgb.train(\n",
    "        params_stage2,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200,\n",
    "    )\n",
    "\n",
    "    oof_pred_stage2[val_idx] = booster_meta.predict(\n",
    "        dval, iteration_range=(0, booster_meta.best_iteration + 1)\n",
    "    )\n",
    "    test_pred_stage2_folds.append(\n",
    "        booster_meta.predict(dtest, iteration_range=(0, booster_meta.best_iteration + 1))\n",
    "    )\n",
    "\n",
    "auc_stage2 = roc_auc_score(y, oof_pred_stage2)\n",
    "log(f\"Stage 2 OOF ROC-AUC: {auc_stage2:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52d27276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:26:21] Searching F1-optimal threshold on Stage 2 OOF probabilities...\n",
      "[23:26:23] Best threshold on OOF: 0.200 (F1=0.9430)\n",
      "\n",
      "==================================================\n",
      "=== Stage 1 (base model) ===\n",
      "[23:26:23] Best threshold on OOF: 0.200 (F1=0.9430)\n",
      "\n",
      "==================================================\n",
      "=== Stage 1 (base model) ===\n",
      "ROC-AUC : 0.92197\n",
      "Accuracy: 0.90537\n",
      "F1      : 0.94304\n",
      "LogLoss : 0.32496\n",
      "\n",
      "=== Stage 2 (meta boosted) ===\n",
      "ROC-AUC : 0.92197\n",
      "Accuracy: 0.90538\n",
      "F1      : 0.94304\n",
      "LogLoss : 0.32508\n",
      "\n",
      "✗ Stage 2 improvement: -0.00000 AUC\n",
      "==================================================\n",
      "ROC-AUC : 0.92197\n",
      "Accuracy: 0.90537\n",
      "F1      : 0.94304\n",
      "LogLoss : 0.32496\n",
      "\n",
      "=== Stage 2 (meta boosted) ===\n",
      "ROC-AUC : 0.92197\n",
      "Accuracy: 0.90538\n",
      "F1      : 0.94304\n",
      "LogLoss : 0.32508\n",
      "\n",
      "✗ Stage 2 improvement: -0.00000 AUC\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) Compare Stage 1 vs Stage 2 and find optimal F1 threshold\n",
    "\n",
    "def evaluate_at_threshold(y_true, proba, thr: float) -> dict:\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, pred),\n",
    "        \"f1\": f1_score(y_true, pred),\n",
    "        \"logloss\": log_loss(y_true, proba),\n",
    "    }\n",
    "\n",
    "thr_grid = np.linspace(0.1, 0.9, 17)\n",
    "\n",
    "log(\"Searching F1-optimal threshold on Stage 2 OOF probabilities...\")\n",
    "best_thr = 0.5\n",
    "best_f1 = -1.0\n",
    "for thr in thr_grid:\n",
    "    metrics = evaluate_at_threshold(y, oof_pred_stage2, thr)\n",
    "    if metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"f1\"]\n",
    "        best_thr = thr\n",
    "\n",
    "log(f\"Best threshold on OOF: {best_thr:.3f} (F1={best_f1:.4f})\")\n",
    "\n",
    "metrics1 = evaluate_at_threshold(y, oof_pred_stage1, best_thr)\n",
    "metrics2 = evaluate_at_threshold(y, oof_pred_stage2, best_thr)\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('=== Stage 1 (base model) ===')\n",
    "print(f\"ROC-AUC : {roc_auc_score(y, oof_pred_stage1):.5f}\")\n",
    "print(f\"Accuracy: {metrics1['accuracy']:.5f}\")\n",
    "print(f\"F1      : {metrics1['f1']:.5f}\")\n",
    "print(f\"LogLoss : {metrics1['logloss']:.5f}\")\n",
    "\n",
    "print('\\n=== Stage 2 (meta boosted) ===')\n",
    "print(f\"ROC-AUC : {roc_auc_score(y, oof_pred_stage2):.5f}\")\n",
    "print(f\"Accuracy: {metrics2['accuracy']:.5f}\")\n",
    "print(f\"F1      : {metrics2['f1']:.5f}\")\n",
    "print(f\"LogLoss : {metrics2['logloss']:.5f}\")\n",
    "\n",
    "improvement = auc_stage2 - auc_stage1\n",
    "print(f\"\\n{'✓' if improvement > 0 else '✗'} Stage 2 improvement: {improvement:+.5f} AUC\")\n",
    "print('='*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0549bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:26:23] Using Stage 1 base predictions for submission.\n",
      "[23:26:23] ✓ Saved submission to: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback/submissions/loan_meta_copilot_20251120_232623.csv\n",
      "[23:26:23] ✓ Saved submission to: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback/submissions/loan_meta_copilot_20251120_232623.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8) Build final test predictions and submission file\n",
    "\n",
    "test_pred_stage1 = test_pred_stage1_folds.mean(axis=0)\n",
    "test_pred_stage2 = np.mean(np.vstack(test_pred_stage2_folds), axis=0)\n",
    "\n",
    "# If meta model improved ROC-AUC, we use Stage 2; otherwise fall back to Stage 1\n",
    "use_stage2 = auc_stage2 >= auc_stage1\n",
    "final_test_proba = test_pred_stage2 if use_stage2 else test_pred_stage1\n",
    "\n",
    "log(f\"Using {'Stage 2 meta' if use_stage2 else 'Stage 1 base'} predictions for submission.\")\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if id_col is not None:\n",
    "    sub[id_col] = test_df[id_col]\n",
    "else:\n",
    "    sub[\"id\"] = np.arange(len(test_df))\n",
    "\n",
    "sub[target_col] = final_test_proba\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "sub_path = Path(f\"submissions/loan_meta_copilot_{ts}.csv\")\n",
    "sub_path.parent.mkdir(exist_ok=True)\n",
    "sub.to_csv(sub_path, index=False)\n",
    "log(f\"✓ Saved submission to: {sub_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
