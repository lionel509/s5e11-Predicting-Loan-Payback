{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef4ef5d",
   "metadata": {},
   "source": [
    "# Loan Payback — Fast Meta Ensemble (AUC-first)\n",
    "\n",
    "Goals:\n",
    "- Reach >= 0.92 AUC quickly; once hit, run 1–2 more seeds then stop.\n",
    "- Push toward 0.93 AUC via: threshold sweep on meta probs, isotonic calibration + threshold, and a shallow Meta-XGB (depth 3–4).\n",
    "- Keep training cycles lean (no 100+ seed runs).\n",
    "\n",
    "Outputs:\n",
    "- Best CV AUC and configuration.\n",
    "- Optional calibrated predictions and threshold.\n",
    "- Submission file under `submissions/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567972bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback\n",
      "Files exist? train=True test=True sample=True\n"
     ]
    }
   ],
   "source": [
    "# Imports & quick checks\n",
    "import os, sys, json, math, warnings, gc, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# Try XGBoost for meta\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"xgboost not installed; meta-XGB will be skipped.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_BASE = 42\n",
    "np.random.seed(RANDOM_BASE)\n",
    "random.seed(RANDOM_BASE)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'Data'\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH = DATA_DIR / 'test.csv'\n",
    "SAMPLE_SUB_PATH = DATA_DIR / 'sample_submission.csv'\n",
    "SUB_DIR = ROOT / 'submissions'\n",
    "SUB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f'ROOT: {ROOT}')\n",
    "print(f'Files exist? train={TRAIN_PATH.exists()} test={TEST_PATH.exists()} sample={SAMPLE_SUB_PATH.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config & target/id detection\n",
    "TARGET_CANDIDATES = ['target','TARGET','label','Label','default','is_default','loan_status','loan_repaid']\n",
    "ID_CANDIDATES = ['id','ID','loan_id','Loan_ID']\n",
    "\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    cols = df.columns.tolist()\n",
    "    id_col = None\n",
    "    for c in ID_CANDIDATES:\n",
    "        if c in cols:\n",
    "            id_col = c\n",
    "            break\n",
    "    \n",
    "    target_col = None\n",
    "    for c in TARGET_CANDIDATES:\n",
    "        if c in cols:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        # Heuristic: last column if binary-like\n",
    "        last = cols[-1]\n",
    "        if df[last].dropna().isin([0,1]).mean() > 0.9:\n",
    "            target_col = last\n",
    "    return id_col, target_col\n",
    "\n",
    "# Peek few rows to detect columns\n",
    "preview = pd.read_csv(TRAIN_PATH, nrows=100)\n",
    "ID_COL, TARGET = detect_columns(preview)\n",
    "print('Detected ID_COL=', ID_COL, ' TARGET=', TARGET)\n",
    "assert TARGET is not None, 'Target column not detected; please set TARGET manually.'\n",
    "\n",
    "# Load full data\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH) if TEST_PATH.exists() else None\n",
    "print(train.shape, 'train shape')\n",
    "if test is not None:\n",
    "    print(test.shape, 'test shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/target split and preprocessing pipeline\n",
    "\n",
    "y = train[TARGET].astype(int)\n",
    "X = train.drop(columns=[TARGET] + ([ID_COL] if ID_COL else []))\n",
    "X_test = None\n",
    "if 'test' in globals() and test is not None:\n",
    "    X_test = test.drop(columns=[ID_COL] if ID_COL else [])\n",
    "\n",
    "num_cols = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "print(f'Numeric: {len(num_cols)}, Categorical: {len(cat_cols)}')\n",
    "\n",
    "numeric_tf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_tf, num_cols),\n",
    "    ('cat', categorical_tf, cat_cols)\n",
    "])\n",
    "\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == 'logreg':\n",
    "        clf = LogisticRegression(max_iter=2000, n_jobs=None, C=1.0, solver='lbfgs')\n",
    "    elif name == 'rf':\n",
    "        clf = RandomForestClassifier(n_estimators=250, max_depth=None, n_jobs=-1, random_state=0)\n",
    "    elif name == 'gb':\n",
    "        clf = GradientBoostingClassifier(random_state=0)\n",
    "    else:\n",
    "        raise ValueError('Unknown base model')\n",
    "    return Pipeline(steps=[('prep', preprocess), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV, metrics, threshold sweep, and isotonic calibration utils\n",
    "def get_cv(n_splits=5, seed=42):\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "def threshold_sweep(y_true, prob, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best = {'threshold': None, 'f1': -1, 'precision': None, 'recall': None}\n",
    "    for t in thresholds:\n",
    "        pred = (prob >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred)\n",
    "        if f1 > best['f1']:\n",
    "            # compute precision & recall via confusion matrix\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec = tp / (tp + fn + 1e-9)\n",
    "            best = {'threshold': float(t), 'f1': float(f1), 'precision': float(prec), 'recall': float(rec)}\n",
    "    return best\n",
    "\n",
    "def fit_isotonic(y_true, prob):\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(prob, y_true)\n",
    "    return iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base models and produce OOF meta features\n",
    "\n",
    "def train_base_models(X, y, X_test=None, seed=42, n_splits=5):\n",
    "    cv = get_cv(n_splits=n_splits, seed=seed)\n",
    "    base_names = ['logreg','rf','gb']\n",
    "    oof = np.zeros((len(X), len(base_names)))\n",
    "    test_preds = np.zeros((len(X_test), len(base_names))) if X_test is not None else None\n",
    "    aucs = {name: [] for name in base_names}\n",
    "\n",
    "    for j, name in enumerate(base_names):\n",
    "        fold_idx = 0\n",
    "        for tr_idx, va_idx in cv.split(X, y):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "            model = build_model(name)\n",
    "            # propagate seed if supported\n",
    "            if 'random_state' in model.named_steps['clf'].get_params():\n",
    "                model.set_params(**{'clf__random_state': seed + fold_idx})\n",
    "            model.fit(X_tr, y_tr)\n",
    "            p = model.predict_proba(X_va)[:,1]\n",
    "            oof[va_idx, j] = p\n",
    "            auc = roc_auc_score(y_va, p)\n",
    "            aucs[name].append(auc)\n",
    "            if X_test is not None:\n",
    "                test_preds[:, j] += model.predict_proba(X_test)[:,1] / cv.get_n_splits()\n",
    "            fold_idx += 1\n",
    "        print(f\"Base {name}: AUC per fold {np.round(aucs[name], 4)} -> mean {np.mean(aucs[name]):.4f}\")\n",
    "    return oof, test_preds, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage audit utilities\n",
    "# 1. Single-feature AUC to flag suspicious leak features.\n",
    "# 2. Temporal leakage heuristic: columns that look like aggregates (e.g., total_*, avg_*) might encode future info.\n",
    "# 3. Type casting helpers.\n",
    "\n",
    "import re\n",
    "\n",
    "LEAK_MAX_FEATURES = 40  # cap evaluation for speed\n",
    "\n",
    "def single_feature_auc_scan(df: pd.DataFrame, y: pd.Series, max_features=LEAK_MAX_FEATURES):\n",
    "    aucs = []\n",
    "    for col in df.columns[:max_features]:\n",
    "        try:\n",
    "            if df[col].nunique() < 2:\n",
    "                continue\n",
    "            vals = df[col].fillna(df[col].median() if df[col].dtype != 'O' else 'missing')\n",
    "            # For categorical -> encode label frequency\n",
    "            if vals.dtype == 'O':\n",
    "                mapping = vals.value_counts(normalize=True).to_dict()\n",
    "                enc = vals.map(mapping).astype(float)\n",
    "            else:\n",
    "                enc = vals.astype(float)\n",
    "            score = roc_auc_score(y, enc) if len(np.unique(enc)) > 1 else 0.5\n",
    "            aucs.append((col, score))\n",
    "        except Exception:\n",
    "            continue\n",
    "    aucs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return aucs\n",
    "\n",
    "AGG_PATTERNS = [r'^total_', r'^sum_', r'^avg_', r'^mean_', r'^max_', r'^min_']\n",
    "\n",
    "def looks_leaky(colname: str) -> bool:\n",
    "    for pat in AGG_PATTERNS:\n",
    "        if re.search(pat, colname):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# KS & PSI drift checks between train/test\n",
    "\n",
    "def ks_stat(train_col, test_col):\n",
    "    # dropna\n",
    "    a = pd.Series(train_col).dropna()\n",
    "    b = pd.Series(test_col).dropna()\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        stat, pval = stats.ks_2samp(a, b)\n",
    "        return stat\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# Population Stability Index for binned values\n",
    "\n",
    "def psi(train_col, test_col, buckets=10):\n",
    "    a = pd.Series(train_col).dropna()\n",
    "    b = pd.Series(test_col).dropna()\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return 0.0\n",
    "    quantiles = np.linspace(0, 1, buckets + 1)\n",
    "    cuts = a.quantile(quantiles).unique()\n",
    "    a_bins = pd.cut(a, bins=np.unique(cuts), include_lowest=True)\n",
    "    b_bins = pd.cut(b, bins=np.unique(cuts), include_lowest=True)\n",
    "    a_dist = a_bins.value_counts(normalize=True)\n",
    "    b_dist = b_bins.value_counts(normalize=True)\n",
    "    psi_val = 0.0\n",
    "    for idx in a_dist.index:\n",
    "        expected = a_dist.get(idx, 1e-6)\n",
    "        actual = b_dist.get(idx, 1e-6)\n",
    "        if expected > 0 and actual > 0:\n",
    "            psi_val += (actual - expected) * math.log(actual / expected)\n",
    "    return psi_val\n",
    "\n",
    "DRIFT_REPORT_LIMIT = 40\n",
    "\n",
    "def drift_report(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    rows = []\n",
    "    shared = [c for c in train_df.columns if c in test_df.columns]\n",
    "    for col in shared[:DRIFT_REPORT_LIMIT]:\n",
    "        try:\n",
    "            k = ks_stat(train_df[col], test_df[col])\n",
    "            p = psi(train_df[col], test_df[col])\n",
    "            rows.append({'feature': col, 'ks': k, 'psi': p})\n",
    "        except Exception:\n",
    "            continue\n",
    "    rep = pd.DataFrame(rows)\n",
    "    if not rep.empty:\n",
    "        rep.sort_values(['ks','psi'], ascending=False, inplace=True)\n",
    "    return rep\n",
    "\n",
    "BOOL_LIKE = ['y','n','yes','no','true','false']\n",
    "\n",
    "def cast_types(df: pd.DataFrame):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            # bool-like\n",
    "            low = df[c].str.lower()\n",
    "            if low.isin(BOOL_LIKE).mean() > 0.9:\n",
    "                df[c] = low.map({'y':1,'yes':1,'true':1,'n':0,'no':0,'false':0}).astype('Int8')\n",
    "    return df\n",
    "\n",
    "print('Leakage & drift utilities ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply type casting, leakage audit, and drift checks\n",
    "# Must run after data load (Cell 3)\n",
    "\n",
    "assert 'train' in globals(), 'Run the data load cell first.'\n",
    "\n",
    "# 1) Type casting\n",
    "if 'ID_COL' in globals() and ID_COL:\n",
    "    train[ID_COL] = train[ID_COL].astype(str)\n",
    "    if 'test' in globals() and test is not None and ID_COL in test.columns:\n",
    "        test[ID_COL] = test[ID_COL].astype(str)\n",
    "\n",
    "train = cast_types(train)\n",
    "if 'test' in globals() and test is not None:\n",
    "    test = cast_types(test)\n",
    "\n",
    "# 2) Leakage audit (simple, top-N features)\n",
    "feat_cols = [c for c in train.columns if c not in [TARGET] + ([ID_COL] if ID_COL else [])]\n",
    "scan_df = train[feat_cols].copy()\n",
    "scan_aucs = single_feature_auc_scan(scan_df, train[TARGET], max_features=min(LEAK_MAX_FEATURES, len(feat_cols)))\n",
    "leaky = [c for (c, auc) in scan_aucs if auc >= 0.92 or auc <= 0.08 or looks_leaky(c)]\n",
    "\n",
    "if len(leaky) > 0:\n",
    "    print('Dropping suspicious leakage features:', leaky)\n",
    "    train.drop(columns=[c for c in leaky if c in train.columns], inplace=True)\n",
    "    if 'test' in globals() and test is not None:\n",
    "        test.drop(columns=[c for c in leaky if c in test.columns], inplace=True)\n",
    "else:\n",
    "    print('No leakage features flagged by simple scan.')\n",
    "\n",
    "# 3) Drift check (requires test)\n",
    "if 'test' in globals() and test is not None:\n",
    "    tr_common = train.drop(columns=[TARGET] + ([ID_COL] if ID_COL else []), errors='ignore')\n",
    "    te_common = test.drop(columns=[ID_COL] if ID_COL else [], errors='ignore')\n",
    "    rep = drift_report(tr_common, te_common)\n",
    "    display(rep.head(12))\n",
    "    # Drop worst offenders by simple rule\n",
    "    drop_drift = rep[(rep['ks'] >= 0.2) | (rep['psi'] >= 0.25)]['feature'].tolist()\n",
    "    if drop_drift:\n",
    "        print('Dropping drift-heavy features:', drop_drift)\n",
    "        train.drop(columns=[c for c in drop_drift if c in train.columns], inplace=True)\n",
    "        test.drop(columns=[c for c in drop_drift if c in test.columns], inplace=True)\n",
    "else:\n",
    "    print('Test set not available; skipping drift check.')\n",
    "\n",
    "print('Preprocessing audits complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24602756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta learner training (XGB if available, else Logistic) + isotonic calibration\n",
    "\n",
    "def train_meta(oof_feats, y, test_feats=None, seed=42, depth=3):\n",
    "    cv = get_cv(n_splits=5, seed=seed)\n",
    "    oof_meta = np.zeros(len(y))\n",
    "    test_meta = np.zeros(len(test_feats)) if test_feats is not None else None\n",
    "    fold_aucs = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(oof_feats, y)):\n",
    "        X_tr, X_va = oof_feats[tr_idx], oof_feats[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        if XGB_AVAILABLE:\n",
    "            clf = xgb.XGBClassifier(\n",
    "                max_depth=depth, n_estimators=300, learning_rate=0.05,\n",
    "                subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0, reg_alpha=0.0,\n",
    "                objective='binary:logistic', eval_metric='auc', random_state=seed+fold, tree_method='hist'\n",
    "            )\n",
    "        else:\n",
    "            clf = LogisticRegression(max_iter=2000)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        p = clf.predict_proba(X_va)[:,1]\n",
    "        oof_meta[va_idx] = p\n",
    "        fold_aucs.append(roc_auc_score(y_va, p))\n",
    "        if test_feats is not None:\n",
    "            test_meta += clf.predict_proba(test_feats)[:,1] / cv.get_n_splits()\n",
    "\n",
    "    meta_auc = roc_auc_score(y, oof_meta)\n",
    "    print(f'Meta AUC: {meta_auc:.5f}; folds {np.round(fold_aucs,4)}')\n",
    "\n",
    "    # Isotonic calibration on OOF\n",
    "    iso = fit_isotonic(y.values, oof_meta)\n",
    "    oof_meta_cal = iso.predict(oof_meta)\n",
    "    meta_auc_cal = roc_auc_score(y, oof_meta_cal)\n",
    "    print(f'Meta AUC (isotonic-calibrated): {meta_auc_cal:.5f}')\n",
    "    test_meta_cal = iso.predict(test_meta) if test_meta is not None else None\n",
    "\n",
    "    best_thr_raw = threshold_sweep(y.values, oof_meta)\n",
    "    best_thr_cal = threshold_sweep(y.values, oof_meta_cal)\n",
    "    print('Best threshold (raw):', best_thr_raw)\n",
    "    print('Best threshold (cal):', best_thr_cal)\n",
    "\n",
    "    return {\n",
    "        'oof': oof_meta, 'oof_cal': oof_meta_cal, 'auc': meta_auc, 'auc_cal': meta_auc_cal,\n",
    "        'test': test_meta, 'test_cal': test_meta_cal, 'best_thr_raw': best_thr_raw, 'best_thr_cal': best_thr_cal\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training orchestrator with early exit once AUC >=0.92 (then 1–2 extra seeds) aiming for 0.93\n",
    "\n",
    "def run_training(seeds=list(range(42, 52)), extra_runs_after_hit=2, target_auc=0.92, aspire_auc=0.93):\n",
    "    results = []\n",
    "    hit = False\n",
    "    extra = 0\n",
    "    best = None\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n==== Seed {seed} ({i+1}/{len(seeds)}) ====\")\n",
    "        oof, test_feats, base_aucs = train_base_models(X, y, X_test, seed=seed, n_splits=5)\n",
    "        meta = train_meta(oof, y, test_feats, seed=seed, depth=4)\n",
    "        auc = float(meta['auc'])\n",
    "        auc_cal = float(meta['auc_cal'])\n",
    "        record = {\n",
    "            'seed': seed, 'auc': auc, 'auc_cal': auc_cal,\n",
    "            'best_thr_raw': meta['best_thr_raw'], 'best_thr_cal': meta['best_thr_cal']\n",
    "        }\n",
    "        results.append(record)\n",
    "        if (best is None) or (auc_cal > best['auc_cal']):\n",
    "            best = {**record, 'oof_cal': meta['oof_cal'], 'test_cal': meta['test_cal']}\n",
    "\n",
    "        if auc_cal >= target_auc and not hit:\n",
    "            hit = True\n",
    "            extra = extra_runs_after_hit\n",
    "            print(f'Hit target AUC {target_auc:.2f}. Will run {extra} extra seeds then stop.')\n",
    "        elif hit and extra > 0:\n",
    "            extra -= 1\n",
    "            if extra == 0:\n",
    "                print('Extra runs finished after hit. Stopping.')\n",
    "                break\n",
    "        if auc_cal >= aspire_auc:\n",
    "            print(f'Reached aspirational AUC {aspire_auc:.2f}. Stopping early!')\n",
    "            break\n",
    "        if i >= 9:\n",
    "            print('Reached max seed cap (10). Stopping.')\n",
    "            break\n",
    "    return pd.DataFrame(results), best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute training\n",
    "SEEDS = [42,43,44,45,46,47,48,49]\n",
    "results_df, best = run_training(seeds=SEEDS, extra_runs_after_hit=2, target_auc=0.92, aspire_auc=0.93)\n",
    "display(results_df.sort_values('auc_cal', ascending=False).head())\n",
    "print('Best:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build submission (if test and sample_submission available)\n",
    "if 'test' in globals() and test is not None and SAMPLE_SUB_PATH.exists() and best is not None:\n",
    "    sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "    sub_id_col = sub.columns[0]\n",
    "    sub_target_col = sub.columns[1] if len(sub.columns) > 1 else (TARGET if TARGET is not None else 'target')\n",
    "    if 'ID_COL' in globals() and ID_COL and sub_id_col != ID_COL and ID_COL in test.columns:\n",
    "        sub[sub_id_col] = test[ID_COL].values\n",
    "    preds = best['test_cal'] if best.get('test_cal') is not None else None\n",
    "    if preds is not None:\n",
    "        sub[sub_target_col] = preds\n",
    "        timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "        out_path = SUB_DIR / f'advanced_submission_META_optimized_{timestamp}.csv'\n",
    "        sub.to_csv(out_path, index=False)\n",
    "        print('Saved submission to:', out_path)\n",
    "    else:\n",
    "        print('No test predictions available to build submission.')\n",
    "else:\n",
    "    print('Submission not created (missing test or sample submission or best).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes & next steps\n",
    "# - Consider adding LightGBM/CatBoost base models for more diversity (stacking boost).\n",
    "# - Hyperparameter refinement for XGB depth, learning_rate, n_estimators with early stopping.\n",
    "# - Feature engineering: interaction terms, target encoding (with CV), monotonic constraints.\n",
    "# - Robust leakage detection: time-based splits if timestamp exists.\n",
    "# - Advanced calibration: Platt scaling or ensemble of calibrators.\n",
    "# - Drift mitigation: reweight training samples by inverse propensity if heavy shift.\n",
    "print('Notebook complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
