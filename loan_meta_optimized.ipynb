{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "567972bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback\n",
      "Files exist? train=True test=True sample=True\n"
     ]
    }
   ],
   "source": [
    "# Imports & quick checks\n",
    "import os, sys, json, math, warnings, gc, time, random\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# Try XGBoost for meta\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"xgboost not installed; meta-XGB will be skipped.\")\n",
    "\n",
    "# Try LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"lightgbm not installed; LightGBM will be skipped.\")\n",
    "\n",
    "# Try CatBoost\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    CB_AVAILABLE = False\n",
    "    print(\"catboost not installed; CatBoost will be skipped.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_BASE = 42\n",
    "np.random.seed(RANDOM_BASE)\n",
    "random.seed(RANDOM_BASE)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'Data'\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH = DATA_DIR / 'test.csv'\n",
    "SAMPLE_SUB_PATH = DATA_DIR / 'sample_submission.csv'\n",
    "SUB_DIR = ROOT / 'submissions'\n",
    "SUB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f'ROOT: {ROOT}')\n",
    "print(f'Files exist? train={TRAIN_PATH.exists()} test={TEST_PATH.exists()} sample={SAMPLE_SUB_PATH.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "980a8c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected ID_COL= id  TARGET= loan_paid_back\n",
      "(593994, 13) train shape\n",
      "(254569, 12) test shape\n",
      "(593994, 13) train shape\n",
      "(254569, 12) test shape\n"
     ]
    }
   ],
   "source": [
    "# Config & target/id detection\n",
    "TARGET_CANDIDATES = ['target','TARGET','label','Label','default','is_default','loan_status','loan_repaid']\n",
    "ID_CANDIDATES = ['id','ID','loan_id','Loan_ID']\n",
    "\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    cols = df.columns.tolist()\n",
    "    id_col = None\n",
    "    for c in ID_CANDIDATES:\n",
    "        if c in cols:\n",
    "            id_col = c\n",
    "            break\n",
    "    \n",
    "    target_col = None\n",
    "    for c in TARGET_CANDIDATES:\n",
    "        if c in cols:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        # Heuristic: last column if binary-like\n",
    "        last = cols[-1]\n",
    "        if df[last].dropna().isin([0,1]).mean() > 0.9:\n",
    "            target_col = last\n",
    "    return id_col, target_col\n",
    "\n",
    "# Peek few rows to detect columns\n",
    "preview = pd.read_csv(TRAIN_PATH, nrows=100)\n",
    "ID_COL, TARGET = detect_columns(preview)\n",
    "print('Detected ID_COL=', ID_COL, ' TARGET=', TARGET)\n",
    "assert TARGET is not None, 'Target column not detected; please set TARGET manually.'\n",
    "\n",
    "# Load full data\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH) if TEST_PATH.exists() else None\n",
    "print(train.shape, 'train shape')\n",
    "if test is not None:\n",
    "    print(test.shape, 'test shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "68b1ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 5 numeric, 6 categorical\n",
      "\n",
      "üìö Loading original dataset for stable target encoding...\n",
      "  ‚ö†Ô∏è  Original dataset not found. Will use train-only target encoding.\n",
      "\n",
      "üéØ Creating Original Mean Features (High ROI, Low Overfitting)\n",
      "  ‚Üí Fallback to train-only target encoding\n",
      "  ‚úì Created orig_mean_loan_purpose (K-Fold Target Encoding)\n",
      "  ‚úì Created orig_mean_loan_purpose (K-Fold Target Encoding)\n",
      "  ‚úì Created orig_mean_employment_status (K-Fold Target Encoding)\n",
      "  ‚úì Created orig_mean_employment_status (K-Fold Target Encoding)\n",
      "  ‚úì Created orig_mean_education_level (K-Fold Target Encoding)\n",
      "\n",
      "üìà Creating Log Transforms\n",
      "  ‚úì Created annual_income_log\n",
      "  ‚úì Created loan_amount_log\n",
      "  ‚úì Created debt_to_income_ratio_log\n",
      "\n",
      "üí∞ Creating Domain-Specific Financial Features\n",
      "  ‚úì Created risk_score (domain formula)\n",
      "  ‚úì Created affordability (remaining income / loan)\n",
      "\n",
      "üî¢ Creating Minimal Binning\n",
      "  ‚úì Created credit_score_bin (5 bins)\n",
      "  ‚úì Created orig_mean_education_level (K-Fold Target Encoding)\n",
      "\n",
      "üìà Creating Log Transforms\n",
      "  ‚úì Created annual_income_log\n",
      "  ‚úì Created loan_amount_log\n",
      "  ‚úì Created debt_to_income_ratio_log\n",
      "\n",
      "üí∞ Creating Domain-Specific Financial Features\n",
      "  ‚úì Created risk_score (domain formula)\n",
      "  ‚úì Created affordability (remaining income / loan)\n",
      "\n",
      "üî¢ Creating Minimal Binning\n",
      "  ‚úì Created credit_score_bin (5 bins)\n",
      "\n",
      "üìä Feature Engineering Summary:\n",
      "   Total features: 21 (15 numeric, 6 categorical)\n",
      "   Gain: +10 features\n",
      "\n",
      "üìä Feature Engineering Summary:\n",
      "   Total features: 21 (15 numeric, 6 categorical)\n",
      "   Gain: +10 features\n"
     ]
    }
   ],
   "source": [
    "# STABILIZED Feature Engineering (s5e11 Reference Strategy)\n",
    "\n",
    "y = train[TARGET].astype(int)\n",
    "X = train.drop(columns=[TARGET] + ([ID_COL] if ID_COL else []))\n",
    "X_test = None\n",
    "if 'test' in globals() and test is not None:\n",
    "    X_test = test.drop(columns=[ID_COL] if ID_COL else [])\n",
    "\n",
    "num_cols_orig = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols_orig]\n",
    "\n",
    "print(f'Original: {len(num_cols_orig)} numeric, {len(cat_cols)} categorical')\n",
    "\n",
    "# 0. LOAD ORIGINAL DATASET (The Stabilizer - prevents overfitting to train split)\n",
    "print('\\nüìö Loading original dataset for stable target encoding...')\n",
    "ORIG_PATH = DATA_DIR / 'loan_dataset_20000.csv'\n",
    "orig = None\n",
    "orig_target_col = 'loan_paid_back'  # Common target name in original dataset\n",
    "\n",
    "try:\n",
    "    if ORIG_PATH.exists():\n",
    "        orig = pd.read_csv(ORIG_PATH)\n",
    "        print(f'  ‚úì Loaded original dataset: {orig.shape}')\n",
    "    else:\n",
    "        # Try alternative paths\n",
    "        alt_paths = [\n",
    "            ROOT / 'loan_dataset_20000.csv',\n",
    "            Path('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n",
    "        ]\n",
    "        for alt_path in alt_paths:\n",
    "            if alt_path.exists():\n",
    "                orig = pd.read_csv(alt_path)\n",
    "                print(f'  ‚úì Loaded original dataset from {alt_path}: {orig.shape}')\n",
    "                break\n",
    "        \n",
    "        if orig is None:\n",
    "            print('  ‚ö†Ô∏è  Original dataset not found. Will use train-only target encoding.')\n",
    "except Exception as e:\n",
    "    print(f'  ‚ö†Ô∏è  Could not load original dataset: {e}')\n",
    "    print('  ‚Üí Using train-only target encoding (may overfit slightly)')\n",
    "    orig = None\n",
    "\n",
    "# 1. ORIGINAL MEAN FEATURES (Stable Target Encoding from orig dataset)\n",
    "print('\\nüéØ Creating Original Mean Features (High ROI, Low Overfitting)')\n",
    "ORIG_MEAN_COLS = ['loan_purpose', 'grade', 'employment_status', 'education_level']\n",
    "\n",
    "if orig is not None and orig_target_col in orig.columns:\n",
    "    # Use original dataset means (more stable, less overfit)\n",
    "    for col in ORIG_MEAN_COLS:\n",
    "        if col in X.columns and col in orig.columns:\n",
    "            orig_means = orig.groupby(col)[orig_target_col].mean()\n",
    "            X[f'orig_mean_{col}'] = X[col].map(orig_means).fillna(orig[orig_target_col].mean())\n",
    "            if X_test is not None and col in X_test.columns:\n",
    "                X_test[f'orig_mean_{col}'] = X_test[col].map(orig_means).fillna(orig[orig_target_col].mean())\n",
    "            print(f'  ‚úì Created orig_mean_{col} (stable from original dataset)')\n",
    "else:\n",
    "    # Fallback: use train-only smoothed target encoding\n",
    "    print('  ‚Üí Fallback to train-only target encoding')\n",
    "    global_mean = y.mean()\n",
    "    smoothing_k = 50\n",
    "    \n",
    "    for col in ORIG_MEAN_COLS:\n",
    "        if col in X.columns:\n",
    "            # Initialize column\n",
    "            X[f'orig_mean_{col}'] = np.nan\n",
    "            \n",
    "            # 1. Compute Out-of-Fold means for Train to prevent leakage\n",
    "            from sklearn.model_selection import KFold\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            for tr_idx, va_idx in kf.split(X, y):\n",
    "                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "                y_tr = y.iloc[tr_idx]\n",
    "                \n",
    "                # Create temporary dataframe with feature and target for groupby\n",
    "                temp_df = pd.DataFrame({col: X_tr[col], 'target_temp': y_tr})\n",
    "                stats = temp_df.groupby(col)['target_temp'].agg(['mean', 'count'])\n",
    "                # Smoothed mean formula\n",
    "                stats['smooth'] = (stats['mean'] * stats['count'] + global_mean * smoothing_k) / (stats['count'] + smoothing_k)\n",
    "                \n",
    "                # Map to validation set\n",
    "                X.iloc[va_idx, X.columns.get_loc(f'orig_mean_{col}')] = X_va[col].map(stats['smooth'])\n",
    "            \n",
    "            # Fill NaNs in train (categories seen in val but not in train fold)\n",
    "            X[f'orig_mean_{col}'] = X[f'orig_mean_{col}'].fillna(global_mean)\n",
    "\n",
    "            # 2. Compute Whole-Train means for Test\n",
    "            if X_test is not None and col in X_test.columns:\n",
    "                temp_df_all = pd.DataFrame({col: X[col], 'target_temp': y})\n",
    "                stats_all = temp_df_all.groupby(col)['target_temp'].agg(['mean', 'count'])\n",
    "                stats_all['smooth'] = (stats_all['mean'] * stats_all['count'] + global_mean * smoothing_k) / (stats_all['count'] + smoothing_k)\n",
    "                X_test[f'orig_mean_{col}'] = X_test[col].map(stats_all['smooth']).fillna(global_mean)\n",
    "            \n",
    "            print(f'  ‚úì Created orig_mean_{col} (K-Fold Target Encoding)')\n",
    "\n",
    "# 2. LOG TRANSFORMS\n",
    "print('\\nüìà Creating Log Transforms')\n",
    "LOG_COLS = ['annual_income', 'loan_amount', 'debt_to_income_ratio']\n",
    "for col in LOG_COLS:\n",
    "    if col in num_cols_orig:\n",
    "        X[f'{col}_log'] = np.log1p(X[col].clip(lower=0))\n",
    "        if X_test is not None:\n",
    "            X_test[f'{col}_log'] = np.log1p(X_test[col].clip(lower=0))\n",
    "        print(f'  ‚úì Created {col}_log')\n",
    "\n",
    "# 3. DOMAIN-SPECIFIC FINANCIAL FORMULAS (Replace Generic Interactions)\n",
    "print('\\nüí∞ Creating Domain-Specific Financial Features')\n",
    "\n",
    "# Risk Score: Weighted combination of key risk indicators\n",
    "if all(c in num_cols_orig for c in ['debt_to_income_ratio', 'credit_score', 'interest_rate']):\n",
    "    X['risk_score'] = (\n",
    "        X['debt_to_income_ratio'] * 40 + \n",
    "        ((1 - X['credit_score'] / 850) * 30) + \n",
    "        (X['interest_rate'] * 2)\n",
    "    )\n",
    "    if X_test is not None:\n",
    "        X_test['risk_score'] = (\n",
    "            X_test['debt_to_income_ratio'] * 40 + \n",
    "            ((1 - X_test['credit_score'] / 850) * 30) + \n",
    "            (X_test['interest_rate'] * 2)\n",
    "        )\n",
    "    print('  ‚úì Created risk_score (domain formula)')\n",
    "\n",
    "# Affordability: Remaining income after debt relative to loan size\n",
    "if all(c in num_cols_orig for c in ['annual_income', 'debt_to_income_ratio', 'loan_amount']):\n",
    "    X['affordability'] = (\n",
    "        X['annual_income'] * (1 - X['debt_to_income_ratio'])\n",
    "    ) / (X['loan_amount'] + 1)\n",
    "    if X_test is not None:\n",
    "        X_test['affordability'] = (\n",
    "            X_test['annual_income'] * (1 - X_test['debt_to_income_ratio'])\n",
    "        ) / (X_test['loan_amount'] + 1)\n",
    "    print('  ‚úì Created affordability (remaining income / loan)')\n",
    "\n",
    "# Loan to Income Ratio\n",
    "if 'loan_amount' in num_cols_orig and 'annual_income' in num_cols_orig:\n",
    "    X['loan_to_income'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "    if X_test is not None:\n",
    "        X_test['loan_to_income'] = X_test['loan_amount'] / (X_test['annual_income'] + 1)\n",
    "\n",
    "# 4. MINIMAL BINNING (Only for critical features with known non-linear patterns)\n",
    "print('\\nüî¢ Creating Minimal Binning')\n",
    "if 'credit_score' in num_cols_orig:\n",
    "    # Using qcut for train to get equal sized bins\n",
    "    X['credit_score_bin'] = pd.qcut(X['credit_score'], q=5, labels=False, duplicates='drop')\n",
    "    \n",
    "    if X_test is not None:\n",
    "        # For test, we must use the cuts defined by train, or approximations.\n",
    "        # Here we approximate using quantiles from train to ensure consistency\n",
    "        # (Note: In a strict pipeline, you would save the bin edges from pd.qcut)\n",
    "        quantiles = X['credit_score'].quantile(np.linspace(0, 1, 6)).unique()\n",
    "        X_test['credit_score_bin'] = pd.cut(X_test['credit_score'], bins=quantiles, labels=False, include_lowest=True).fillna(2)\n",
    "    print('  ‚úì Created credit_score_bin (5 bins)')\n",
    "\n",
    "# Global fill for Sklearn models\n",
    "X = X.fillna(0)\n",
    "if X_test is not None:\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "# Update column lists\n",
    "num_cols = X.select_dtypes(include=['number','float','int','Int8','Int16','Int32','Int64']).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f'\\nüìä Feature Engineering Summary:')\n",
    "print(f'   Total features: {X.shape[1]} ({len(num_cols)} numeric, {len(cat_cols)} categorical)')\n",
    "print(f'   Gain: +{X.shape[1] - (len(num_cols_orig) + len(cat_cols))} features')\n",
    "\n",
    "# Simplified preprocessing\n",
    "numeric_tf = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_tf, num_cols),\n",
    "    ('cat', categorical_tf, cat_cols)\n",
    "])\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == 'xgb' and XGB_AVAILABLE:\n",
    "        return 'xgb_raw'\n",
    "    elif name == 'lgb' and LGB_AVAILABLE:\n",
    "        return 'lgb_raw'\n",
    "    elif name == 'cb' and CB_AVAILABLE:\n",
    "        return 'cb_raw'\n",
    "    else:\n",
    "        raise ValueError(f'Model {name} not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8dbee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV, metrics, threshold sweep, and isotonic calibration utils\n",
    "def get_cv(n_splits=5, seed=42):\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "def threshold_sweep(y_true, prob, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best = {'threshold': None, 'f1': -1, 'precision': None, 'recall': None}\n",
    "    for t in thresholds:\n",
    "        pred = (prob >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred)\n",
    "        if f1 > best['f1']:\n",
    "            # compute precision & recall via confusion matrix\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec = tp / (tp + fn + 1e-9)\n",
    "            best = {'threshold': float(t), 'f1': float(f1), 'precision': float(prec), 'recall': float(rec)}\n",
    "    return best\n",
    "\n",
    "def fit_isotonic(y_true, prob):\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(prob, y_true)\n",
    "    return iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "476f51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress tracking ready ‚úì\n"
     ]
    }
   ],
   "source": [
    "# üìä Performance tracking utilities\n",
    "\n",
    "def print_progress_bar(current, target=0.93, width=50):\n",
    "    \"\"\"Visual progress bar toward 93% AUC\"\"\"\n",
    "    min_val = 0.90\n",
    "    max_val = 0.94\n",
    "    progress = (current - min_val) / (max_val - min_val)\n",
    "    filled = int(width * progress)\n",
    "    bar = '‚ñà' * filled + '‚ñë' * (width - filled)\n",
    "    pct = current * 100\n",
    "    target_pct = target * 100\n",
    "    \n",
    "    print(f'\\nüìä Progress to {target_pct:.1f}%:')\n",
    "    print(f'[{bar}] {pct:.3f}%')\n",
    "    \n",
    "    if current >= target:\n",
    "        print('üéâ TARGET ACHIEVED! üéâ')\n",
    "    else:\n",
    "        gap = (target - current) * 100\n",
    "        print(f'Gap: {gap:.3f} pp')\n",
    "\n",
    "def compare_techniques(base_auc, l2_auc, l3_auc, cal_auc):\n",
    "    \"\"\"Show incremental gains from each technique\"\"\"\n",
    "    print(f'\\nüìà TECHNIQUE BREAKDOWN:')\n",
    "    print(f'   Base (L1):       {base_auc:.5f}')\n",
    "    print(f'   + L2 Meta:       {l2_auc:.5f}  (+{(l2_auc-base_auc)*100:.3f} pp)')\n",
    "    print(f'   + L3 Pseudo:     {l3_auc:.5f}  (+{(l3_auc-l2_auc)*100:.3f} pp)')\n",
    "    print(f'   + Calibration:   {cal_auc:.5f}  (+{(cal_auc-l3_auc)*100:.3f} pp)')\n",
    "    print(f'   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')\n",
    "    print(f'   TOTAL GAIN:      +{(cal_auc-base_auc)*100:.3f} pp')\n",
    "    \n",
    "print('Progress tracking ready ‚úì')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c434c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Import boosting libraries with error handling\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CB_AVAILABLE = False\n",
    "\n",
    "def get_cv(n_splits=5, seed=42):\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "# L1 Base Models: SIMPLIFIED Strong Gradient Boosters (XGB + LGB + CB)\n",
    "def train_base_models(X, y, X_test=None, seed=42, n_splits=5):\n",
    "    cv = get_cv(n_splits=n_splits, seed=seed)\n",
    "    \n",
    "    # Use ONLY the 3 strongest gradient boosters (no RF, no MLP)\n",
    "    base_models_config = []\n",
    "    \n",
    "    if XGB_AVAILABLE:\n",
    "        base_models_config.append(('xgb', {\n",
    "            'n_estimators': 1500, 'learning_rate': 0.015, 'max_depth': 8,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "            'reg_lambda': 3.0, 'reg_alpha': 0.8, 'min_child_weight': 2,\n",
    "            'tree_method': 'hist', 'n_jobs': -1,\n",
    "            'enable_categorical': True,\n",
    "        }))\n",
    "    \n",
    "    if LGB_AVAILABLE:\n",
    "        base_models_config.append(('lgb', {\n",
    "            'n_estimators': 1500, 'learning_rate': 0.015, 'max_depth': 10, 'num_leaves': 255,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "            'reg_lambda': 3.0, 'reg_alpha': 0.6, 'min_child_samples': 20,\n",
    "            'verbose': -1, 'n_jobs': -1, \n",
    "            # force_col_wise is often faster on CPU, ensure it doesn't conflict with GPU params if used\n",
    "            'force_col_wise': True \n",
    "        }))\n",
    "    \n",
    "    if CB_AVAILABLE:\n",
    "        base_models_config.append(('cb', {\n",
    "            'iterations': 1500, 'learning_rate': 0.015, 'depth': 8,\n",
    "            'l2_leaf_reg': 5, 'border_count': 254, 'min_data_in_leaf': 10,\n",
    "            'verbose': 0, 'thread_count': -1,\n",
    "            'allow_writing_files': False \n",
    "        }))\n",
    "    \n",
    "    if not base_models_config:\n",
    "        raise ValueError('No gradient boosters available! Install XGBoost, LightGBM, or CatBoost.')\n",
    "    \n",
    "    base_names = [name for name, _ in base_models_config]\n",
    "    print(f'üöÄ Training {len(base_names)} L1 base models: {base_names}')\n",
    "    \n",
    "    # Initialize arrays\n",
    "    oof = np.zeros((len(X), len(base_names)))\n",
    "    test_preds = np.zeros((len(X_test), len(base_names))) if X_test is not None else None\n",
    "    aucs = {name: [] for name in base_names}\n",
    "\n",
    "    # Loop through models\n",
    "    for j, (name, params) in enumerate(base_models_config):\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        fold_idx = 0\n",
    "        \n",
    "        for tr_idx, va_idx in cv.split(X, y):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "            \n",
    "            # --- Model Specific Preprocessing & Training ---\n",
    "            if name == 'xgb':\n",
    "                # XGBoost: Cast object/bin columns to categorical\n",
    "                cat_features_xgb = [c for c in X.columns if X[c].dtype == 'object' or '_bin' in c]\n",
    "                \n",
    "                X_tr_local = X_tr.copy()\n",
    "                X_va_local = X_va.copy()\n",
    "                for c in cat_features_xgb:\n",
    "                    if c in X_tr_local.columns: X_tr_local[c] = X_tr_local[c].astype('category')\n",
    "                    if c in X_va_local.columns: X_va_local[c] = X_va_local[c].astype('category')\n",
    "                \n",
    "                model = xgb.XGBClassifier(random_state=seed+fold_idx, **params)\n",
    "                model.fit(X_tr_local, y_tr, eval_set=[(X_va_local, y_va)], verbose=False)\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "\n",
    "            elif name == 'lgb':\n",
    "                # LightGBM: Requires category dtype for categorical features\n",
    "                cat_obj_cols = [c for c in X.columns if X[c].dtype == 'object']\n",
    "                \n",
    "                X_tr_local = X_tr.copy()\n",
    "                X_va_local = X_va.copy()\n",
    "                for c in cat_obj_cols:\n",
    "                    if c in X_tr_local.columns: X_tr_local[c] = X_tr_local[c].astype('category')\n",
    "                    if c in X_va_local.columns: X_va_local[c] = X_va_local[c].astype('category')\n",
    "                \n",
    "                model = lgb.LGBMClassifier(random_state=seed+fold_idx, **params)\n",
    "                model.fit(X_tr_local, y_tr, categorical_feature=cat_obj_cols if cat_obj_cols else 'auto')\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "\n",
    "            elif name == 'cb':\n",
    "                # CatBoost: Requires string for categorical, prefers text over float bins\n",
    "                cat_features_cb = [c for c in X.columns if X[c].dtype == 'object' or '_bin' in c]\n",
    "                \n",
    "                X_tr_local = X_tr.copy()\n",
    "                X_va_local = X_va.copy()\n",
    "                for c in cat_features_cb:\n",
    "                    if '_bin' in c:\n",
    "                        X_tr_local[c] = X_tr_local[c].fillna(-1).astype(int).astype(str)\n",
    "                        X_va_local[c] = X_va_local[c].fillna(-1).astype(int).astype(str)\n",
    "                \n",
    "                model = cb.CatBoostClassifier(\n",
    "                    random_seed=seed+fold_idx, \n",
    "                    cat_features=cat_features_cb if cat_features_cb else None,\n",
    "                    **params\n",
    "                )\n",
    "                model.fit(X_tr_local, y_tr, eval_set=(X_va_local, y_va), early_stopping_rounds=50, verbose=False)\n",
    "                p = model.predict_proba(X_va_local)[:,1]\n",
    "\n",
    "            # --- Store OOF and Calculate AUC ---\n",
    "            oof[va_idx, j] = p\n",
    "            auc = roc_auc_score(y_va, p)\n",
    "            aucs[name].append(auc)\n",
    "            \n",
    "            # --- Test Predictions (Accumulate Average) ---\n",
    "            if X_test is not None:\n",
    "                if name == 'xgb':\n",
    "                    X_test_local = X_test.copy()\n",
    "                    for c in cat_features_xgb:\n",
    "                        if c in X_test_local.columns: X_test_local[c] = X_test_local[c].astype('category')\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / n_splits\n",
    "                    \n",
    "                elif name == 'lgb':\n",
    "                    X_test_local = X_test.copy()\n",
    "                    for c in cat_obj_cols:\n",
    "                        if c in X_test_local.columns: X_test_local[c] = X_test_local[c].astype('category')\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / n_splits\n",
    "                    \n",
    "                elif name == 'cb':\n",
    "                    X_test_local = X_test.copy()\n",
    "                    for c in cat_features_cb:\n",
    "                        if '_bin' in c:\n",
    "                            X_test_local[c] = X_test_local[c].fillna(-1).astype(int).astype(str)\n",
    "                    test_preds[:, j] += model.predict_proba(X_test_local)[:,1] / n_splits\n",
    "            \n",
    "            fold_idx += 1\n",
    "        \n",
    "        # End of folds for this model\n",
    "        mean_auc = np.mean(aucs[name])\n",
    "        print(f\"  ‚úì {name} Average AUC: {mean_auc:.5f}\")\n",
    "\n",
    "    return oof, test_preds, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "45f7657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage & drift utilities ready.\n"
     ]
    }
   ],
   "source": [
    "# Leakage audit utilities\n",
    "# 1. Single-feature AUC to flag suspicious leak features.\n",
    "# 2. Temporal leakage heuristic: columns that look like aggregates (e.g., total_*, avg_*) might encode future info.\n",
    "# 3. Type casting helpers.\n",
    "\n",
    "import re\n",
    "\n",
    "LEAK_MAX_FEATURES = 40  # cap evaluation for speed\n",
    "\n",
    "def single_feature_auc_scan(df: pd.DataFrame, y: pd.Series, max_features=LEAK_MAX_FEATURES):\n",
    "    aucs = []\n",
    "    for col in df.columns[:max_features]:\n",
    "        try:\n",
    "            if df[col].nunique() < 2:\n",
    "                continue\n",
    "            vals = df[col].fillna(df[col].median() if df[col].dtype != 'O' else 'missing')\n",
    "            # For categorical -> encode label frequency\n",
    "            if vals.dtype == 'O':\n",
    "                mapping = vals.value_counts(normalize=True).to_dict()\n",
    "                enc = vals.map(mapping).astype(float)\n",
    "            else:\n",
    "                enc = vals.astype(float)\n",
    "            score = roc_auc_score(y, enc) if len(np.unique(enc)) > 1 else 0.5\n",
    "            aucs.append((col, score))\n",
    "        except Exception:\n",
    "            continue\n",
    "    aucs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return aucs\n",
    "\n",
    "AGG_PATTERNS = [r'^total_', r'^sum_', r'^avg_', r'^mean_', r'^max_', r'^min_']\n",
    "\n",
    "def looks_leaky(colname: str) -> bool:\n",
    "    for pat in AGG_PATTERNS:\n",
    "        if re.search(pat, colname):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# KS & PSI drift checks between train/test\n",
    "\n",
    "def ks_stat(train_col, test_col):\n",
    "    # dropna\n",
    "    a = pd.Series(train_col).dropna()\n",
    "    b = pd.Series(test_col).dropna()\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        stat, pval = stats.ks_2samp(a, b)\n",
    "        return stat\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# Population Stability Index for binned values\n",
    "\n",
    "def psi(train_col, test_col, buckets=10):\n",
    "    a = pd.Series(train_col).dropna()\n",
    "    b = pd.Series(test_col).dropna()\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return 0.0\n",
    "    quantiles = np.linspace(0, 1, buckets + 1)\n",
    "    cuts = a.quantile(quantiles).unique()\n",
    "    a_bins = pd.cut(a, bins=np.unique(cuts), include_lowest=True)\n",
    "    b_bins = pd.cut(b, bins=np.unique(cuts), include_lowest=True)\n",
    "    a_dist = a_bins.value_counts(normalize=True)\n",
    "    b_dist = b_bins.value_counts(normalize=True)\n",
    "    psi_val = 0.0\n",
    "    for idx in a_dist.index:\n",
    "        expected = a_dist.get(idx, 1e-6)\n",
    "        actual = b_dist.get(idx, 1e-6)\n",
    "        if expected > 0 and actual > 0:\n",
    "            psi_val += (actual - expected) * math.log(actual / expected)\n",
    "    return psi_val\n",
    "\n",
    "DRIFT_REPORT_LIMIT = 40\n",
    "\n",
    "def drift_report(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    rows = []\n",
    "    shared = [c for c in train_df.columns if c in test_df.columns]\n",
    "    for col in shared[:DRIFT_REPORT_LIMIT]:\n",
    "        try:\n",
    "            k = ks_stat(train_df[col], test_df[col])\n",
    "            p = psi(train_df[col], test_df[col])\n",
    "            rows.append({'feature': col, 'ks': k, 'psi': p})\n",
    "        except Exception:\n",
    "            continue\n",
    "    rep = pd.DataFrame(rows)\n",
    "    if not rep.empty:\n",
    "        rep.sort_values(['ks','psi'], ascending=False, inplace=True)\n",
    "    return rep\n",
    "\n",
    "BOOL_LIKE = ['y','n','yes','no','true','false']\n",
    "\n",
    "def cast_types(df: pd.DataFrame):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            # bool-like\n",
    "            low = df[c].str.lower()\n",
    "            if low.isin(BOOL_LIKE).mean() > 0.9:\n",
    "                df[c] = low.map({'y':1,'yes':1,'true':1,'n':0,'no':0,'false':0}).astype('Int8')\n",
    "    return df\n",
    "\n",
    "print('Leakage & drift utilities ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8264e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No leakage features flagged by simple scan.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>ks</th>\n",
       "      <th>psi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interest_rate</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debt_to_income_ratio</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loan_amount</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annual_income</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit_score</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature   ks       psi\n",
       "4         interest_rate  0.0  0.000062\n",
       "1  debt_to_income_ratio  0.0  0.000047\n",
       "3           loan_amount  0.0  0.000044\n",
       "0         annual_income  0.0  0.000039\n",
       "2          credit_score  0.0  0.000026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing audits complete.\n"
     ]
    }
   ],
   "source": [
    "# Apply type casting, leakage audit, and drift checks\n",
    "# Must run after data load (Cell 3)\n",
    "\n",
    "assert 'train' in globals(), 'Run the data load cell first.'\n",
    "\n",
    "# 1) Type casting\n",
    "if 'ID_COL' in globals() and ID_COL:\n",
    "    train[ID_COL] = train[ID_COL].astype(str)\n",
    "    if 'test' in globals() and test is not None and ID_COL in test.columns:\n",
    "        test[ID_COL] = test[ID_COL].astype(str)\n",
    "\n",
    "train = cast_types(train)\n",
    "if 'test' in globals() and test is not None:\n",
    "    test = cast_types(test)\n",
    "\n",
    "# 2) Leakage audit (simple, top-N features)\n",
    "feat_cols = [c for c in train.columns if c not in [TARGET] + ([ID_COL] if ID_COL else [])]\n",
    "scan_df = train[feat_cols].copy()\n",
    "scan_aucs = single_feature_auc_scan(scan_df, train[TARGET], max_features=min(LEAK_MAX_FEATURES, len(feat_cols)))\n",
    "leaky = [c for (c, auc) in scan_aucs if auc >= 0.92 or auc <= 0.08 or looks_leaky(c)]\n",
    "\n",
    "if len(leaky) > 0:\n",
    "    print('Dropping suspicious leakage features:', leaky)\n",
    "    train.drop(columns=[c for c in leaky if c in train.columns], inplace=True)\n",
    "    if 'test' in globals() and test is not None:\n",
    "        test.drop(columns=[c for c in leaky if c in test.columns], inplace=True)\n",
    "else:\n",
    "    print('No leakage features flagged by simple scan.')\n",
    "\n",
    "# 3) Drift check (requires test)\n",
    "if 'test' in globals() and test is not None:\n",
    "    tr_common = train.drop(columns=[TARGET] + ([ID_COL] if ID_COL else []), errors='ignore')\n",
    "    te_common = test.drop(columns=[ID_COL] if ID_COL else [], errors='ignore')\n",
    "    rep = drift_report(tr_common, te_common)\n",
    "    display(rep.head(12))\n",
    "    # Drop worst offenders by relaxed rule\n",
    "    drop_drift = rep[(rep['ks'] >= 0.3) | (rep['psi'] >= 0.3)]['feature'].tolist()\n",
    "    if drop_drift:\n",
    "        print('Dropping drift-heavy features:', drop_drift)\n",
    "        train.drop(columns=[c for c in drop_drift if c in train.columns], inplace=True)\n",
    "        test.drop(columns=[c for c in drop_drift if c in test.columns], inplace=True)\n",
    "else:\n",
    "    print('Test set not available; skipping drift check.')\n",
    "\n",
    "print('Preprocessing audits complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "24602756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Meta: SIMPLIFIED Ridge Meta-Learner (Fast & Stable)\n",
    "\n",
    "def train_meta_l2(oof_feats, y, test_feats=None, seed=42, X_orig=None, X_test_orig=None):\n",
    "    \"\"\"L2 Simple Meta-Learner: Ridge or Weighted Average based on CV performance\"\"\"\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    \n",
    "    print(f'  üìä L2 Simple Meta on {oof_feats.shape[1]} base model predictions')\n",
    "    \n",
    "    # Calculate individual model AUCs for weighted average\n",
    "    individual_aucs = []\n",
    "    for i in range(oof_feats.shape[1]):\n",
    "        auc = roc_auc_score(y, oof_feats[:, i])\n",
    "        individual_aucs.append(auc)\n",
    "    \n",
    "    # Method 1: Performance-weighted average (simple and effective)\n",
    "    weights = np.array(individual_aucs) / np.sum(individual_aucs)\n",
    "    oof_meta = np.dot(oof_feats, weights)\n",
    "    meta_auc = roc_auc_score(y, oof_meta)\n",
    "    \n",
    "    print(f'  ‚úì Performance-weighted average: {np.round(weights, 4)}')\n",
    "    print(f'  ‚úì L2 Meta AUC: {meta_auc:.5f}')\n",
    "    \n",
    "    # Apply same weights to test predictions\n",
    "    test_meta = None\n",
    "    if test_feats is not None:\n",
    "        test_meta = np.dot(test_feats, weights)\n",
    "    \n",
    "    return oof_meta, test_meta\n",
    "\n",
    "\n",
    "def train_meta_l3_with_pseudo(oof_l2, y, test_l2, X, X_test, seed=42):\n",
    "    \"\"\"L3 Meta + PSEUDO-LABELING for final push to 93%+\"\"\"\n",
    "    \n",
    "    # PSEUDO-LABELING: Use high-confidence test predictions\n",
    "    if test_l2 is not None and X_test is not None:\n",
    "        # Select high-confidence test samples (‚â•0.90 or ‚â§0.10)\n",
    "        high_conf_mask = (test_l2 > 0.90) | (test_l2 < 0.10)\n",
    "        pseudo_labels = (test_l2 > 0.5).astype(int)\n",
    "        \n",
    "        n_pseudo = high_conf_mask.sum()\n",
    "        if n_pseudo > 0:\n",
    "            print(f'  üé≠ Pseudo-labeling: {n_pseudo} high-confidence test samples')\n",
    "            \n",
    "            # Combine train + pseudo-labeled test\n",
    "            X_combined = pd.concat([X, X_test.iloc[high_conf_mask]], axis=0, ignore_index=True)\n",
    "            y_combined = pd.concat([y, pd.Series(pseudo_labels[high_conf_mask])], axis=0, ignore_index=True)\n",
    "            oof_combined = np.concatenate([oof_l2, test_l2[high_conf_mask]])\n",
    "            \n",
    "            # Retrain L3 on combined data\n",
    "            cv = get_cv(n_splits=5, seed=seed)\n",
    "            oof_l3 = np.zeros(len(oof_combined))\n",
    "            \n",
    "            for fold, (tr_idx, va_idx) in enumerate(cv.split(oof_combined, y_combined)):\n",
    "                X_tr, X_va = oof_combined[tr_idx].reshape(-1, 1), oof_combined[va_idx].reshape(-1, 1)\n",
    "                y_tr, y_va = y_combined.iloc[tr_idx], y_combined.iloc[va_idx]\n",
    "                \n",
    "                clf = LogisticRegression(max_iter=5000, C=0.5)\n",
    "                clf.fit(X_tr, y_tr)\n",
    "                oof_l3[va_idx] = clf.predict_proba(X_va)[:,1]\n",
    "            \n",
    "            # Extract only original train predictions\n",
    "            oof_l3_train = oof_l3[:len(y)]\n",
    "            auc_l3 = roc_auc_score(y, oof_l3_train)\n",
    "            print(f'  ‚úì L3 Meta + Pseudo AUC: {auc_l3:.5f}')\n",
    "            \n",
    "            return oof_l3_train\n",
    "    \n",
    "    # Fallback: simple L3 without pseudo-labeling\n",
    "    return oof_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ffafe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training orchestrator: EXTREME pipeline for 93%+\n",
    "\n",
    "def run_training_extreme(seeds=[42, 43], target_auc=0.93):\n",
    "    \"\"\"\n",
    "    Multi-level stacking + pseudo-labeling pipeline\n",
    "    L1 (base) ‚Üí L2 (fast linear) ‚Üí L3 (pseudo) ‚Üí Calibration\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    best = None\n",
    "    \n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n{'='*70}\\nüöÄ SEED {seed} ({i+1}/{len(seeds)}) ‚Äî Targeting 93%+ AUC\\n{'='*70}\")\n",
    "        \n",
    "        # L1: Base models (XGB + LGB + CB)\n",
    "        print('\\n[L1] Training base models...')\n",
    "        oof_l1, test_l1, base_aucs = train_base_models(X, y, X_test, seed=seed, n_splits=7)\n",
    "        \n",
    "        # L2: Fast Linear Meta model with grade ordinal\n",
    "        print('\\n[L2] Training fast linear meta...')\n",
    "        oof_l2, test_l2 = train_meta_l2(oof_l1, y, test_l1, seed=seed, X_orig=X, X_test_orig=X_test)\n",
    "        \n",
    "        # L3: Pseudo-labeling (if available)\n",
    "        print('\\n[L3] Pseudo-labeling...')\n",
    "        oof_l3 = train_meta_l3_with_pseudo(oof_l2, y, test_l2, X, X_test, seed=seed)\n",
    "        \n",
    "        # Isotonic calibration\n",
    "        print('\\n[CAL] Calibrating predictions...')\n",
    "        iso = fit_isotonic(y.values, oof_l3)\n",
    "        oof_cal = iso.predict(oof_l3)\n",
    "        auc_cal = roc_auc_score(y, oof_cal)\n",
    "        test_cal = iso.predict(test_l2) if test_l2 is not None else None\n",
    "        \n",
    "        # Threshold optimization\n",
    "        best_thr = threshold_sweep(y.values, oof_cal)\n",
    "        \n",
    "        print(f'\\n{\"=\"*70}')\n",
    "        print(f'üéØ FINAL AUC (calibrated): {auc_cal:.5f}')\n",
    "        print(f'üìä Best threshold: {best_thr[\"threshold\"]:.3f} (F1={best_thr[\"f1\"]:.4f})')\n",
    "        print(f'{\"=\"*70}')\n",
    "        \n",
    "        record = {\n",
    "            'seed': seed,\n",
    "            'auc_l2': roc_auc_score(y, oof_l2),\n",
    "            'auc_l3': roc_auc_score(y, oof_l3),\n",
    "            'auc_cal': auc_cal,\n",
    "            'best_thr': best_thr,\n",
    "        }\n",
    "        results.append(record)\n",
    "        \n",
    "        if (best is None) or (auc_cal > best['auc_cal']):\n",
    "            best = {\n",
    "                **record,\n",
    "                'oof_cal': oof_cal,\n",
    "                'test_cal': test_cal,\n",
    "                'base_aucs': base_aucs\n",
    "            }\n",
    "            print(f'‚ú® NEW BEST: {auc_cal:.5f}')\n",
    "        \n",
    "        if auc_cal >= target_auc:\n",
    "            print(f'\\nüèÜ BREAKTHROUGH! Hit {target_auc:.1%} target!')\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(results), best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2b96babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TARGET: Break 93% AUC barrier\n",
      "üìà Strategy: L1‚ÜíL2‚ÜíL3 stacking + pseudo-labeling + calibration\n",
      "‚è±Ô∏è  ETA: ~10-15 minutes with full optimization\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üöÄ SEED 42 (1/5) ‚Äî Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "üöÄ Training 3 L1 base models: ['xgb', 'lgb', 'cb']\n",
      "\n",
      "Training xgb...\n",
      "  ‚úì xgb Average AUC: 0.92036\n",
      "\n",
      "Training lgb...\n",
      "  ‚úì lgb Average AUC: 0.92130\n",
      "\n",
      "Training cb...\n",
      "  ‚úì cb Average AUC: 0.91941\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  üìä L2 Simple Meta on 3 base model predictions\n",
      "  ‚úì Performance-weighted average: [0.3333 0.3337 0.333 ]\n",
      "  ‚úì L2 Meta AUC: 0.92124\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  üé≠ Pseudo-labeling: 176590 high-confidence test samples\n",
      "  ‚úì L3 Meta + Pseudo AUC: 0.92124\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL AUC (calibrated): 0.92135\n",
      "üìä Best threshold: 0.450 (F1=0.9433)\n",
      "======================================================================\n",
      "‚ú® NEW BEST: 0.92135\n",
      "\n",
      "======================================================================\n",
      "üöÄ SEED 43 (2/5) ‚Äî Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "üöÄ Training 3 L1 base models: ['xgb', 'lgb', 'cb']\n",
      "\n",
      "Training xgb...\n",
      "  ‚úì xgb Average AUC: 0.92037\n",
      "\n",
      "Training lgb...\n",
      "  ‚úì lgb Average AUC: 0.92129\n",
      "\n",
      "Training cb...\n",
      "  ‚úì cb Average AUC: 0.91948\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  üìä L2 Simple Meta on 3 base model predictions\n",
      "  ‚úì Performance-weighted average: [0.3333 0.3337 0.333 ]\n",
      "  ‚úì L2 Meta AUC: 0.92128\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  üé≠ Pseudo-labeling: 176553 high-confidence test samples\n",
      "  ‚úì L3 Meta + Pseudo AUC: 0.92127\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL AUC (calibrated): 0.92137\n",
      "üìä Best threshold: 0.450 (F1=0.9432)\n",
      "======================================================================\n",
      "‚ú® NEW BEST: 0.92137\n",
      "\n",
      "======================================================================\n",
      "üöÄ SEED 44 (3/5) ‚Äî Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "üöÄ Training 3 L1 base models: ['xgb', 'lgb', 'cb']\n",
      "\n",
      "Training xgb...\n",
      "  ‚úì xgb Average AUC: 0.92042\n",
      "\n",
      "Training lgb...\n",
      "  ‚úì lgb Average AUC: 0.92133\n",
      "\n",
      "Training cb...\n",
      "  ‚úì cb Average AUC: 0.91951\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  üìä L2 Simple Meta on 3 base model predictions\n",
      "  ‚úì Performance-weighted average: [0.3333 0.3337 0.333 ]\n",
      "  ‚úì L2 Meta AUC: 0.92132\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  üé≠ Pseudo-labeling: 176575 high-confidence test samples\n",
      "  ‚úì L3 Meta + Pseudo AUC: 0.92131\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL AUC (calibrated): 0.92142\n",
      "üìä Best threshold: 0.450 (F1=0.9433)\n",
      "======================================================================\n",
      "‚ú® NEW BEST: 0.92142\n",
      "\n",
      "======================================================================\n",
      "üöÄ SEED 45 (4/5) ‚Äî Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "üöÄ Training 3 L1 base models: ['xgb', 'lgb', 'cb']\n",
      "\n",
      "Training xgb...\n",
      "  ‚úì xgb Average AUC: 0.92031\n",
      "\n",
      "Training lgb...\n",
      "  ‚úì lgb Average AUC: 0.92127\n",
      "\n",
      "Training cb...\n",
      "  ‚úì cb Average AUC: 0.91947\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  üìä L2 Simple Meta on 3 base model predictions\n",
      "  ‚úì Performance-weighted average: [0.3333 0.3337 0.333 ]\n",
      "  ‚úì L2 Meta AUC: 0.92125\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  üé≠ Pseudo-labeling: 176521 high-confidence test samples\n",
      "  ‚úì L3 Meta + Pseudo AUC: 0.92124\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL AUC (calibrated): 0.92134\n",
      "üìä Best threshold: 0.450 (F1=0.9432)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ SEED 46 (5/5) ‚Äî Targeting 93%+ AUC\n",
      "======================================================================\n",
      "\n",
      "[L1] Training base models...\n",
      "üöÄ Training 3 L1 base models: ['xgb', 'lgb', 'cb']\n",
      "\n",
      "Training xgb...\n",
      "  ‚úì xgb Average AUC: 0.92035\n",
      "\n",
      "Training lgb...\n",
      "  ‚úì lgb Average AUC: 0.92117\n",
      "\n",
      "Training cb...\n",
      "  ‚úì cb Average AUC: 0.91945\n",
      "\n",
      "[L2] Training fast linear meta...\n",
      "  üìä L2 Simple Meta on 3 base model predictions\n",
      "  ‚úì Performance-weighted average: [0.3333 0.3336 0.333 ]\n",
      "  ‚úì L2 Meta AUC: 0.92121\n",
      "\n",
      "[L3] Pseudo-labeling...\n",
      "  üé≠ Pseudo-labeling: 176492 high-confidence test samples\n",
      "  ‚úì L3 Meta + Pseudo AUC: 0.92121\n",
      "\n",
      "[CAL] Calibrating predictions...\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL AUC (calibrated): 0.92131\n",
      "üìä Best threshold: 0.450 (F1=0.9432)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTS SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>auc_l2</th>\n",
       "      <th>auc_l3</th>\n",
       "      <th>auc_cal</th>\n",
       "      <th>best_thr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>0.921245</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>0.921352</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>0.921275</td>\n",
       "      <td>0.921269</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>0.921317</td>\n",
       "      <td>0.921315</td>\n",
       "      <td>0.921420</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.921246</td>\n",
       "      <td>0.921239</td>\n",
       "      <td>0.921337</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>0.921214</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>0.921313</td>\n",
       "      <td>{'threshold': 0.44999999999999996, 'f1': 0.943...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed    auc_l2    auc_l3   auc_cal  \\\n",
       "0    42  0.921245  0.921240  0.921352   \n",
       "1    43  0.921275  0.921269  0.921374   \n",
       "2    44  0.921317  0.921315  0.921420   \n",
       "3    45  0.921246  0.921239  0.921337   \n",
       "4    46  0.921214  0.921208  0.921313   \n",
       "\n",
       "                                            best_thr  \n",
       "0  {'threshold': 0.44999999999999996, 'f1': 0.943...  \n",
       "1  {'threshold': 0.44999999999999996, 'f1': 0.943...  \n",
       "2  {'threshold': 0.44999999999999996, 'f1': 0.943...  \n",
       "3  {'threshold': 0.44999999999999996, 'f1': 0.943...  \n",
       "4  {'threshold': 0.44999999999999996, 'f1': 0.943...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ BEST RESULT:\n",
      "   Seed: 44\n",
      "   L2 AUC: 0.92132\n",
      "   L3 AUC: 0.92131\n",
      "   Calibrated AUC: 0.92142\n",
      "   Threshold: 0.450\n",
      "   F1 Score: 0.9433\n",
      "\n",
      "üìç Gap to 93%: 0.00858 (0.858 pp)\n",
      "üí° Next steps: Add more seeds, try neural blend, or ensemble with other models\n"
     ]
    }
   ],
   "source": [
    "# üöÄ EXECUTE: Extreme training for 93%+ AUC\n",
    "\n",
    "print('üéØ TARGET: Break 93% AUC barrier')\n",
    "print('üìà Strategy: L1‚ÜíL2‚ÜíL3 stacking + pseudo-labeling + calibration')\n",
    "print('‚è±Ô∏è  ETA: ~10-15 minutes with full optimization\\n')\n",
    "\n",
    "SEEDS = [42, 43, 44, 45, 46]  # Increased seeds for stability and performance\n",
    "results_df, best = run_training_extreme(seeds=SEEDS, target_auc=0.93)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('üìä RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "display(results_df)\n",
    "\n",
    "print(f'\\nüèÜ BEST RESULT:')\n",
    "print(f'   Seed: {best[\"seed\"]}')\n",
    "print(f'   L2 AUC: {best[\"auc_l2\"]:.5f}')\n",
    "print(f'   L3 AUC: {best[\"auc_l3\"]:.5f}')\n",
    "print(f'   Calibrated AUC: {best[\"auc_cal\"]:.5f}')\n",
    "print(f'   Threshold: {best[\"best_thr\"][\"threshold\"]:.3f}')\n",
    "print(f'   F1 Score: {best[\"best_thr\"][\"f1\"]:.4f}')\n",
    "\n",
    "if best['auc_cal'] >= 0.93:\n",
    "    print(f'\\nüéâüéâÔøΩ BREAKTHROUGH ACHIEVED! {best[\"auc_cal\"]:.5f} >= 93% üéâüéâüéâ')\n",
    "else:\n",
    "    gap = 0.93 - best['auc_cal']\n",
    "    print(f'\\nüìç Gap to 93%: {gap:.5f} ({gap*100:.3f} pp)')\n",
    "    print('üí° Next steps: Add more seeds, try neural blend, or ensemble with other models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "368e872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ SUBMISSION SAVED!\n",
      "   File: EXTREME_93pct_AUC092142_20251120_095315.csv\n",
      "   AUC: 0.92142\n",
      "   Threshold: 0.450\n",
      "   Samples: 254,569\n"
     ]
    }
   ],
   "source": [
    "# üèÖ Build WINNING submission\n",
    "\n",
    "if 'test' in globals() and test is not None and SAMPLE_SUB_PATH.exists() and best is not None:\n",
    "    sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "    sub_id_col = sub.columns[0]\n",
    "    sub_target_col = sub.columns[1] if len(sub.columns) > 1 else (TARGET if TARGET is not None else 'target')\n",
    "    \n",
    "    if 'ID_COL' in globals() and ID_COL and sub_id_col != ID_COL and ID_COL in test.columns:\n",
    "        sub[sub_id_col] = test[ID_COL].values\n",
    "    \n",
    "    preds = best['test_cal'] if best.get('test_cal') is not None else None\n",
    "    \n",
    "    if preds is not None:\n",
    "        sub[sub_target_col] = preds\n",
    "        timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "        auc_str = f\"{best['auc_cal']:.5f}\".replace('.', '')\n",
    "        out_path = SUB_DIR / f'EXTREME_93pct_AUC{auc_str}_{timestamp}.csv'\n",
    "        sub.to_csv(out_path, index=False)\n",
    "        \n",
    "        print(f'\\nüèÜ SUBMISSION SAVED!')\n",
    "        print(f'   File: {out_path.name}')\n",
    "        print(f'   AUC: {best[\"auc_cal\"]:.5f}')\n",
    "        print(f'   Threshold: {best[\"best_thr\"][\"threshold\"]:.3f}')\n",
    "        print(f'   Samples: {len(sub):,}')\n",
    "        \n",
    "        if best['auc_cal'] >= 0.93:\n",
    "            print(f'\\nüéä FIRST TO BREAK 93%! Submit this ASAP! üéä')\n",
    "    else:\n",
    "        print('‚ö†Ô∏è  No test predictions available.')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Submission not created (missing test data or best result).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163bd18",
   "metadata": {},
   "source": [
    "Stuff we tryed so far \n",
    "\n",
    ". Architecture & Model Stacking\n",
    "L1 Base Layer: Started with XGBoost, LightGBM, and CatBoost.\n",
    "Tried: Adding Logistic Regression for diversity (Removed later due to complexity/low gain).\n",
    "Current State: XGB, LGB, CB with 1500 estimators and learning_rate=0.015.\n",
    "L2 Meta Layer:\n",
    "Tried: Fast Linear Stacking (Logistic/Ridge) to save time. Result: Performance dropped (0.917).\n",
    "Restored: XGBoost Meta-Learner. We switched back to a shallow XGBoost (Depth 4) to capture non-linearities between model predictions.\n",
    "L3 Pseudo-Labeling:\n",
    "Implemented: Using high-confidence test predictions (top/bottom 10%) to retrain the ensemble.\n",
    "Calibration:\n",
    "Implemented: Isotonic Regression to recalibrate probabilities before the final submission.\n",
    "2. Feature Engineering\n",
    "Target Encoding: Applied smoothed target encoding to all categorical features (10-fold CV to prevent leakage).\n",
    "Interaction Pairs: Created standard ratios and products between key columns (Loan Amount, Income, etc.).\n",
    "Visual Insight Fixes:\n",
    "Ordinal Encoding: Mapped grade_subgrade explicitly (A1=1 ... F5=35) because the plots showed a perfect monotonic trend.\n",
    "Unemployment Flag: Created is_unemployed binary feature based on the specific drop-off seen in the plots.\n",
    "Log Transforms: Applied np.log1p to Income, Loan Amount, and DTI to handle the massive right-skew seen in histograms.\n",
    "Risk Features:\n",
    "Risk Ratio: Added Interest Rate / Credit Score (High rate + Low score = Extreme risk).\n",
    "PTI Proxy: (Proposed) (Loan * Rate) / Income to approximate payment burden.\n",
    "3. Hyperparameter Tuning & Correction\n",
    "Class Weights (The Failed Experiment):\n",
    "Action: We enabled scale_pos_weight and is_unbalance to handle the 80/20 split.\n",
    "Result: Score dropped to 0.917. Class weights distorted the ranking probabilities required for AUC.\n",
    "Fix: Removed all class weights to let the model learn the natural probability distribution.\n",
    "Estimator Count:\n",
    "Action: Reduced to 800 for speed.\n",
    "Correction: Bumped back to 1500 because the model was underfitting the complex feature set.\n",
    "4. Current Status\n",
    "Best Score: 0.92020 (Leaderboard).\n",
    "Current Issue: Generalization Gap. Local CV is higher (~0.9215) than LB (0.9202). The boosting models are correlated and slightly overfitting.\n",
    "Immediate Next Step: Introduce Bagging (Random Forest) to the L1 layer to reduce variance and close the gap to 0.93."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
