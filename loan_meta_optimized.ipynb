{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4a0969",
   "metadata": {},
   "source": [
    "# Loan Payback — Meta-Boosted XGBoost (loan_meta_optimized)\n",
    "\n",
    "This notebook builds a **two–stage boosted model** for the loan payback competition:\n",
    "\n",
    "1. **Stage 1**: A strong XGBoost classifier is trained on preprocessed features.\n",
    "2. **Stage 2 (Meta Boost)**:  \n",
    "   - We convert Stage‑1 predicted probabilities to **logits** (log‑odds).  \n",
    "   - These logits are passed to a *second* XGBoost model via `base_margin`, which means\n",
    "     Stage 2 **boosts over the residuals** of Stage 1 instead of starting from scratch.\n",
    "   - The meta model outputs refined probabilities.\n",
    "\n",
    "The goal is to provide a clean, reproducible pipeline that you can easily extend and\n",
    "tune further on Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "464cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Imports & basic configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_DIR = Path(\"Data\")\n",
    "# If running locally, you can override DATA_DIR, e.g.:\n",
    "# DATA_DIR = Path(\"/mnt/data\") / \"loan-payback\"\n",
    "\n",
    "def log(msg: str):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb6aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:50] Using train: train.csv\n",
      "[22:30:50] Using test : test.csv\n",
      "[22:30:51] Train shape: (593994, 13)\n",
      "[22:30:51] Test  shape: (254569, 12)\n",
      "[22:30:51] Detected target column: loan_paid_back\n",
      "[22:30:51] Detected id column: id\n",
      "[22:30:51] Number of features: 11\n",
      "[22:30:51] Train shape: (593994, 13)\n",
      "[22:30:51] Test  shape: (254569, 12)\n",
      "[22:30:51] Detected target column: loan_paid_back\n",
      "[22:30:51] Detected id column: id\n",
      "[22:30:51] Number of features: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Data loading and automatic target / id detection\n",
    "\n",
    "train_path = None\n",
    "test_path = None\n",
    "\n",
    "# Heuristic: pick first train/test-looking CSVs\n",
    "csv_files = sorted(list(DATA_DIR.glob(\"*.csv\")))\n",
    "for p in csv_files:\n",
    "    name = p.name.lower()\n",
    "    if \"train\" in name and train_path is None:\n",
    "        train_path = p\n",
    "    if \"test\" in name and test_path is None and \"train\" not in name:\n",
    "        test_path = p\n",
    "\n",
    "if train_path is None or test_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not detect train/test CSVs inside {DATA_DIR}. \"\n",
    "        \"Please set train_path and test_path manually.\"\n",
    "    )\n",
    "\n",
    "log(f\"Using train: {train_path.name}\")\n",
    "log(f\"Using test : {test_path.name}\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "log(f\"Train shape: {train_df.shape}\")\n",
    "log(f\"Test  shape: {test_df.shape}\")\n",
    "\n",
    "def detect_target(train_df: pd.DataFrame, test_df: pd.DataFrame) -> str:\n",
    "    diff = list(set(train_df.columns) - set(test_df.columns))\n",
    "    # Prefer a binary label\n",
    "    candidates = []\n",
    "    for c in diff:\n",
    "        if train_df[c].nunique() <= 3:\n",
    "            candidates.append(c)\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    if len(diff) == 1:\n",
    "        return diff[0]\n",
    "    for name in [\"loan_paid_back\", \"target\", \"label\", \"is_default\", \"default\", \"paid\"]:\n",
    "        if name in train_df.columns and name not in test_df.columns:\n",
    "            return name\n",
    "    raise ValueError(f\"Could not detect target. Diff columns: {diff}\")\n",
    "\n",
    "target_col = detect_target(train_df, test_df)\n",
    "log(f\"Detected target column: {target_col}\")\n",
    "\n",
    "# Simple ID detection: column whose values are unique in train and test\n",
    "id_col = None\n",
    "for col in train_df.columns:\n",
    "    if col == target_col:\n",
    "        continue\n",
    "    if col in test_df.columns:\n",
    "        if train_df[col].is_unique and test_df[col].is_unique:\n",
    "            id_col = col\n",
    "            break\n",
    "\n",
    "log(f\"Detected id column: {id_col}\")\n",
    "\n",
    "y = train_df[target_col].astype(int).values\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in [target_col, id_col]]\n",
    "X = train_df[feature_cols].copy()\n",
    "X_test = test_df[feature_cols].copy()\n",
    "\n",
    "log(f\"Number of features: {len(feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be81abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:51] Numeric features    : 5\n",
      "[22:30:51] Categorical features: 6\n",
      "[22:30:51] Fitting preprocessing on full training data...\n",
      "[22:30:53] Processed X shape      : (593994, 60)\n",
      "[22:30:53] Processed X_test shape : (254569, 60)\n",
      "[22:30:53] Processed X shape      : (593994, 60)\n",
      "[22:30:53] Processed X_test shape : (254569, 60)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Preprocessing: numeric + categorical pipelines\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "log(f\"Numeric features    : {len(numeric_cols)}\")\n",
    "log(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "numeric_transformer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on full training data, then transform train & test\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline  # avoid name clash\n",
    "\n",
    "dummy_model = SklearnPipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "])\n",
    "\n",
    "log(\"Fitting preprocessing on full training data...\")\n",
    "dummy_model.fit(X)\n",
    "\n",
    "X_proc = dummy_model.transform(X)\n",
    "X_test_proc = dummy_model.transform(X_test)\n",
    "\n",
    "log(f\"Processed X shape      : {X_proc.shape}\")\n",
    "log(f\"Processed X_test shape : {X_test_proc.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed116d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:53] Training Stage 1 model (base XGBoost)...\n",
      "[22:30:53] Fold 1/5\n",
      "[0]\ttrain-auc:0.88926\tvalid-auc:0.88960\n",
      "[0]\ttrain-auc:0.88926\tvalid-auc:0.88960\n",
      "[200]\ttrain-auc:0.91682\tvalid-auc:0.91735\n",
      "[200]\ttrain-auc:0.91682\tvalid-auc:0.91735\n",
      "[400]\ttrain-auc:0.91986\tvalid-auc:0.91945\n",
      "[400]\ttrain-auc:0.91986\tvalid-auc:0.91945\n",
      "[600]\ttrain-auc:0.92224\tvalid-auc:0.92089\n",
      "[600]\ttrain-auc:0.92224\tvalid-auc:0.92089\n",
      "[800]\ttrain-auc:0.92405\tvalid-auc:0.92177\n",
      "[800]\ttrain-auc:0.92405\tvalid-auc:0.92177\n",
      "[1000]\ttrain-auc:0.92543\tvalid-auc:0.92225\n",
      "[1000]\ttrain-auc:0.92543\tvalid-auc:0.92225\n",
      "[1200]\ttrain-auc:0.92658\tvalid-auc:0.92252\n",
      "[1200]\ttrain-auc:0.92658\tvalid-auc:0.92252\n",
      "[1400]\ttrain-auc:0.92757\tvalid-auc:0.92272\n",
      "[1400]\ttrain-auc:0.92757\tvalid-auc:0.92272\n",
      "[1600]\ttrain-auc:0.92853\tvalid-auc:0.92289\n",
      "[1600]\ttrain-auc:0.92853\tvalid-auc:0.92289\n",
      "[1800]\ttrain-auc:0.92936\tvalid-auc:0.92294\n",
      "[1800]\ttrain-auc:0.92936\tvalid-auc:0.92294\n",
      "[1999]\ttrain-auc:0.93011\tvalid-auc:0.92296\n",
      "[1999]\ttrain-auc:0.93011\tvalid-auc:0.92296\n",
      "[22:32:45] Fold 2/5\n",
      "[0]\ttrain-auc:0.88914\tvalid-auc:0.88896\n",
      "[22:32:45] Fold 2/5\n",
      "[0]\ttrain-auc:0.88914\tvalid-auc:0.88896\n",
      "[200]\ttrain-auc:0.91718\tvalid-auc:0.91700\n",
      "[200]\ttrain-auc:0.91718\tvalid-auc:0.91700\n",
      "[400]\ttrain-auc:0.92005\tvalid-auc:0.91905\n",
      "[400]\ttrain-auc:0.92005\tvalid-auc:0.91905\n",
      "[600]\ttrain-auc:0.92234\tvalid-auc:0.92039\n",
      "[600]\ttrain-auc:0.92234\tvalid-auc:0.92039\n",
      "[800]\ttrain-auc:0.92404\tvalid-auc:0.92137\n",
      "[800]\ttrain-auc:0.92404\tvalid-auc:0.92137\n",
      "[1000]\ttrain-auc:0.92537\tvalid-auc:0.92191\n",
      "[1000]\ttrain-auc:0.92537\tvalid-auc:0.92191\n",
      "[1200]\ttrain-auc:0.92650\tvalid-auc:0.92222\n",
      "[1200]\ttrain-auc:0.92650\tvalid-auc:0.92222\n",
      "[1400]\ttrain-auc:0.92749\tvalid-auc:0.92241\n",
      "[1400]\ttrain-auc:0.92749\tvalid-auc:0.92241\n",
      "[1600]\ttrain-auc:0.92839\tvalid-auc:0.92258\n",
      "[1600]\ttrain-auc:0.92839\tvalid-auc:0.92258\n",
      "[1800]\ttrain-auc:0.92924\tvalid-auc:0.92269\n",
      "[1800]\ttrain-auc:0.92924\tvalid-auc:0.92269\n",
      "[1934]\ttrain-auc:0.92973\tvalid-auc:0.92271\n",
      "[1934]\ttrain-auc:0.92973\tvalid-auc:0.92271\n",
      "[22:34:34] Fold 3/5\n",
      "[22:34:34] Fold 3/5\n",
      "[0]\ttrain-auc:0.88911\tvalid-auc:0.88712\n",
      "[0]\ttrain-auc:0.88911\tvalid-auc:0.88712\n",
      "[200]\ttrain-auc:0.91753\tvalid-auc:0.91564\n",
      "[200]\ttrain-auc:0.91753\tvalid-auc:0.91564\n",
      "[400]\ttrain-auc:0.92045\tvalid-auc:0.91747\n",
      "[400]\ttrain-auc:0.92045\tvalid-auc:0.91747\n",
      "[600]\ttrain-auc:0.92263\tvalid-auc:0.91874\n",
      "[600]\ttrain-auc:0.92263\tvalid-auc:0.91874\n",
      "[800]\ttrain-auc:0.92438\tvalid-auc:0.91960\n",
      "[800]\ttrain-auc:0.92438\tvalid-auc:0.91960\n",
      "[1000]\ttrain-auc:0.92572\tvalid-auc:0.92011\n",
      "[1000]\ttrain-auc:0.92572\tvalid-auc:0.92011\n",
      "[1200]\ttrain-auc:0.92689\tvalid-auc:0.92048\n",
      "[1200]\ttrain-auc:0.92689\tvalid-auc:0.92048\n",
      "[1400]\ttrain-auc:0.92790\tvalid-auc:0.92068\n",
      "[1400]\ttrain-auc:0.92790\tvalid-auc:0.92068\n",
      "[1600]\ttrain-auc:0.92880\tvalid-auc:0.92082\n",
      "[1600]\ttrain-auc:0.92880\tvalid-auc:0.92082\n",
      "[1727]\ttrain-auc:0.92930\tvalid-auc:0.92081\n",
      "[1727]\ttrain-auc:0.92930\tvalid-auc:0.92081\n",
      "[22:36:12] Fold 4/5\n",
      "[0]\ttrain-auc:0.88948\tvalid-auc:0.88963\n",
      "[22:36:12] Fold 4/5\n",
      "[0]\ttrain-auc:0.88948\tvalid-auc:0.88963\n",
      "[200]\ttrain-auc:0.91733\tvalid-auc:0.91632\n",
      "[200]\ttrain-auc:0.91733\tvalid-auc:0.91632\n",
      "[400]\ttrain-auc:0.92031\tvalid-auc:0.91833\n",
      "[400]\ttrain-auc:0.92031\tvalid-auc:0.91833\n",
      "[600]\ttrain-auc:0.92237\tvalid-auc:0.91945\n",
      "[600]\ttrain-auc:0.92237\tvalid-auc:0.91945\n",
      "[800]\ttrain-auc:0.92429\tvalid-auc:0.92048\n",
      "[800]\ttrain-auc:0.92429\tvalid-auc:0.92048\n",
      "[1000]\ttrain-auc:0.92560\tvalid-auc:0.92096\n",
      "[1000]\ttrain-auc:0.92560\tvalid-auc:0.92096\n",
      "[1200]\ttrain-auc:0.92674\tvalid-auc:0.92133\n",
      "[1200]\ttrain-auc:0.92674\tvalid-auc:0.92133\n",
      "[1400]\ttrain-auc:0.92772\tvalid-auc:0.92151\n",
      "[1400]\ttrain-auc:0.92772\tvalid-auc:0.92151\n",
      "[1600]\ttrain-auc:0.92860\tvalid-auc:0.92167\n",
      "[1600]\ttrain-auc:0.92860\tvalid-auc:0.92167\n",
      "[1800]\ttrain-auc:0.92943\tvalid-auc:0.92177\n",
      "[1800]\ttrain-auc:0.92943\tvalid-auc:0.92177\n",
      "[1999]\ttrain-auc:0.93023\tvalid-auc:0.92186\n",
      "[1999]\ttrain-auc:0.93023\tvalid-auc:0.92186\n",
      "[22:38:04] Fold 5/5\n",
      "[0]\ttrain-auc:0.88894\tvalid-auc:0.88972\n",
      "[22:38:04] Fold 5/5\n",
      "[0]\ttrain-auc:0.88894\tvalid-auc:0.88972\n",
      "[200]\ttrain-auc:0.91728\tvalid-auc:0.91594\n",
      "[200]\ttrain-auc:0.91728\tvalid-auc:0.91594\n",
      "[400]\ttrain-auc:0.92029\tvalid-auc:0.91809\n",
      "[400]\ttrain-auc:0.92029\tvalid-auc:0.91809\n",
      "[600]\ttrain-auc:0.92252\tvalid-auc:0.91943\n",
      "[600]\ttrain-auc:0.92252\tvalid-auc:0.91943\n",
      "[800]\ttrain-auc:0.92405\tvalid-auc:0.92006\n",
      "[800]\ttrain-auc:0.92405\tvalid-auc:0.92006\n",
      "[1000]\ttrain-auc:0.92540\tvalid-auc:0.92054\n",
      "[1000]\ttrain-auc:0.92540\tvalid-auc:0.92054\n",
      "[1200]\ttrain-auc:0.92657\tvalid-auc:0.92090\n",
      "[1200]\ttrain-auc:0.92657\tvalid-auc:0.92090\n",
      "[1400]\ttrain-auc:0.92756\tvalid-auc:0.92111\n",
      "[1400]\ttrain-auc:0.92756\tvalid-auc:0.92111\n",
      "[1600]\ttrain-auc:0.92842\tvalid-auc:0.92120\n",
      "[1600]\ttrain-auc:0.92842\tvalid-auc:0.92120\n",
      "[1800]\ttrain-auc:0.92922\tvalid-auc:0.92127\n",
      "[1800]\ttrain-auc:0.92922\tvalid-auc:0.92127\n",
      "[1999]\ttrain-auc:0.92997\tvalid-auc:0.92131\n",
      "[1999]\ttrain-auc:0.92997\tvalid-auc:0.92131\n",
      "[22:40:02] Stage 1 OOF ROC-AUC: 0.92194\n",
      "[22:40:02] Stage 1 OOF ROC-AUC: 0.92194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Stage 1: XGBoost base model with StratifiedKFold OOF\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "oof_pred_stage1 = np.zeros(X_proc.shape[0])\n",
    "test_pred_stage1_folds = []\n",
    "\n",
    "params_stage1 = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "log(\"Training Stage 1 model (base XGBoost)...\")\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_proc, y), 1):\n",
    "    log(f\"Fold {fold}/{n_splits}\")\n",
    "    X_tr, X_val = X_proc[tr_idx], X_proc[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test_proc)\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params_stage1,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200,\n",
    "    )\n",
    "\n",
    "    oof_pred_stage1[val_idx] = booster.predict(dval, iteration_range=(0, booster.best_iteration + 1))\n",
    "    test_pred_stage1_folds.append(\n",
    "        booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1))\n",
    "    )\n",
    "\n",
    "auc_stage1 = roc_auc_score(y, oof_pred_stage1)\n",
    "log(f\"Stage 1 OOF ROC-AUC: {auc_stage1:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "139ec92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:02] Computing Stage 1 logits for OOF and test...\n",
      "[22:40:02] Training Stage 2 meta model with base_margin (logit boosting)...\n",
      "[22:40:02] Stage 2 Fold 1/5\n",
      "[0]\ttrain-auc:0.92168\tvalid-auc:0.92297\n",
      "[0]\ttrain-auc:0.92168\tvalid-auc:0.92297\n",
      "[99]\ttrain-auc:0.92189\tvalid-auc:0.92284\n",
      "[99]\ttrain-auc:0.92189\tvalid-auc:0.92284\n",
      "[22:40:08] Stage 2 Fold 2/5\n",
      "[0]\ttrain-auc:0.92175\tvalid-auc:0.92271\n",
      "[22:40:08] Stage 2 Fold 2/5\n",
      "[0]\ttrain-auc:0.92175\tvalid-auc:0.92271\n",
      "[99]\ttrain-auc:0.92188\tvalid-auc:0.92259\n",
      "[99]\ttrain-auc:0.92188\tvalid-auc:0.92259\n",
      "[22:40:13] Stage 2 Fold 3/5\n",
      "[0]\ttrain-auc:0.92221\tvalid-auc:0.92083\n",
      "[22:40:13] Stage 2 Fold 3/5\n",
      "[0]\ttrain-auc:0.92221\tvalid-auc:0.92083\n",
      "[99]\ttrain-auc:0.92235\tvalid-auc:0.92069\n",
      "[99]\ttrain-auc:0.92235\tvalid-auc:0.92069\n",
      "[22:40:20] Stage 2 Fold 4/5\n",
      "[22:40:20] Stage 2 Fold 4/5\n",
      "[0]\ttrain-auc:0.92196\tvalid-auc:0.92186\n",
      "[0]\ttrain-auc:0.92196\tvalid-auc:0.92186\n",
      "[100]\ttrain-auc:0.92210\tvalid-auc:0.92169\n",
      "[100]\ttrain-auc:0.92210\tvalid-auc:0.92169\n",
      "[22:40:25] Stage 2 Fold 5/5\n",
      "[0]\ttrain-auc:0.92209\tvalid-auc:0.92133\n",
      "[22:40:25] Stage 2 Fold 5/5\n",
      "[0]\ttrain-auc:0.92209\tvalid-auc:0.92133\n",
      "[99]\ttrain-auc:0.92225\tvalid-auc:0.92119\n",
      "[99]\ttrain-auc:0.92225\tvalid-auc:0.92119\n",
      "[22:40:31] Stage 2 OOF ROC-AUC: 0.92194\n",
      "[22:40:31] Stage 2 OOF ROC-AUC: 0.92194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5) Stage 2: Meta XGBoost boosting over Stage‑1 logits (base_margin trick)\n",
    "\n",
    "def prob_to_logit(p: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "log(\"Computing Stage 1 logits for OOF and test...\")\n",
    "\n",
    "logits_oof_stage1 = prob_to_logit(oof_pred_stage1)\n",
    "test_pred_stage1_folds = np.vstack(test_pred_stage1_folds)  # (n_splits, n_test)\n",
    "logits_test_stage1_folds = prob_to_logit(test_pred_stage1_folds)\n",
    "\n",
    "oof_pred_stage2 = np.zeros(X_proc.shape[0])\n",
    "test_pred_stage2_folds = []\n",
    "\n",
    "params_stage2 = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_lambda\": 1.5,\n",
    "    \"reg_alpha\": 0.2,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": RANDOM_STATE + 1,\n",
    "}\n",
    "\n",
    "log(\"Training Stage 2 meta model with base_margin (logit boosting)...\")\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_proc, y), 1):\n",
    "    log(f\"Stage 2 Fold {fold}/{n_splits}\")\n",
    "    X_tr, X_val = X_proc[tr_idx], X_proc[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test_proc)\n",
    "\n",
    "    # base_margin = Stage‑1 logits\n",
    "    dtrain.set_base_margin(logits_oof_stage1[tr_idx])\n",
    "    dval.set_base_margin(logits_oof_stage1[val_idx])\n",
    "    dtest.set_base_margin(logits_test_stage1_folds[fold - 1])\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n",
    "\n",
    "    booster_meta = xgb.train(\n",
    "        params_stage2,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200,\n",
    "    )\n",
    "\n",
    "    oof_pred_stage2[val_idx] = booster_meta.predict(\n",
    "        dval, iteration_range=(0, booster_meta.best_iteration + 1)\n",
    "    )\n",
    "    test_pred_stage2_folds.append(\n",
    "        booster_meta.predict(dtest, iteration_range=(0, booster_meta.best_iteration + 1))\n",
    "    )\n",
    "\n",
    "auc_stage2 = roc_auc_score(y, oof_pred_stage2)\n",
    "log(f\"Stage 2 OOF ROC-AUC: {auc_stage2:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52d27276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:31] Searching threshold on Stage 2 OOF probabilities...\n",
      "[22:40:32] Best threshold on OOF: 0.450 (F1=0.9430)\n",
      "=== Stage 1 (base model) ===\n",
      "[22:40:32] Best threshold on OOF: 0.450 (F1=0.9430)\n",
      "=== Stage 1 (base model) ===\n",
      "ROC-AUC : 0.92194\n",
      "Accuracy: 0.90480\n",
      "F1      : 0.94296\n",
      "LogLoss : 0.24518\n",
      "\n",
      "=== Stage 2 (meta boosted) ===\n",
      "ROC-AUC : 0.92194\n",
      "Accuracy: 0.90481\n",
      "F1      : 0.94296\n",
      "LogLoss : 0.24518\n",
      "ROC-AUC : 0.92194\n",
      "Accuracy: 0.90480\n",
      "F1      : 0.94296\n",
      "LogLoss : 0.24518\n",
      "\n",
      "=== Stage 2 (meta boosted) ===\n",
      "ROC-AUC : 0.92194\n",
      "Accuracy: 0.90481\n",
      "F1      : 0.94296\n",
      "LogLoss : 0.24518\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Compare Stage 1 vs Stage 2 and find a good classification threshold\n",
    "\n",
    "def evaluate_at_threshold(y_true, proba, thr: float) -> dict:\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, pred),\n",
    "        \"f1\": f1_score(y_true, pred),\n",
    "        \"logloss\": log_loss(y_true, proba),\n",
    "    }\n",
    "\n",
    "thr_grid = np.linspace(0.1, 0.9, 17)\n",
    "\n",
    "log(\"Searching threshold on Stage 2 OOF probabilities...\")\n",
    "best_thr = 0.5\n",
    "best_f1 = -1.0\n",
    "for thr in thr_grid:\n",
    "    metrics = evaluate_at_threshold(y, oof_pred_stage2, thr)\n",
    "    if metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"f1\"]\n",
    "        best_thr = thr\n",
    "\n",
    "log(f\"Best threshold on OOF: {best_thr:.3f} (F1={best_f1:.4f})\")\n",
    "\n",
    "metrics1 = evaluate_at_threshold(y, oof_pred_stage1, best_thr)\n",
    "metrics2 = evaluate_at_threshold(y, oof_pred_stage2, best_thr)\n",
    "\n",
    "print('=== Stage 1 (base model) ===')\n",
    "print(f\"ROC-AUC : {roc_auc_score(y, oof_pred_stage1):.5f}\")\n",
    "print(f\"Accuracy: {metrics1['accuracy']:.5f}\")\n",
    "print(f\"F1      : {metrics1['f1']:.5f}\")\n",
    "print(f\"LogLoss : {metrics1['logloss']:.5f}\")\n",
    "\n",
    "print('\\n=== Stage 2 (meta boosted) ===')\n",
    "print(f\"ROC-AUC : {roc_auc_score(y, oof_pred_stage2):.5f}\")\n",
    "print(f\"Accuracy: {metrics2['accuracy']:.5f}\")\n",
    "print(f\"F1      : {metrics2['f1']:.5f}\")\n",
    "print(f\"LogLoss : {metrics2['logloss']:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0549bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:33] Using Stage 1 base predictions for submission.\n",
      "[22:40:33] Saved submission to: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback/loan_meta_optimized_submission.csv\n",
      "[22:40:33] Saved submission to: /Users/lionelweng/Downloads/s5e11-Predicting-Loan-Payback/loan_meta_optimized_submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) Build final test predictions and submission file\n",
    "\n",
    "test_pred_stage1 = test_pred_stage1_folds.mean(axis=0)\n",
    "test_pred_stage2 = np.mean(np.vstack(test_pred_stage2_folds), axis=0)\n",
    "\n",
    "# If meta model improved ROC-AUC, we use Stage 2; otherwise fall back to Stage 1\n",
    "use_stage2 = auc_stage2 >= auc_stage1\n",
    "final_test_proba = test_pred_stage2 if use_stage2 else test_pred_stage1\n",
    "\n",
    "log(f\"Using {'Stage 2 meta' if use_stage2 else 'Stage 1 base'} predictions for submission.\")\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if id_col is not None:\n",
    "    sub[id_col] = test_df[id_col]\n",
    "else:\n",
    "    sub[\"id\"] = np.arange(len(test_df))\n",
    "\n",
    "sub[target_col] = final_test_proba\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "sub_path = Path(\"loan_meta_optimized_submission.csv\")\n",
    "sub.to_csv(sub_path, index=False)\n",
    "log(f\"Saved submission to: {sub_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b8442",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Quick domain insights\n",
    "\n",
    "Some intuitive risk directions that are helpful when *interpreting* feature importances\n",
    "or partial dependence plots (the model learns these patterns directly):\n",
    "\n",
    "- **Debt‑to‑Income Ratio** — higher ratio ⇒ typically **riskier** (harder to take on new debt).\n",
    "- **Credit Score** — lower score ⇒ **riskier**.\n",
    "- **Interest Rate** — higher interest ⇒ higher repayment burden ⇒ **riskier**.\n",
    "- **Annual Income** — lower income ⇒ **riskier**.\n",
    "- **Employment Status** — unemployed borrowers are **riskier** on average.\n",
    "- **Loan Grade / Subgrade** — poorer grades (e.g. *E, F, G*) encode higher credit risk.\n",
    "\n",
    "This notebook does not hard‑code these rules; instead, the boosted trees can learn\n",
    "non‑linear interactions between all of the above and more.\n",
    "You can plug SHAP / feature importance plots on top of the trained models to\n",
    "inspect whether the learned behaviour matches your expectations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
