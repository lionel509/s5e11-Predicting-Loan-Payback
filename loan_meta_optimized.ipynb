{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4a0969",
   "metadata": {},
   "source": [
    "# Loan Payback — Fold-Safe XGBoost (95% ROC-AUC Target)\n",
    "\n",
    "This notebook builds a **fold-safe imbalance-aware XGBoost model** with advanced feature engineering:\n",
    "\n",
    "## Key Features:\n",
    "1. **Fold-Safe Preprocessing**: All transformations fit inside CV loop to prevent leakage\n",
    "2. **Advanced Feature Engineering**: Log transforms, ratios, interactions, target encoding, missing indicators\n",
    "3. **Imbalance Handling**: Automatic `scale_pos_weight` calculation\n",
    "4. **5-Fold Stratified CV** with proper validation\n",
    "5. **Hyperparameter Optimization**: Optuna search for optimal parameters\n",
    "6. **Target: 95%+ ROC-AUC**\n",
    "\n",
    "## Architecture:\n",
    "- Single strong XGBoost with careful regularization\n",
    "- Per-fold feature engineering and preprocessing\n",
    "- Target encoding with proper fold isolation\n",
    "- Missing value indicators as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports & basic configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# For hyperparameter optimization (optional - uncomment if using)\n",
    "# import optuna\n",
    "\n",
    "RANDOM_STATE = 999\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_DIR = Path(\"Data\")\n",
    "\n",
    "def log(msg: str):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb6aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:16:45] Using train: train.csv\n",
      "[02:16:45] Using test : test.csv\n",
      "[02:16:45] Train shape: (593994, 13)\n",
      "[02:16:45] Test  shape: (254569, 12)\n",
      "[02:16:45] Detected target column: loan_paid_back\n",
      "[02:16:45] Detected id column: id\n",
      "[02:16:45] Class distribution: 119500 negatives, 474494 positives\n",
      "[02:16:45] scale_pos_weight = 0.2518\n",
      "[02:16:45] Number of base features: 11\n",
      "[02:16:45] Train shape: (593994, 13)\n",
      "[02:16:45] Test  shape: (254569, 12)\n",
      "[02:16:45] Detected target column: loan_paid_back\n",
      "[02:16:45] Detected id column: id\n",
      "[02:16:45] Class distribution: 119500 negatives, 474494 positives\n",
      "[02:16:45] scale_pos_weight = 0.2518\n",
      "[02:16:45] Number of base features: 11\n"
     ]
    }
   ],
   "source": [
    "# 2) Data loading and automatic target / id detection\n",
    "\n",
    "train_path = None\n",
    "test_path = None\n",
    "\n",
    "# Heuristic: pick first train/test-looking CSVs\n",
    "csv_files = sorted(list(DATA_DIR.glob(\"*.csv\")))\n",
    "for p in csv_files:\n",
    "    name = p.name.lower()\n",
    "    if \"train\" in name and train_path is None:\n",
    "        train_path = p\n",
    "    if \"test\" in name and test_path is None and \"train\" not in name:\n",
    "        test_path = p\n",
    "\n",
    "if train_path is None or test_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not detect train/test CSVs inside {DATA_DIR}. \"\n",
    "        \"Please set train_path and test_path manually.\"\n",
    "    )\n",
    "\n",
    "log(f\"Using train: {train_path.name}\")\n",
    "log(f\"Using test : {test_path.name}\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "log(f\"Train shape: {train_df.shape}\")\n",
    "log(f\"Test  shape: {test_df.shape}\")\n",
    "\n",
    "def detect_target(train_df: pd.DataFrame, test_df: pd.DataFrame) -> str:\n",
    "    diff = list(set(train_df.columns) - set(test_df.columns))\n",
    "    # Prefer a binary label\n",
    "    candidates = []\n",
    "    for c in diff:\n",
    "        if train_df[c].nunique() <= 3:\n",
    "            candidates.append(c)\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    if len(diff) == 1:\n",
    "        return diff[0]\n",
    "    for name in [\"loan_paid_back\", \"target\", \"label\", \"is_default\", \"default\", \"paid\"]:\n",
    "        if name in train_df.columns and name not in test_df.columns:\n",
    "            return name\n",
    "    raise ValueError(f\"Could not detect target. Diff columns: {diff}\")\n",
    "\n",
    "target_col = detect_target(train_df, test_df)\n",
    "log(f\"Detected target column: {target_col}\")\n",
    "\n",
    "# Simple ID detection: column whose values are unique in train and test\n",
    "id_col = None\n",
    "for col in train_df.columns:\n",
    "    if col == target_col:\n",
    "        continue\n",
    "    if col in test_df.columns:\n",
    "        if train_df[col].is_unique and test_df[col].is_unique:\n",
    "            id_col = col\n",
    "            break\n",
    "\n",
    "log(f\"Detected id column: {id_col}\")\n",
    "\n",
    "y = train_df[target_col].astype(int).values\n",
    "\n",
    "# Compute class imbalance for scale_pos_weight\n",
    "n_pos = (y == 1).sum()\n",
    "n_neg = (y == 0).sum()\n",
    "scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "log(f\"Class distribution: {n_neg} negatives, {n_pos} positives\")\n",
    "log(f\"scale_pos_weight = {scale_pos_weight:.4f}\")\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in [target_col, id_col]]\n",
    "X_raw = train_df[feature_cols].copy()\n",
    "X_test_raw = test_df[feature_cols].copy()\n",
    "\n",
    "log(f\"Number of base features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be81abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Feature Engineering Functions (Fold-Safe)\n",
    "\n",
    "def add_missing_indicators(df: pd.DataFrame, missing_cols: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Add binary missing indicators for columns with >2% missing values.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if missing_cols is None:\n",
    "        # Detect columns with significant missing values\n",
    "        missing_cols = []\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().sum() / len(df) > 0.02:\n",
    "                missing_cols.append(col)\n",
    "    \n",
    "    for col in missing_cols:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_missing\"] = df[col].isnull().astype(int)\n",
    "    \n",
    "    return df, missing_cols\n",
    "\n",
    "def engineer_features(df: pd.DataFrame, fit_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create advanced features. If fit_df is provided, compute statistics from it.\n",
    "    Otherwise, compute from df itself (for fold training data).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    ref_df = fit_df if fit_df is not None else df\n",
    "    \n",
    "    # Map common column name variations (case-insensitive)\n",
    "    col_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        col_map[col_lower] = col\n",
    "    \n",
    "    # Log transforms for skewed amounts\n",
    "    if \"loan_amount\" in col_map or \"loan_amnt\" in col_map:\n",
    "        loan_col = col_map.get(\"loan_amount\", col_map.get(\"loan_amnt\"))\n",
    "        df[\"loan_amount_log\"] = np.log1p(df[loan_col])\n",
    "    \n",
    "    if \"annual_income\" in col_map or \"annual_inc\" in col_map:\n",
    "        income_col = col_map.get(\"annual_income\", col_map.get(\"annual_inc\"))\n",
    "        df[\"annual_income_log\"] = np.log1p(df[income_col])\n",
    "    \n",
    "    # Income per loan ratio\n",
    "    loan_col = col_map.get(\"loan_amount\", col_map.get(\"loan_amnt\"))\n",
    "    income_col = col_map.get(\"annual_income\", col_map.get(\"annual_inc\"))\n",
    "    if loan_col and income_col:\n",
    "        df[\"income_per_loan\"] = df[income_col] / (df[loan_col] + 1)\n",
    "        df[\"loan_to_income\"] = df[loan_col] / (df[income_col] + 1)\n",
    "    \n",
    "    # DTI × Interest Rate interaction\n",
    "    dti_col = col_map.get(\"debt_to_income_ratio\", col_map.get(\"dti\"))\n",
    "    rate_col = col_map.get(\"interest_rate\", col_map.get(\"int_rate\"))\n",
    "    if dti_col and rate_col:\n",
    "        df[\"dti_x_rate\"] = df[dti_col] * df[rate_col]\n",
    "        df[\"dti_div_rate\"] = df[dti_col] / (df[rate_col] + 0.01)\n",
    "    \n",
    "    # Normalized credit score (using statistics from reference df)\n",
    "    credit_col = col_map.get(\"credit_score\", col_map.get(\"fico_range_low\"))\n",
    "    if credit_col:\n",
    "        mean_credit = ref_df[credit_col].mean()\n",
    "        std_credit = ref_df[credit_col].std()\n",
    "        if std_credit > 0:\n",
    "            df[\"credit_score_norm\"] = (df[credit_col] - mean_credit) / std_credit\n",
    "    \n",
    "    # Percentile rank for credit score\n",
    "    if credit_col:\n",
    "        df[\"credit_score_rank\"] = df[credit_col].rank(pct=True)\n",
    "    \n",
    "    # Loan amount squared (for non-linear relationships)\n",
    "    if loan_col:\n",
    "        df[\"loan_amount_sq\"] = df[loan_col] ** 2\n",
    "    \n",
    "    # Interest rate interactions\n",
    "    if rate_col and income_col:\n",
    "        df[\"income_x_rate\"] = df[income_col] * df[rate_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def target_encode_categorical(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    cat_cols: list,\n",
    "    target: np.ndarray,\n",
    "    smoothing: float = 10.0\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Target encode categorical columns with smoothing.\n",
    "    Computes mean target per category on train, applies to val and test.\n",
    "    \"\"\"\n",
    "    train_encoded = train_df.copy()\n",
    "    val_encoded = val_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    \n",
    "    global_mean = target.mean()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Compute target statistics per category\n",
    "        target_stats = pd.DataFrame({\n",
    "            col: train_df[col],\n",
    "            'target': target\n",
    "        }).groupby(col)['target'].agg(['sum', 'count'])\n",
    "        \n",
    "        # Smoothed target encoding\n",
    "        target_stats['target_enc'] = (\n",
    "            (target_stats['sum'] + smoothing * global_mean) /\n",
    "            (target_stats['count'] + smoothing)\n",
    "        )\n",
    "        \n",
    "        # Create mapping\n",
    "        encoding_map = target_stats['target_enc'].to_dict()\n",
    "        \n",
    "        # Apply encoding\n",
    "        train_encoded[f\"{col}_target_enc\"] = train_df[col].map(encoding_map).fillna(global_mean)\n",
    "        val_encoded[f\"{col}_target_enc\"] = val_df[col].map(encoding_map).fillna(global_mean)\n",
    "        test_encoded[f\"{col}_target_enc\"] = test_df[col].map(encoding_map).fillna(global_mean)\n",
    "    \n",
    "    return train_encoded, val_encoded, test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b36608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Preprocessing Functions (Fold-Safe)\n",
    "\n",
    "def preprocess_fold(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_val: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    numeric_cols: list,\n",
    "    categorical_cols: list\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocess data for a single fold.\n",
    "    Fit transformers on X_train only, then transform all sets.\n",
    "    \"\"\"\n",
    "    from scipy import sparse\n",
    "    \n",
    "    # 1) Add missing indicators\n",
    "    X_train_eng, missing_cols = add_missing_indicators(X_train)\n",
    "    X_val_eng, _ = add_missing_indicators(X_val, missing_cols)\n",
    "    X_test_eng, _ = add_missing_indicators(X_test, missing_cols)\n",
    "    \n",
    "    # 2) Engineer features (fold-safe: use X_train stats)\n",
    "    X_train_eng = engineer_features(X_train_eng, fit_df=X_train_eng)\n",
    "    X_val_eng = engineer_features(X_val_eng, fit_df=X_train_eng)\n",
    "    X_test_eng = engineer_features(X_test_eng, fit_df=X_train_eng)\n",
    "    \n",
    "    # 3) Target encode categoricals\n",
    "    X_train_eng, X_val_eng, X_test_eng = target_encode_categorical(\n",
    "        X_train_eng, X_val_eng, X_test_eng,\n",
    "        categorical_cols, y_train, smoothing=10.0\n",
    "    )\n",
    "    \n",
    "    # 4) Update column lists with new features\n",
    "    numeric_cols_ext = X_train_eng.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols_ext = X_train_eng.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    \n",
    "    # 5) Impute numerics\n",
    "    numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_num = numeric_imputer.fit_transform(X_train_eng[numeric_cols_ext])\n",
    "    X_val_num = numeric_imputer.transform(X_val_eng[numeric_cols_ext])\n",
    "    X_test_num = numeric_imputer.transform(X_test_eng[numeric_cols_ext])\n",
    "    \n",
    "    # 6) One-hot encode categoricals (sparse)\n",
    "    if len(categorical_cols_ext) > 0:\n",
    "        cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, drop='if_binary')\n",
    "        X_train_cat = cat_encoder.fit_transform(X_train_eng[categorical_cols_ext])\n",
    "        X_val_cat = cat_encoder.transform(X_val_eng[categorical_cols_ext])\n",
    "        X_test_cat = cat_encoder.transform(X_test_eng[categorical_cols_ext])\n",
    "        \n",
    "        # Combine numeric and categorical\n",
    "        X_train_final = sparse.hstack([X_train_num, X_train_cat])\n",
    "        X_val_final = sparse.hstack([X_val_num, X_val_cat])\n",
    "        X_test_final = sparse.hstack([X_test_num, X_test_cat])\n",
    "    else:\n",
    "        X_train_final = X_train_num\n",
    "        X_val_final = X_val_num\n",
    "        X_test_final = X_test_num\n",
    "    \n",
    "    return X_train_final, X_val_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed116d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:19:24] Base numeric features    : 5\n",
      "[12:19:24] Base categorical features: 6\n",
      "[12:19:24] Using overridden params from search.\n",
      "[12:19:24] \n",
      "Training Fold-Safe XGBoost model...\n",
      "[12:19:24] Params: max_depth=8, min_child_weight=20, eta=0.030, scale_pos_weight=0.301\n",
      "[12:19:24] \n",
      "============================================================\n",
      "[12:19:24] Fold 1/5\n",
      "[12:19:24] ============================================================\n",
      "[12:19:26] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[12:19:26] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[0]\ttrain-auc:0.91308\tvalid-auc:0.91329\n",
      "[0]\ttrain-auc:0.91308\tvalid-auc:0.91329\n",
      "[100]\ttrain-auc:0.91781\tvalid-auc:0.91723\n",
      "[100]\ttrain-auc:0.91781\tvalid-auc:0.91723\n",
      "[200]\ttrain-auc:0.92091\tvalid-auc:0.91906\n",
      "[200]\ttrain-auc:0.92091\tvalid-auc:0.91906\n",
      "[300]\ttrain-auc:0.92392\tvalid-auc:0.92019\n",
      "[300]\ttrain-auc:0.92392\tvalid-auc:0.92019\n",
      "[400]\ttrain-auc:0.92621\tvalid-auc:0.92075\n",
      "[400]\ttrain-auc:0.92621\tvalid-auc:0.92075\n",
      "[500]\ttrain-auc:0.92833\tvalid-auc:0.92124\n",
      "[500]\ttrain-auc:0.92833\tvalid-auc:0.92124\n",
      "[600]\ttrain-auc:0.93010\tvalid-auc:0.92154\n",
      "[600]\ttrain-auc:0.93010\tvalid-auc:0.92154\n",
      "[700]\ttrain-auc:0.93136\tvalid-auc:0.92172\n",
      "[700]\ttrain-auc:0.93136\tvalid-auc:0.92172\n",
      "[800]\ttrain-auc:0.93225\tvalid-auc:0.92176\n",
      "[800]\ttrain-auc:0.93225\tvalid-auc:0.92176\n",
      "[900]\ttrain-auc:0.93296\tvalid-auc:0.92181\n",
      "[900]\ttrain-auc:0.93296\tvalid-auc:0.92181\n",
      "[1000]\ttrain-auc:0.93368\tvalid-auc:0.92184\n",
      "[1000]\ttrain-auc:0.93368\tvalid-auc:0.92184\n",
      "[1100]\ttrain-auc:0.93417\tvalid-auc:0.92185\n",
      "[1100]\ttrain-auc:0.93417\tvalid-auc:0.92185\n",
      "[1200]\ttrain-auc:0.93462\tvalid-auc:0.92185\n",
      "[1200]\ttrain-auc:0.93462\tvalid-auc:0.92185\n",
      "[1290]\ttrain-auc:0.93494\tvalid-auc:0.92184\n",
      "[1290]\ttrain-auc:0.93494\tvalid-auc:0.92184\n",
      "[12:20:47] Fold 1 ROC-AUC: 0.921864\n",
      "[12:20:47] \n",
      "============================================================\n",
      "[12:20:47] Fold 2/5\n",
      "[12:20:47] ============================================================\n",
      "[12:20:47] Fold 1 ROC-AUC: 0.921864\n",
      "[12:20:47] \n",
      "============================================================\n",
      "[12:20:47] Fold 2/5\n",
      "[12:20:47] ============================================================\n",
      "[12:20:49] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[12:20:49] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[0]\ttrain-auc:0.91300\tvalid-auc:0.91296\n",
      "[0]\ttrain-auc:0.91300\tvalid-auc:0.91296\n",
      "[100]\ttrain-auc:0.91770\tvalid-auc:0.91711\n",
      "[100]\ttrain-auc:0.91770\tvalid-auc:0.91711\n",
      "[200]\ttrain-auc:0.92098\tvalid-auc:0.91892\n",
      "[200]\ttrain-auc:0.92098\tvalid-auc:0.91892\n",
      "[300]\ttrain-auc:0.92352\tvalid-auc:0.91987\n",
      "[300]\ttrain-auc:0.92352\tvalid-auc:0.91987\n",
      "[400]\ttrain-auc:0.92622\tvalid-auc:0.92067\n",
      "[400]\ttrain-auc:0.92622\tvalid-auc:0.92067\n",
      "[500]\ttrain-auc:0.92827\tvalid-auc:0.92107\n",
      "[500]\ttrain-auc:0.92827\tvalid-auc:0.92107\n",
      "[600]\ttrain-auc:0.92999\tvalid-auc:0.92136\n",
      "[600]\ttrain-auc:0.92999\tvalid-auc:0.92136\n",
      "[700]\ttrain-auc:0.93134\tvalid-auc:0.92157\n",
      "[700]\ttrain-auc:0.93134\tvalid-auc:0.92157\n",
      "[800]\ttrain-auc:0.93234\tvalid-auc:0.92167\n",
      "[800]\ttrain-auc:0.93234\tvalid-auc:0.92167\n",
      "[900]\ttrain-auc:0.93302\tvalid-auc:0.92168\n",
      "[900]\ttrain-auc:0.93302\tvalid-auc:0.92168\n",
      "[1000]\ttrain-auc:0.93360\tvalid-auc:0.92168\n",
      "[1000]\ttrain-auc:0.93360\tvalid-auc:0.92168\n",
      "[1100]\ttrain-auc:0.93424\tvalid-auc:0.92170\n",
      "[1100]\ttrain-auc:0.93424\tvalid-auc:0.92170\n",
      "[1200]\ttrain-auc:0.93459\tvalid-auc:0.92171\n",
      "[1200]\ttrain-auc:0.93459\tvalid-auc:0.92171\n",
      "[1300]\ttrain-auc:0.93505\tvalid-auc:0.92172\n",
      "[1300]\ttrain-auc:0.93505\tvalid-auc:0.92172\n",
      "[1400]\ttrain-auc:0.93543\tvalid-auc:0.92175\n",
      "[1400]\ttrain-auc:0.93543\tvalid-auc:0.92175\n",
      "[1500]\ttrain-auc:0.93573\tvalid-auc:0.92175\n",
      "[1500]\ttrain-auc:0.93573\tvalid-auc:0.92175\n",
      "[1524]\ttrain-auc:0.93580\tvalid-auc:0.92175\n",
      "[1524]\ttrain-auc:0.93580\tvalid-auc:0.92175\n",
      "[12:22:24] Fold 2 ROC-AUC: 0.921755\n",
      "[12:22:24] \n",
      "============================================================\n",
      "[12:22:24] Fold 3/5\n",
      "[12:22:24] ============================================================\n",
      "[12:22:24] Fold 2 ROC-AUC: 0.921755\n",
      "[12:22:24] \n",
      "============================================================\n",
      "[12:22:24] Fold 3/5\n",
      "[12:22:24] ============================================================\n",
      "[12:22:26] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[12:22:26] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[0]\ttrain-auc:0.91361\tvalid-auc:0.91088\n",
      "[0]\ttrain-auc:0.91361\tvalid-auc:0.91088\n",
      "[100]\ttrain-auc:0.91824\tvalid-auc:0.91494\n",
      "[100]\ttrain-auc:0.91824\tvalid-auc:0.91494\n",
      "[200]\ttrain-auc:0.92117\tvalid-auc:0.91665\n",
      "[200]\ttrain-auc:0.92117\tvalid-auc:0.91665\n",
      "[300]\ttrain-auc:0.92390\tvalid-auc:0.91781\n",
      "[300]\ttrain-auc:0.92390\tvalid-auc:0.91781\n",
      "[400]\ttrain-auc:0.92621\tvalid-auc:0.91861\n",
      "[400]\ttrain-auc:0.92621\tvalid-auc:0.91861\n",
      "[500]\ttrain-auc:0.92827\tvalid-auc:0.91915\n",
      "[500]\ttrain-auc:0.92827\tvalid-auc:0.91915\n",
      "[600]\ttrain-auc:0.92996\tvalid-auc:0.91953\n",
      "[600]\ttrain-auc:0.92996\tvalid-auc:0.91953\n",
      "[700]\ttrain-auc:0.93136\tvalid-auc:0.91975\n",
      "[700]\ttrain-auc:0.93136\tvalid-auc:0.91975\n",
      "[800]\ttrain-auc:0.93236\tvalid-auc:0.91992\n",
      "[800]\ttrain-auc:0.93236\tvalid-auc:0.91992\n",
      "[900]\ttrain-auc:0.93311\tvalid-auc:0.92000\n",
      "[900]\ttrain-auc:0.93311\tvalid-auc:0.92000\n",
      "[1000]\ttrain-auc:0.93373\tvalid-auc:0.92012\n",
      "[1000]\ttrain-auc:0.93373\tvalid-auc:0.92012\n",
      "[1100]\ttrain-auc:0.93423\tvalid-auc:0.92017\n",
      "[1100]\ttrain-auc:0.93423\tvalid-auc:0.92017\n",
      "[1200]\ttrain-auc:0.93459\tvalid-auc:0.92022\n",
      "[1200]\ttrain-auc:0.93459\tvalid-auc:0.92022\n",
      "[1300]\ttrain-auc:0.93490\tvalid-auc:0.92021\n",
      "[1300]\ttrain-auc:0.93490\tvalid-auc:0.92021\n",
      "[1400]\ttrain-auc:0.93522\tvalid-auc:0.92024\n",
      "[1400]\ttrain-auc:0.93522\tvalid-auc:0.92024\n",
      "[1500]\ttrain-auc:0.93554\tvalid-auc:0.92027\n",
      "[1500]\ttrain-auc:0.93554\tvalid-auc:0.92027\n",
      "[1600]\ttrain-auc:0.93582\tvalid-auc:0.92028\n",
      "[1600]\ttrain-auc:0.93582\tvalid-auc:0.92028\n",
      "[1700]\ttrain-auc:0.93616\tvalid-auc:0.92027\n",
      "[1700]\ttrain-auc:0.93616\tvalid-auc:0.92027\n",
      "[1800]\ttrain-auc:0.93638\tvalid-auc:0.92028\n",
      "[1800]\ttrain-auc:0.93638\tvalid-auc:0.92028\n",
      "[1900]\ttrain-auc:0.93655\tvalid-auc:0.92029\n",
      "[1900]\ttrain-auc:0.93655\tvalid-auc:0.92029\n",
      "[2000]\ttrain-auc:0.93669\tvalid-auc:0.92029\n",
      "[2000]\ttrain-auc:0.93669\tvalid-auc:0.92029\n",
      "[2100]\ttrain-auc:0.93690\tvalid-auc:0.92030\n",
      "[2100]\ttrain-auc:0.93690\tvalid-auc:0.92030\n",
      "[2200]\ttrain-auc:0.93709\tvalid-auc:0.92031\n",
      "[2200]\ttrain-auc:0.93709\tvalid-auc:0.92031\n",
      "[2300]\ttrain-auc:0.93724\tvalid-auc:0.92029\n",
      "[2300]\ttrain-auc:0.93724\tvalid-auc:0.92029\n",
      "[2318]\ttrain-auc:0.93728\tvalid-auc:0.92029\n",
      "[2318]\ttrain-auc:0.93728\tvalid-auc:0.92029\n",
      "[12:24:44] Fold 3 ROC-AUC: 0.920318\n",
      "[12:24:44] \n",
      "============================================================\n",
      "[12:24:44] Fold 4/5\n",
      "[12:24:44] ============================================================\n",
      "[12:24:44] Fold 3 ROC-AUC: 0.920318\n",
      "[12:24:44] \n",
      "============================================================\n",
      "[12:24:44] Fold 4/5\n",
      "[12:24:44] ============================================================\n",
      "[12:24:46] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[12:24:46] Processed shapes - Train: (475195, 76), Val: (118799, 76), Test: (254569, 76)\n",
      "[0]\ttrain-auc:0.91326\tvalid-auc:0.91226\n",
      "[0]\ttrain-auc:0.91326\tvalid-auc:0.91226\n",
      "[100]\ttrain-auc:0.91805\tvalid-auc:0.91636\n",
      "[100]\ttrain-auc:0.91805\tvalid-auc:0.91636\n",
      "[200]\ttrain-auc:0.92120\tvalid-auc:0.91812\n",
      "[200]\ttrain-auc:0.92120\tvalid-auc:0.91812\n",
      "[300]\ttrain-auc:0.92387\tvalid-auc:0.91900\n",
      "[300]\ttrain-auc:0.92387\tvalid-auc:0.91900\n",
      "[400]\ttrain-auc:0.92633\tvalid-auc:0.91979\n",
      "[400]\ttrain-auc:0.92633\tvalid-auc:0.91979\n",
      "[500]\ttrain-auc:0.92845\tvalid-auc:0.92036\n",
      "[500]\ttrain-auc:0.92845\tvalid-auc:0.92036\n",
      "[600]\ttrain-auc:0.93009\tvalid-auc:0.92068\n",
      "[600]\ttrain-auc:0.93009\tvalid-auc:0.92068\n",
      "[700]\ttrain-auc:0.93126\tvalid-auc:0.92085\n",
      "[700]\ttrain-auc:0.93126\tvalid-auc:0.92085\n",
      "[800]\ttrain-auc:0.93222\tvalid-auc:0.92089\n",
      "[800]\ttrain-auc:0.93222\tvalid-auc:0.92089\n",
      "[900]\ttrain-auc:0.93300\tvalid-auc:0.92092\n",
      "[900]\ttrain-auc:0.93300\tvalid-auc:0.92092\n",
      "[1000]\ttrain-auc:0.93367\tvalid-auc:0.92098\n",
      "[1000]\ttrain-auc:0.93367\tvalid-auc:0.92098\n",
      "[1100]\ttrain-auc:0.93418\tvalid-auc:0.92100\n",
      "[1100]\ttrain-auc:0.93418\tvalid-auc:0.92100\n",
      "[1200]\ttrain-auc:0.93458\tvalid-auc:0.92101\n",
      "[1200]\ttrain-auc:0.93458\tvalid-auc:0.92101\n",
      "[1300]\ttrain-auc:0.93487\tvalid-auc:0.92103\n",
      "[1300]\ttrain-auc:0.93487\tvalid-auc:0.92103\n",
      "[1400]\ttrain-auc:0.93527\tvalid-auc:0.92104\n",
      "[1400]\ttrain-auc:0.93527\tvalid-auc:0.92104\n",
      "[1500]\ttrain-auc:0.93569\tvalid-auc:0.92103\n",
      "[1500]\ttrain-auc:0.93569\tvalid-auc:0.92103\n",
      "[1594]\ttrain-auc:0.93601\tvalid-auc:0.92104\n",
      "[1594]\ttrain-auc:0.93601\tvalid-auc:0.92104\n",
      "[12:26:22] Fold 4 ROC-AUC: 0.921049\n",
      "[12:26:22] \n",
      "============================================================\n",
      "[12:26:22] Fold 5/5\n",
      "[12:26:22] ============================================================\n",
      "[12:26:22] Fold 4 ROC-AUC: 0.921049\n",
      "[12:26:22] \n",
      "============================================================\n",
      "[12:26:22] Fold 5/5\n",
      "[12:26:22] ============================================================\n",
      "[12:26:25] Processed shapes - Train: (475196, 76), Val: (118798, 76), Test: (254569, 76)\n",
      "[12:26:25] Processed shapes - Train: (475196, 76), Val: (118798, 76), Test: (254569, 76)\n",
      "[0]\ttrain-auc:0.91347\tvalid-auc:0.91073\n",
      "[0]\ttrain-auc:0.91347\tvalid-auc:0.91073\n",
      "[100]\ttrain-auc:0.91811\tvalid-auc:0.91540\n",
      "[100]\ttrain-auc:0.91811\tvalid-auc:0.91540\n",
      "[200]\ttrain-auc:0.92104\tvalid-auc:0.91724\n",
      "[200]\ttrain-auc:0.92104\tvalid-auc:0.91724\n",
      "[300]\ttrain-auc:0.92371\tvalid-auc:0.91827\n",
      "[300]\ttrain-auc:0.92371\tvalid-auc:0.91827\n",
      "[400]\ttrain-auc:0.92617\tvalid-auc:0.91911\n",
      "[400]\ttrain-auc:0.92617\tvalid-auc:0.91911\n",
      "[500]\ttrain-auc:0.92822\tvalid-auc:0.91970\n",
      "[500]\ttrain-auc:0.92822\tvalid-auc:0.91970\n",
      "[600]\ttrain-auc:0.92999\tvalid-auc:0.92009\n",
      "[600]\ttrain-auc:0.92999\tvalid-auc:0.92009\n",
      "[700]\ttrain-auc:0.93127\tvalid-auc:0.92022\n",
      "[700]\ttrain-auc:0.93127\tvalid-auc:0.92022\n",
      "[800]\ttrain-auc:0.93224\tvalid-auc:0.92031\n",
      "[800]\ttrain-auc:0.93224\tvalid-auc:0.92031\n",
      "[900]\ttrain-auc:0.93291\tvalid-auc:0.92037\n",
      "[900]\ttrain-auc:0.93291\tvalid-auc:0.92037\n",
      "[1000]\ttrain-auc:0.93350\tvalid-auc:0.92041\n",
      "[1000]\ttrain-auc:0.93350\tvalid-auc:0.92041\n",
      "[1100]\ttrain-auc:0.93405\tvalid-auc:0.92041\n",
      "[1100]\ttrain-auc:0.93405\tvalid-auc:0.92041\n",
      "[1200]\ttrain-auc:0.93447\tvalid-auc:0.92042\n",
      "[1200]\ttrain-auc:0.93447\tvalid-auc:0.92042\n",
      "[1300]\ttrain-auc:0.93494\tvalid-auc:0.92044\n",
      "[1300]\ttrain-auc:0.93494\tvalid-auc:0.92044\n",
      "[1400]\ttrain-auc:0.93538\tvalid-auc:0.92048\n",
      "[1400]\ttrain-auc:0.93538\tvalid-auc:0.92048\n",
      "[1500]\ttrain-auc:0.93559\tvalid-auc:0.92048\n",
      "[1500]\ttrain-auc:0.93559\tvalid-auc:0.92048\n",
      "[1600]\ttrain-auc:0.93577\tvalid-auc:0.92048\n",
      "[1600]\ttrain-auc:0.93577\tvalid-auc:0.92048\n",
      "[1613]\ttrain-auc:0.93584\tvalid-auc:0.92049\n",
      "[1613]\ttrain-auc:0.93584\tvalid-auc:0.92049\n",
      "[12:28:02] Fold 5 ROC-AUC: 0.920492\n",
      "[12:28:03] \n",
      "============================================================\n",
      "[12:28:03] Overall OOF ROC-AUC: 0.921090\n",
      "[12:28:03] ============================================================\n",
      "[12:28:03] ⚠ Current: 0.9211, Target: 0.9500 (Gap: 0.0289)\n",
      "[12:28:02] Fold 5 ROC-AUC: 0.920492\n",
      "[12:28:03] \n",
      "============================================================\n",
      "[12:28:03] Overall OOF ROC-AUC: 0.921090\n",
      "[12:28:03] ============================================================\n",
      "[12:28:03] ⚠ Current: 0.9211, Target: 0.9500 (Gap: 0.0289)\n"
     ]
    }
   ],
   "source": [
    "# 5) Main Training Loop - Fold-Safe XGBoost\n",
    "\n",
    "# Identify column types from raw data\n",
    "numeric_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_raw.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "log(f\"Base numeric features    : {len(numeric_cols)}\")\n",
    "log(f\"Base categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Cross-validation setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "oof_predictions = np.zeros(len(X_raw))\n",
    "test_predictions_folds = []\n",
    "\n",
    "# Optimized parameters targeting 95% ROC-AUC\n",
    "_default_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 8,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"subsample\": 0.85,\n",
    "    \"colsample_bytree\": 0.85,\n",
    "    \"colsample_bylevel\": 0.85,\n",
    "    \"reg_lambda\": 2.0,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"gamma\": 1.0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "# If a tuning cell populated `params`, merge it over defaults\n",
    "try:\n",
    "    params  # noqa: F821\n",
    "    params = {**_default_params, **params}\n",
    "    log(\"Using overridden params from search.\")\n",
    "except NameError:\n",
    "    params = _default_params\n",
    "\n",
    "log(\"\\nTraining Fold-Safe XGBoost model...\")\n",
    "log(f\"Params: max_depth={params['max_depth']}, \"\n",
    "    f\"min_child_weight={params['min_child_weight']}, \"\n",
    "    f\"eta={params['learning_rate']:.3f}, \"\n",
    "    f\"scale_pos_weight={params['scale_pos_weight']:.3f}\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_raw, y), 1):\n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"Fold {fold}/{n_splits}\")\n",
    "    log(f\"{'='*60}\")\n",
    "    \n",
    "    # Split raw data\n",
    "    X_train_fold = X_raw.iloc[train_idx].copy()\n",
    "    X_val_fold = X_raw.iloc[val_idx].copy()\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # Preprocess fold (fit on train, transform all)\n",
    "    X_train_proc, X_val_proc, X_test_proc = preprocess_fold(\n",
    "        X_train_fold, X_val_fold, X_test_raw,\n",
    "        y_train_fold, numeric_cols, categorical_cols\n",
    "    )\n",
    "    \n",
    "    log(f\"Processed shapes - Train: {X_train_proc.shape}, Val: {X_val_proc.shape}, Test: {X_test_proc.shape}\")\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train_proc, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(X_val_proc, label=y_val_fold)\n",
    "    dtest = xgb.DMatrix(X_test_proc)\n",
    "    \n",
    "    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n",
    "    \n",
    "    # Train with more rounds for high AUC\n",
    "    booster = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=5000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=150,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    oof_predictions[val_idx] = booster.predict(dval, iteration_range=(0, booster.best_iteration + 1))\n",
    "    test_predictions_folds.append(\n",
    "        booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1))\n",
    "    )\n",
    "    \n",
    "    # Fold metrics\n",
    "    fold_auc = roc_auc_score(y_val_fold, oof_predictions[val_idx])\n",
    "    log(f\"Fold {fold} ROC-AUC: {fold_auc:.6f}\")\n",
    "\n",
    "# Overall OOF ROC-AUC\n",
    "oof_roc_auc = roc_auc_score(y, oof_predictions)\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(f\"Overall OOF ROC-AUC: {oof_roc_auc:.6f}\")\n",
    "log(f\"{'='*60}\")\n",
    "\n",
    "if oof_roc_auc >= 0.95:\n",
    "    log(\"✓ TARGET ACHIEVED: 95%+ ROC-AUC!\")\n",
    "else:\n",
    "    log(f\"⚠ Current: {oof_roc_auc:.4f}, Target: 0.9500 (Gap: {0.95 - oof_roc_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139ec92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:27:11] \\nSearching for optimal F1 threshold on OOF predictions...\n",
      "[02:27:12] Best threshold: 0.200 (F1=0.9428)\n",
      "\\n============================================================\n",
      "=== Model Performance Summary ===\n",
      "============================================================\n",
      "ROC-AUC  : 0.920962 ⚠\n",
      "Accuracy : 0.904822\n",
      "F1 Score : 0.942799\n",
      "LogLoss  : 0.322461\n",
      "Threshold: 0.200\n",
      "============================================================\n",
      "[02:27:12] Best threshold: 0.200 (F1=0.9428)\n",
      "\\n============================================================\n",
      "=== Model Performance Summary ===\n",
      "============================================================\n",
      "ROC-AUC  : 0.920962 ⚠\n",
      "Accuracy : 0.904822\n",
      "F1 Score : 0.942799\n",
      "LogLoss  : 0.322461\n",
      "Threshold: 0.200\n",
      "============================================================\n",
      "\\nPer-Fold ROC-AUC:\n",
      "  Fold 1: 0.921743\n",
      "  Fold 2: 0.921532\n",
      "  Fold 3: 0.920112\n",
      "  Fold 4: 0.921009\n",
      "  Fold 5: 0.920448\n",
      "  Mean: 0.920969 ± 0.000620\n",
      "============================================================\n",
      "\\nPer-Fold ROC-AUC:\n",
      "  Fold 1: 0.921743\n",
      "  Fold 2: 0.921532\n",
      "  Fold 3: 0.920112\n",
      "  Fold 4: 0.921009\n",
      "  Fold 5: 0.920448\n",
      "  Mean: 0.920969 ± 0.000620\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6) Detailed Performance Analysis\n",
    "\n",
    "def evaluate_at_threshold(y_true, proba, thr: float) -> dict:\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, pred),\n",
    "        \"f1\": f1_score(y_true, pred),\n",
    "        \"logloss\": log_loss(y_true, proba),\n",
    "    }\n",
    "\n",
    "# Find optimal threshold\n",
    "thr_grid = np.linspace(0.1, 0.9, 17)\n",
    "\n",
    "log(\"\\\\nSearching for optimal F1 threshold on OOF predictions...\")\n",
    "best_thr = 0.5\n",
    "best_f1 = -1.0\n",
    "for thr in thr_grid:    \n",
    "    metrics = evaluate_at_threshold(y, oof_predictions, thr)\n",
    "    if metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"f1\"]\n",
    "        best_thr = thr\n",
    "\n",
    "log(f\"Best threshold: {best_thr:.3f} (F1={best_f1:.4f})\")\n",
    "\n",
    "metrics_opt = evaluate_at_threshold(y, oof_predictions, best_thr)\n",
    "\n",
    "print('\\\\n' + '='*60)\n",
    "print('=== Model Performance Summary ===')\n",
    "print('='*60)\n",
    "print(f\"ROC-AUC  : {oof_roc_auc:.6f} {'✓' if oof_roc_auc >= 0.95 else '⚠'}\")\n",
    "print(f\"Accuracy : {metrics_opt['accuracy']:.6f}\")\n",
    "print(f\"F1 Score : {metrics_opt['f1']:.6f}\")\n",
    "print(f\"LogLoss  : {metrics_opt['logloss']:.6f}\")\n",
    "print(f\"Threshold: {best_thr:.3f}\")\n",
    "print('='*60)\n",
    "\n",
    "# Per-fold AUC variance\n",
    "fold_aucs = []\n",
    "for fold, (_, val_idx) in enumerate(skf.split(X_raw, y), 1):\n",
    "    fold_auc = roc_auc_score(y[val_idx], oof_predictions[val_idx])\n",
    "    fold_aucs.append(fold_auc)\n",
    "\n",
    "print(f\"\\\\nPer-Fold ROC-AUC:\")\n",
    "for fold, auc in enumerate(fold_aucs, 1):\n",
    "    print(f\"  Fold {fold}: {auc:.6f}\")\n",
    "print(f\"  Mean: {np.mean(fold_aucs):.6f} ± {np.std(fold_aucs):.6f}\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d27276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:43] \\nGenerating submission file...\n",
      "[12:31:43] Test predictions shape: (254569,)\n",
      "[12:31:43] Test predictions range: [0.0004, 0.9995]\n",
      "[12:31:43] ✓ Saved submission to: loan_foldsafe_0.9211_20251121_123143.csv\n",
      "[12:31:43]   OOF ROC-AUC: 0.921090\n",
      "\\nSubmission preview:\n",
      "       id  loan_paid_back\n",
      "0  593994        0.809298\n",
      "1  593995        0.926775\n",
      "2  593996        0.230836\n",
      "3  593997        0.757955\n",
      "4  593998        0.905684\n",
      "5  593999        0.926507\n",
      "6  594000        0.969937\n",
      "7  594001        0.903081\n",
      "8  594002        0.833782\n",
      "9  594003        0.001224\n"
     ]
    }
   ],
   "source": [
    "# 7) Generate Submission File\n",
    "\n",
    "# Average test predictions across folds\n",
    "test_predictions = np.mean(test_predictions_folds, axis=0)\n",
    "\n",
    "log(f\"\\\\nGenerating submission file...\")\n",
    "log(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "log(f\"Test predictions range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if id_col is not None:\n",
    "    sub[id_col] = test_df[id_col]\n",
    "else:\n",
    "    sub[\"id\"] = np.arange(len(test_df))\n",
    "\n",
    "sub[target_col] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "sub_path = Path(f\"submissions/loan_foldsafe_{oof_roc_auc:.4f}_{ts}.csv\")\n",
    "sub_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "sub.to_csv(sub_path, index=False)\n",
    "\n",
    "log(f\"✓ Saved submission to: {sub_path.name}\")\n",
    "log(f\"  OOF ROC-AUC: {oof_roc_auc:.6f}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\\\nSubmission preview:\")\n",
    "print(sub.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0549bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:27:13] \\n============================================================\n",
      "[02:27:13] Feature Importance Analysis\n",
      "[02:27:13] ============================================================\n",
      "[02:27:13] To analyze feature importance:\n",
      "[02:27:13] 1. Install SHAP: pip install shap\n",
      "[02:27:13] 2. Retrain on full data or use last fold's booster\n",
      "[02:27:13] 3. Extract top features to guide next iteration\n",
      "[02:27:13] \n",
      "[02:27:13] Example SHAP analysis:\n",
      "[02:27:13]   import shap\n",
      "[02:27:13]   explainer = shap.TreeExplainer(booster)\n",
      "[02:27:13]   shap_values = explainer.shap_values(X_val_proc)\n",
      "[02:27:13]   shap.summary_plot(shap_values, X_val_proc)\n",
      "[02:27:13] ============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8) Feature Importance Analysis (Optional)\n",
    "\n",
    "# Note: Feature importance requires retraining or storing feature names\n",
    "# Since we use sparse matrices, feature names are not directly available\n",
    "# This cell provides a template for future SHAP or importance analysis\n",
    "\n",
    "log(\"\\\\n\" + \"=\"*60)\n",
    "log(\"Feature Importance Analysis\")\n",
    "log(\"=\"*60)\n",
    "log(\"To analyze feature importance:\")\n",
    "log(\"1. Install SHAP: pip install shap\")\n",
    "log(\"2. Retrain on full data or use last fold's booster\")\n",
    "log(\"3. Extract top features to guide next iteration\")\n",
    "log(\"\")\n",
    "log(\"Example SHAP analysis:\")\n",
    "log(\"  import shap\")\n",
    "log(\"  explainer = shap.TreeExplainer(booster)\")\n",
    "log(\"  shap_values = explainer.shap_values(X_val_proc)\")\n",
    "log(\"  shap.summary_plot(shap_values, X_val_proc)\")\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a578b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 02:27:13,567] A new study created in memory with name: no-name-ee85fb72-40b2-40a2-abf5-c89f51304362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:27:13] Starting Optuna hyperparameter search...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94ddcd83b2e48da9f3012edddd2992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 02:28:03,297] Trial 0 finished with value: 0.9181749461434654 and parameters: {'max_depth': 9, 'min_child_weight': 13, 'learning_rate': 0.01211310107833567, 'subsample': 0.8599203610605888, 'colsample_bytree': 0.72273131569352, 'reg_lambda': 1.9950155635685554, 'reg_alpha': 0.8547618948826736, 'gamma': 2.771929062153263, 'scale_pos_weight': 0.2647545549035971}. Best is trial 0 with value: 0.9181749461434654.\n",
      "[I 2025-11-21 02:28:52,563] Trial 1 finished with value: 0.9178427572076648 and parameters: {'max_depth': 9, 'min_child_weight': 17, 'learning_rate': 0.012364808668605618, 'subsample': 0.7856926129081762, 'colsample_bytree': 0.7503899027038833, 'reg_lambda': 3.6829590354594766, 'reg_alpha': 0.06679852871655512, 'gamma': 4.546250218040291, 'scale_pos_weight': 0.24229322137135695}. Best is trial 0 with value: 0.9181749461434654.\n",
      "[I 2025-11-21 02:28:52,563] Trial 1 finished with value: 0.9178427572076648 and parameters: {'max_depth': 9, 'min_child_weight': 17, 'learning_rate': 0.012364808668605618, 'subsample': 0.7856926129081762, 'colsample_bytree': 0.7503899027038833, 'reg_lambda': 3.6829590354594766, 'reg_alpha': 0.06679852871655512, 'gamma': 4.546250218040291, 'scale_pos_weight': 0.24229322137135695}. Best is trial 0 with value: 0.9181749461434654.\n",
      "[I 2025-11-21 02:29:41,043] Trial 2 finished with value: 0.9187316482991006 and parameters: {'max_depth': 9, 'min_child_weight': 12, 'learning_rate': 0.015863825473064145, 'subsample': 0.8878231221911579, 'colsample_bytree': 0.7242724858586839, 'reg_lambda': 2.3556100474042587, 'reg_alpha': 0.5632779251192448, 'gamma': 1.951388902948461, 'scale_pos_weight': 0.289232361672254}. Best is trial 2 with value: 0.9187316482991006.\n",
      "[I 2025-11-21 02:29:41,043] Trial 2 finished with value: 0.9187316482991006 and parameters: {'max_depth': 9, 'min_child_weight': 12, 'learning_rate': 0.015863825473064145, 'subsample': 0.8878231221911579, 'colsample_bytree': 0.7242724858586839, 'reg_lambda': 2.3556100474042587, 'reg_alpha': 0.5632779251192448, 'gamma': 1.951388902948461, 'scale_pos_weight': 0.289232361672254}. Best is trial 2 with value: 0.9187316482991006.\n",
      "[I 2025-11-21 02:30:22,831] Trial 3 finished with value: 0.9175702084967571 and parameters: {'max_depth': 5, 'min_child_weight': 13, 'learning_rate': 0.024138486566740835, 'subsample': 0.783050767404603, 'colsample_bytree': 0.9433172033844553, 'reg_lambda': 1.7882424395506173, 'reg_alpha': 1.0165149972808782, 'gamma': 0.7397536797045395, 'scale_pos_weight': 0.22126632680905153}. Best is trial 2 with value: 0.9187316482991006.\n",
      "[I 2025-11-21 02:30:22,831] Trial 3 finished with value: 0.9175702084967571 and parameters: {'max_depth': 5, 'min_child_weight': 13, 'learning_rate': 0.024138486566740835, 'subsample': 0.783050767404603, 'colsample_bytree': 0.9433172033844553, 'reg_lambda': 1.7882424395506173, 'reg_alpha': 1.0165149972808782, 'gamma': 0.7397536797045395, 'scale_pos_weight': 0.22126632680905153}. Best is trial 2 with value: 0.9187316482991006.\n",
      "[I 2025-11-21 02:31:05,070] Trial 4 finished with value: 0.9192195016711305 and parameters: {'max_depth': 10, 'min_child_weight': 5, 'learning_rate': 0.03523940856273252, 'subsample': 0.9083694292293633, 'colsample_bytree': 0.9344768346235995, 'reg_lambda': 4.876707484618034, 'reg_alpha': 1.665646083871861, 'gamma': 0.3290880293730081, 'scale_pos_weight': 0.24215540001424735}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:31:05,070] Trial 4 finished with value: 0.9192195016711305 and parameters: {'max_depth': 10, 'min_child_weight': 5, 'learning_rate': 0.03523940856273252, 'subsample': 0.9083694292293633, 'colsample_bytree': 0.9344768346235995, 'reg_lambda': 4.876707484618034, 'reg_alpha': 1.665646083871861, 'gamma': 0.3290880293730081, 'scale_pos_weight': 0.24215540001424735}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:31:44,073] Trial 5 finished with value: 0.9179264981557187 and parameters: {'max_depth': 7, 'min_child_weight': 13, 'learning_rate': 0.04825103824439917, 'subsample': 0.9047475522351325, 'colsample_bytree': 0.7468853110236919, 'reg_lambda': 3.6412165373696554, 'reg_alpha': 1.3652215337882705, 'gamma': 4.995490738817084, 'scale_pos_weight': 0.2500975133040518}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:31:44,073] Trial 5 finished with value: 0.9179264981557187 and parameters: {'max_depth': 7, 'min_child_weight': 13, 'learning_rate': 0.04825103824439917, 'subsample': 0.9047475522351325, 'colsample_bytree': 0.7468853110236919, 'reg_lambda': 3.6412165373696554, 'reg_alpha': 1.3652215337882705, 'gamma': 4.995490738817084, 'scale_pos_weight': 0.2500975133040518}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:32:32,488] Trial 6 finished with value: 0.9184419875033404 and parameters: {'max_depth': 9, 'min_child_weight': 17, 'learning_rate': 0.015230176771836695, 'subsample': 0.7402684390797293, 'colsample_bytree': 0.8746257881093169, 'reg_lambda': 4.547791282464115, 'reg_alpha': 1.8303112369779808, 'gamma': 1.562245075245912, 'scale_pos_weight': 0.2975953968305477}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:32:32,488] Trial 6 finished with value: 0.9184419875033404 and parameters: {'max_depth': 9, 'min_child_weight': 17, 'learning_rate': 0.015230176771836695, 'subsample': 0.7402684390797293, 'colsample_bytree': 0.8746257881093169, 'reg_lambda': 4.547791282464115, 'reg_alpha': 1.8303112369779808, 'gamma': 1.562245075245912, 'scale_pos_weight': 0.2975953968305477}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:33:14,347] Trial 7 finished with value: 0.9185629700261364 and parameters: {'max_depth': 9, 'min_child_weight': 5, 'learning_rate': 0.032146563712262814, 'subsample': 0.8454148071033252, 'colsample_bytree': 0.9386421740960534, 'reg_lambda': 3.840493756135629, 'reg_alpha': 0.39500677562817743, 'gamma': 4.745032541527773, 'scale_pos_weight': 0.28794835532579055}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:33:14,347] Trial 7 finished with value: 0.9185629700261364 and parameters: {'max_depth': 9, 'min_child_weight': 5, 'learning_rate': 0.032146563712262814, 'subsample': 0.8454148071033252, 'colsample_bytree': 0.9386421740960534, 'reg_lambda': 3.840493756135629, 'reg_alpha': 0.39500677562817743, 'gamma': 4.745032541527773, 'scale_pos_weight': 0.28794835532579055}. Best is trial 4 with value: 0.9192195016711305.\n",
      "[I 2025-11-21 02:33:55,383] Trial 8 finished with value: 0.919344936185974 and parameters: {'max_depth': 7, 'min_child_weight': 18, 'learning_rate': 0.04978069592737021, 'subsample': 0.8410326594513169, 'colsample_bytree': 0.8472503990501417, 'reg_lambda': 2.4081215716437447, 'reg_alpha': 1.790975728308718, 'gamma': 2.221863278128611, 'scale_pos_weight': 0.2596280455455923}. Best is trial 8 with value: 0.919344936185974.\n",
      "[I 2025-11-21 02:33:55,383] Trial 8 finished with value: 0.919344936185974 and parameters: {'max_depth': 7, 'min_child_weight': 18, 'learning_rate': 0.04978069592737021, 'subsample': 0.8410326594513169, 'colsample_bytree': 0.8472503990501417, 'reg_lambda': 2.4081215716437447, 'reg_alpha': 1.790975728308718, 'gamma': 2.221863278128611, 'scale_pos_weight': 0.2596280455455923}. Best is trial 8 with value: 0.919344936185974.\n",
      "[I 2025-11-21 02:34:43,031] Trial 9 finished with value: 0.9173159850899879 and parameters: {'max_depth': 8, 'min_child_weight': 9, 'learning_rate': 0.010376401166020779, 'subsample': 0.9094173432874049, 'colsample_bytree': 0.7798832299184988, 'reg_lambda': 2.1898486675462228, 'reg_alpha': 0.36344724696962927, 'gamma': 4.156759100724249, 'scale_pos_weight': 0.22010181568851903}. Best is trial 8 with value: 0.919344936185974.\n",
      "[I 2025-11-21 02:34:43,031] Trial 9 finished with value: 0.9173159850899879 and parameters: {'max_depth': 8, 'min_child_weight': 9, 'learning_rate': 0.010376401166020779, 'subsample': 0.9094173432874049, 'colsample_bytree': 0.7798832299184988, 'reg_lambda': 2.1898486675462228, 'reg_alpha': 0.36344724696962927, 'gamma': 4.156759100724249, 'scale_pos_weight': 0.22010181568851903}. Best is trial 8 with value: 0.919344936185974.\n",
      "[I 2025-11-21 02:35:23,022] Trial 10 finished with value: 0.9185260710140675 and parameters: {'max_depth': 6, 'min_child_weight': 20, 'learning_rate': 0.047379200239878315, 'subsample': 0.701306174702912, 'colsample_bytree': 0.8348747698075418, 'reg_lambda': 0.7744497087469873, 'reg_alpha': 1.977322083820875, 'gamma': 3.27434912813599, 'scale_pos_weight': 0.20164300373280614}. Best is trial 8 with value: 0.919344936185974.\n",
      "[I 2025-11-21 02:35:23,022] Trial 10 finished with value: 0.9185260710140675 and parameters: {'max_depth': 6, 'min_child_weight': 20, 'learning_rate': 0.047379200239878315, 'subsample': 0.701306174702912, 'colsample_bytree': 0.8348747698075418, 'reg_lambda': 0.7744497087469873, 'reg_alpha': 1.977322083820875, 'gamma': 3.27434912813599, 'scale_pos_weight': 0.20164300373280614}. Best is trial 8 with value: 0.919344936185974.\n",
      "[I 2025-11-21 02:36:08,211] Trial 11 finished with value: 0.9195162217375874 and parameters: {'max_depth': 7, 'min_child_weight': 5, 'learning_rate': 0.03645132011552828, 'subsample': 0.9277132497503787, 'colsample_bytree': 0.8832341007878202, 'reg_lambda': 4.9656387387478675, 'reg_alpha': 1.5368178371115961, 'gamma': 0.15826490035596705, 'scale_pos_weight': 0.2676162721779832}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:36:08,211] Trial 11 finished with value: 0.9195162217375874 and parameters: {'max_depth': 7, 'min_child_weight': 5, 'learning_rate': 0.03645132011552828, 'subsample': 0.9277132497503787, 'colsample_bytree': 0.8832341007878202, 'reg_lambda': 4.9656387387478675, 'reg_alpha': 1.5368178371115961, 'gamma': 0.15826490035596705, 'scale_pos_weight': 0.2676162721779832}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:36:53,144] Trial 12 finished with value: 0.9194306981021299 and parameters: {'max_depth': 7, 'min_child_weight': 9, 'learning_rate': 0.03523373411252567, 'subsample': 0.9463017885584087, 'colsample_bytree': 0.8786150378719545, 'reg_lambda': 3.080557305791213, 'reg_alpha': 1.4670458730537765, 'gamma': 0.02300725826319336, 'scale_pos_weight': 0.2693668893596159}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:36:53,144] Trial 12 finished with value: 0.9194306981021299 and parameters: {'max_depth': 7, 'min_child_weight': 9, 'learning_rate': 0.03523373411252567, 'subsample': 0.9463017885584087, 'colsample_bytree': 0.8786150378719545, 'reg_lambda': 3.080557305791213, 'reg_alpha': 1.4670458730537765, 'gamma': 0.02300725826319336, 'scale_pos_weight': 0.2693668893596159}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:37:35,870] Trial 13 finished with value: 0.9189733391040846 and parameters: {'max_depth': 6, 'min_child_weight': 8, 'learning_rate': 0.030431065243560967, 'subsample': 0.9475901770552286, 'colsample_bytree': 0.8886497166750194, 'reg_lambda': 3.062974354199141, 'reg_alpha': 1.3702860594202768, 'gamma': 0.950862448433763, 'scale_pos_weight': 0.27254071389844614}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:37:35,870] Trial 13 finished with value: 0.9189733391040846 and parameters: {'max_depth': 6, 'min_child_weight': 8, 'learning_rate': 0.030431065243560967, 'subsample': 0.9475901770552286, 'colsample_bytree': 0.8886497166750194, 'reg_lambda': 3.062974354199141, 'reg_alpha': 1.3702860594202768, 'gamma': 0.950862448433763, 'scale_pos_weight': 0.27254071389844614}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:38:19,085] Trial 14 finished with value: 0.9182076577748103 and parameters: {'max_depth': 6, 'min_child_weight': 8, 'learning_rate': 0.022969185917452783, 'subsample': 0.944892088746419, 'colsample_bytree': 0.8998736906407913, 'reg_lambda': 0.935942763061707, 'reg_alpha': 1.4275903628096225, 'gamma': 0.0018618834466719615, 'scale_pos_weight': 0.27963108389947144}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:38:19,085] Trial 14 finished with value: 0.9182076577748103 and parameters: {'max_depth': 6, 'min_child_weight': 8, 'learning_rate': 0.022969185917452783, 'subsample': 0.944892088746419, 'colsample_bytree': 0.8998736906407913, 'reg_lambda': 0.935942763061707, 'reg_alpha': 1.4275903628096225, 'gamma': 0.0018618834466719615, 'scale_pos_weight': 0.27963108389947144}. Best is trial 11 with value: 0.9195162217375874.\n",
      "[I 2025-11-21 02:39:03,501] Trial 15 finished with value: 0.9195469354158783 and parameters: {'max_depth': 7, 'min_child_weight': 10, 'learning_rate': 0.03877364642067423, 'subsample': 0.9395115940774422, 'colsample_bytree': 0.8626358778545256, 'reg_lambda': 4.306647074289115, 'reg_alpha': 1.0561413699959288, 'gamma': 1.1788827783756406, 'scale_pos_weight': 0.270581435198766}. Best is trial 15 with value: 0.9195469354158783.\n",
      "[I 2025-11-21 02:39:03,501] Trial 15 finished with value: 0.9195469354158783 and parameters: {'max_depth': 7, 'min_child_weight': 10, 'learning_rate': 0.03877364642067423, 'subsample': 0.9395115940774422, 'colsample_bytree': 0.8626358778545256, 'reg_lambda': 4.306647074289115, 'reg_alpha': 1.0561413699959288, 'gamma': 1.1788827783756406, 'scale_pos_weight': 0.270581435198766}. Best is trial 15 with value: 0.9195469354158783.\n",
      "[I 2025-11-21 02:39:49,212] Trial 16 finished with value: 0.9197620622879608 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.03905452578428291, 'subsample': 0.8795581023043217, 'colsample_bytree': 0.8063984841378058, 'reg_lambda': 4.4130646170539265, 'reg_alpha': 1.0002304593564757, 'gamma': 1.1769131334131697, 'scale_pos_weight': 0.3008454937916103}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:39:49,212] Trial 16 finished with value: 0.9197620622879608 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.03905452578428291, 'subsample': 0.8795581023043217, 'colsample_bytree': 0.8063984841378058, 'reg_lambda': 4.4130646170539265, 'reg_alpha': 1.0002304593564757, 'gamma': 1.1769131334131697, 'scale_pos_weight': 0.3008454937916103}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:40:34,391] Trial 17 finished with value: 0.919603823093962 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.040556900761097026, 'subsample': 0.872179917918442, 'colsample_bytree': 0.7976105922974914, 'reg_lambda': 4.243259376870989, 'reg_alpha': 1.0591783168153106, 'gamma': 1.3342670254391698, 'scale_pos_weight': 0.3002465369188112}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:40:34,391] Trial 17 finished with value: 0.919603823093962 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.040556900761097026, 'subsample': 0.872179917918442, 'colsample_bytree': 0.7976105922974914, 'reg_lambda': 4.243259376870989, 'reg_alpha': 1.0591783168153106, 'gamma': 1.3342670254391698, 'scale_pos_weight': 0.3002465369188112}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:41:20,522] Trial 18 finished with value: 0.9194158541722833 and parameters: {'max_depth': 8, 'min_child_weight': 15, 'learning_rate': 0.02748541319106482, 'subsample': 0.8748388118561411, 'colsample_bytree': 0.8012844453932618, 'reg_lambda': 4.2797646574515475, 'reg_alpha': 0.8512747440215537, 'gamma': 1.5505310049012349, 'scale_pos_weight': 0.2988805180566573}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:41:20,522] Trial 18 finished with value: 0.9194158541722833 and parameters: {'max_depth': 8, 'min_child_weight': 15, 'learning_rate': 0.02748541319106482, 'subsample': 0.8748388118561411, 'colsample_bytree': 0.8012844453932618, 'reg_lambda': 4.2797646574515475, 'reg_alpha': 0.8512747440215537, 'gamma': 1.5505310049012349, 'scale_pos_weight': 0.2988805180566573}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:42:08,817] Trial 19 finished with value: 0.9190778400661577 and parameters: {'max_depth': 10, 'min_child_weight': 11, 'learning_rate': 0.020169186659060954, 'subsample': 0.8101105551559991, 'colsample_bytree': 0.8021217951178385, 'reg_lambda': 3.428234448880324, 'reg_alpha': 1.175773525691054, 'gamma': 2.7983163692734205, 'scale_pos_weight': 0.2838982144809009}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:42:08,817] Trial 19 finished with value: 0.9190778400661577 and parameters: {'max_depth': 10, 'min_child_weight': 11, 'learning_rate': 0.020169186659060954, 'subsample': 0.8101105551559991, 'colsample_bytree': 0.8021217951178385, 'reg_lambda': 3.428234448880324, 'reg_alpha': 1.175773525691054, 'gamma': 2.7983163692734205, 'scale_pos_weight': 0.2838982144809009}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:42:49,587] Trial 20 finished with value: 0.9187589001967531 and parameters: {'max_depth': 8, 'min_child_weight': 15, 'learning_rate': 0.041555632933502104, 'subsample': 0.8190279785441175, 'colsample_bytree': 0.8045085297686686, 'reg_lambda': 4.159719995663378, 'reg_alpha': 0.7216808935394788, 'gamma': 3.5308067960524614, 'scale_pos_weight': 0.30110106749709664}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:42:49,587] Trial 20 finished with value: 0.9187589001967531 and parameters: {'max_depth': 8, 'min_child_weight': 15, 'learning_rate': 0.041555632933502104, 'subsample': 0.8190279785441175, 'colsample_bytree': 0.8045085297686686, 'reg_lambda': 4.159719995663378, 'reg_alpha': 0.7216808935394788, 'gamma': 3.5308067960524614, 'scale_pos_weight': 0.30110106749709664}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:43:35,316] Trial 21 finished with value: 0.919684308832454 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.0406830607297748, 'subsample': 0.8857177810684597, 'colsample_bytree': 0.8496399621201511, 'reg_lambda': 4.455468056627328, 'reg_alpha': 1.1420901847176101, 'gamma': 1.2368818418637626, 'scale_pos_weight': 0.2909110142360498}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:43:35,316] Trial 21 finished with value: 0.919684308832454 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.0406830607297748, 'subsample': 0.8857177810684597, 'colsample_bytree': 0.8496399621201511, 'reg_lambda': 4.455468056627328, 'reg_alpha': 1.1420901847176101, 'gamma': 1.2368818418637626, 'scale_pos_weight': 0.2909110142360498}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:44:21,943] Trial 22 finished with value: 0.9196352545419315 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.042506228420127856, 'subsample': 0.875730250461652, 'colsample_bytree': 0.7756266644800094, 'reg_lambda': 4.652257178601861, 'reg_alpha': 1.1972297186926908, 'gamma': 1.505884550764356, 'scale_pos_weight': 0.2930703408757527}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:44:21,943] Trial 22 finished with value: 0.9196352545419315 and parameters: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.042506228420127856, 'subsample': 0.875730250461652, 'colsample_bytree': 0.7756266644800094, 'reg_lambda': 4.652257178601861, 'reg_alpha': 1.1972297186926908, 'gamma': 1.505884550764356, 'scale_pos_weight': 0.2930703408757527}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:45:09,453] Trial 23 finished with value: 0.9193190746793858 and parameters: {'max_depth': 8, 'min_child_weight': 7, 'learning_rate': 0.02733645750838732, 'subsample': 0.8863518748516255, 'colsample_bytree': 0.7675279026885101, 'reg_lambda': 4.681534120644077, 'reg_alpha': 1.2166472584999846, 'gamma': 0.6591574444164141, 'scale_pos_weight': 0.2903953734969469}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:45:09,453] Trial 23 finished with value: 0.9193190746793858 and parameters: {'max_depth': 8, 'min_child_weight': 7, 'learning_rate': 0.02733645750838732, 'subsample': 0.8863518748516255, 'colsample_bytree': 0.7675279026885101, 'reg_lambda': 4.681534120644077, 'reg_alpha': 1.2166472584999846, 'gamma': 0.6591574444164141, 'scale_pos_weight': 0.2903953734969469}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:45:53,749] Trial 24 finished with value: 0.9195414520498761 and parameters: {'max_depth': 8, 'min_child_weight': 15, 'learning_rate': 0.04309558223395311, 'subsample': 0.8486822920560774, 'colsample_bytree': 0.8240679688168948, 'reg_lambda': 3.8734685050361852, 'reg_alpha': 1.2017514708059056, 'gamma': 1.9558329897772269, 'scale_pos_weight': 0.276835779242773}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:45:53,749] Trial 24 finished with value: 0.9195414520498761 and parameters: {'max_depth': 8, 'min_child_weight': 15, 'learning_rate': 0.04309558223395311, 'subsample': 0.8486822920560774, 'colsample_bytree': 0.8240679688168948, 'reg_lambda': 3.8734685050361852, 'reg_alpha': 1.2017514708059056, 'gamma': 1.9558329897772269, 'scale_pos_weight': 0.276835779242773}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:46:44,180] Trial 25 finished with value: 0.9190587868163387 and parameters: {'max_depth': 9, 'min_child_weight': 11, 'learning_rate': 0.01961322729848077, 'subsample': 0.8944216918109874, 'colsample_bytree': 0.7006823438824332, 'reg_lambda': 4.560513459496779, 'reg_alpha': 0.7859873849594599, 'gamma': 1.7720343752099241, 'scale_pos_weight': 0.29093044431319165}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:46:44,180] Trial 25 finished with value: 0.9190587868163387 and parameters: {'max_depth': 9, 'min_child_weight': 11, 'learning_rate': 0.01961322729848077, 'subsample': 0.8944216918109874, 'colsample_bytree': 0.7006823438824332, 'reg_lambda': 4.560513459496779, 'reg_alpha': 0.7859873849594599, 'gamma': 1.7720343752099241, 'scale_pos_weight': 0.29093044431319165}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:47:31,288] Trial 26 finished with value: 0.9194525955180479 and parameters: {'max_depth': 8, 'min_child_weight': 14, 'learning_rate': 0.02914557157699211, 'subsample': 0.9191679043651999, 'colsample_bytree': 0.8257253369768109, 'reg_lambda': 3.1737904334711113, 'reg_alpha': 0.623411976072216, 'gamma': 1.045102200468409, 'scale_pos_weight': 0.27966533818488243}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:47:31,288] Trial 26 finished with value: 0.9194525955180479 and parameters: {'max_depth': 8, 'min_child_weight': 14, 'learning_rate': 0.02914557157699211, 'subsample': 0.9191679043651999, 'colsample_bytree': 0.8257253369768109, 'reg_lambda': 3.1737904334711113, 'reg_alpha': 0.623411976072216, 'gamma': 1.045102200468409, 'scale_pos_weight': 0.27966533818488243}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:48:11,151] Trial 27 finished with value: 0.9190637633574796 and parameters: {'max_depth': 7, 'min_child_weight': 10, 'learning_rate': 0.04357173008959704, 'subsample': 0.8710750099995812, 'colsample_bytree': 0.7774661480295352, 'reg_lambda': 4.101028517790222, 'reg_alpha': 1.2798634983306447, 'gamma': 2.3775972399685257, 'scale_pos_weight': 0.2935540621071473}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:48:11,151] Trial 27 finished with value: 0.9190637633574796 and parameters: {'max_depth': 7, 'min_child_weight': 10, 'learning_rate': 0.04357173008959704, 'subsample': 0.8710750099995812, 'colsample_bytree': 0.7774661480295352, 'reg_lambda': 4.101028517790222, 'reg_alpha': 1.2798634983306447, 'gamma': 2.3775972399685257, 'scale_pos_weight': 0.2935540621071473}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:48:59,634] Trial 28 finished with value: 0.9193684370541 and parameters: {'max_depth': 10, 'min_child_weight': 7, 'learning_rate': 0.033562163894926854, 'subsample': 0.8328354941646374, 'colsample_bytree': 0.9085405939626363, 'reg_lambda': 4.998116677994058, 'reg_alpha': 1.5875684479421792, 'gamma': 0.46052598408145906, 'scale_pos_weight': 0.2846820058970656}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:48:59,634] Trial 28 finished with value: 0.9193684370541 and parameters: {'max_depth': 10, 'min_child_weight': 7, 'learning_rate': 0.033562163894926854, 'subsample': 0.8328354941646374, 'colsample_bytree': 0.9085405939626363, 'reg_lambda': 4.998116677994058, 'reg_alpha': 1.5875684479421792, 'gamma': 0.46052598408145906, 'scale_pos_weight': 0.2846820058970656}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[I 2025-11-21 02:49:41,597] Trial 29 finished with value: 0.9189088962471592 and parameters: {'max_depth': 8, 'min_child_weight': 12, 'learning_rate': 0.04323047654031979, 'subsample': 0.8633547397063078, 'colsample_bytree': 0.8394531311196035, 'reg_lambda': 1.5745050579125652, 'reg_alpha': 0.9308760008530612, 'gamma': 2.915963582147123, 'scale_pos_weight': 0.2606448867073336}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[02:49:41] \\nBest ROC-AUC: 0.919762\n",
      "[02:49:41] Best params: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.03905452578428291, 'subsample': 0.8795581023043217, 'colsample_bytree': 0.8063984841378058, 'reg_lambda': 4.4130646170539265, 'reg_alpha': 1.0002304593564757, 'gamma': 1.1769131334131697, 'scale_pos_weight': 0.3008454937916103}\n",
      "[02:49:41] \\nTo enable Optuna search:\n",
      "[02:49:41] 1. Uncomment the code in this cell\n",
      "[02:49:41] 2. Install: pip install optuna\n",
      "[02:49:41] 3. Run cell to find optimal hyperparameters\n",
      "[02:49:41] 4. Retrain model with optimized params\n",
      "[I 2025-11-21 02:49:41,597] Trial 29 finished with value: 0.9189088962471592 and parameters: {'max_depth': 8, 'min_child_weight': 12, 'learning_rate': 0.04323047654031979, 'subsample': 0.8633547397063078, 'colsample_bytree': 0.8394531311196035, 'reg_lambda': 1.5745050579125652, 'reg_alpha': 0.9308760008530612, 'gamma': 2.915963582147123, 'scale_pos_weight': 0.2606448867073336}. Best is trial 16 with value: 0.9197620622879608.\n",
      "[02:49:41] \\nBest ROC-AUC: 0.919762\n",
      "[02:49:41] Best params: {'max_depth': 8, 'min_child_weight': 11, 'learning_rate': 0.03905452578428291, 'subsample': 0.8795581023043217, 'colsample_bytree': 0.8063984841378058, 'reg_lambda': 4.4130646170539265, 'reg_alpha': 1.0002304593564757, 'gamma': 1.1769131334131697, 'scale_pos_weight': 0.3008454937916103}\n",
      "[02:49:41] \\nTo enable Optuna search:\n",
      "[02:49:41] 1. Uncomment the code in this cell\n",
      "[02:49:41] 2. Install: pip install optuna\n",
      "[02:49:41] 3. Run cell to find optimal hyperparameters\n",
      "[02:49:41] 4. Retrain model with optimized params\n"
     ]
    }
   ],
   "source": [
    "# 9) Hyperparameter Optimization with Optuna (Optional)\n",
    "\n",
    "# Uncomment and run this cell to find optimal hyperparameters\n",
    "# Requires: pip install optuna\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params_opt = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.95),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.95),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 5.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 2.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", \n",
    "                                                 scale_pos_weight * 0.8, \n",
    "                                                 scale_pos_weight * 1.2),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    "    \n",
    "    # Quick CV (2 folds for speed, reduced rounds)\n",
    "    skf_opt = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_opt.split(X_raw, y):\n",
    "        X_train_fold = X_raw.iloc[train_idx].copy()\n",
    "        X_val_fold = X_raw.iloc[val_idx].copy()\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        X_train_proc, X_val_proc, _ = preprocess_fold(\n",
    "            X_train_fold, X_val_fold, X_test_raw,\n",
    "            y_train_fold, numeric_cols, categorical_cols\n",
    "        )\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train_proc, label=y_train_fold)\n",
    "        dval = xgb.DMatrix(X_val_proc, label=y_val_fold)\n",
    "        \n",
    "        booster = xgb.train(\n",
    "            params_opt,\n",
    "            dtrain,\n",
    "            num_boost_round=500,  # Reduced from 3000\n",
    "            evals=[(dval, \"valid\")],\n",
    "            early_stopping_rounds=30,  # Reduced from 100\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        \n",
    "        preds = booster.predict(dval, iteration_range=(0, booster.best_iteration + 1))\n",
    "        score = roc_auc_score(y_val_fold, preds)\n",
    "        oof_scores.append(score)\n",
    "        \n",
    "        # Early trial pruning - stop fold loop if first fold is terrible\n",
    "        if len(oof_scores) == 1 and score < 0.85:\n",
    "            trial.report(score, 0)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(oof_scores)\n",
    "\n",
    "# Run optimization\n",
    "log(\"Starting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    ")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True, n_jobs=1)  # Reduced to 30 trials\n",
    "\n",
    "log(f\"\\\\nBest ROC-AUC: {study.best_value:.6f}\")\n",
    "log(f\"Best params: {study.best_params}\")\n",
    "\n",
    "# Update params with best values\n",
    "params.update(study.best_params)\n",
    "\n",
    "log(\"\\\\nTo enable Optuna search:\")\n",
    "log(\"1. Uncomment the code in this cell\")\n",
    "log(\"2. Install: pip install optuna\")\n",
    "log(\"3. Run cell to find optimal hyperparameters\")\n",
    "log(\"4. Retrain model with optimized params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4799a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:49:41] Starting fast manual hyperparameter grid search...\n",
      "[02:49:41] Testing 54 parameter combinations...\n",
      "[02:50:16] Trial 1/54: ROC-AUC=0.91740 ✓ NEW BEST\n",
      "[02:50:16] Trial 1/54: ROC-AUC=0.91740 ✓ NEW BEST\n",
      "[02:50:52] Trial 2/54: ROC-AUC=0.91743 ✓ NEW BEST\n",
      "[02:50:52] Trial 2/54: ROC-AUC=0.91743 ✓ NEW BEST\n",
      "[02:52:01] Trial 4/54: ROC-AUC=0.91855 ✓ NEW BEST\n",
      "[02:52:01] Trial 4/54: ROC-AUC=0.91855 ✓ NEW BEST\n",
      "[02:52:37] Trial 5/54: ROC-AUC=0.91846\n",
      "[02:52:37] Trial 5/54: ROC-AUC=0.91846\n",
      "[02:55:32] Trial 10/54: ROC-AUC=0.91852\n",
      "[02:55:32] Trial 10/54: ROC-AUC=0.91852\n",
      "[02:58:22] Trial 15/54: ROC-AUC=0.91739\n",
      "[02:58:22] Trial 15/54: ROC-AUC=0.91739\n",
      "[02:59:32] Trial 17/54: ROC-AUC=0.91857 ✓ NEW BEST\n",
      "[02:59:32] Trial 17/54: ROC-AUC=0.91857 ✓ NEW BEST\n",
      "[03:01:18] Trial 20/54: ROC-AUC=0.91794\n",
      "[03:01:18] Trial 20/54: ROC-AUC=0.91794\n",
      "[03:02:31] Trial 22/54: ROC-AUC=0.91892 ✓ NEW BEST\n",
      "[03:02:31] Trial 22/54: ROC-AUC=0.91892 ✓ NEW BEST\n",
      "[03:20:36] Trial 25/54: ROC-AUC=0.91799\n",
      "[03:20:36] Trial 25/54: ROC-AUC=0.91799\n",
      "[03:37:24] Trial 28/54: ROC-AUC=0.91894 ✓ NEW BEST\n",
      "[03:37:24] Trial 28/54: ROC-AUC=0.91894 ✓ NEW BEST\n",
      "[04:09:05] Trial 30/54: ROC-AUC=0.91889\n",
      "[04:09:05] Trial 30/54: ROC-AUC=0.91889\n",
      "[04:46:59] Trial 35/54: ROC-AUC=0.91899 ✓ NEW BEST\n",
      "[04:46:59] Trial 35/54: ROC-AUC=0.91899 ✓ NEW BEST\n",
      "[05:44:16] Trial 40/54: ROC-AUC=0.91910 ✓ NEW BEST\n",
      "[05:44:16] Trial 40/54: ROC-AUC=0.91910 ✓ NEW BEST\n",
      "[05:44:54] Trial 41/54: ROC-AUC=0.91916 ✓ NEW BEST\n",
      "[05:44:54] Trial 41/54: ROC-AUC=0.91916 ✓ NEW BEST\n",
      "[06:05:55] Trial 45/54: ROC-AUC=0.91840\n",
      "[06:05:55] Trial 45/54: ROC-AUC=0.91840\n",
      "[06:21:51] Trial 46/54: ROC-AUC=0.91927 ✓ NEW BEST\n",
      "[06:21:51] Trial 46/54: ROC-AUC=0.91927 ✓ NEW BEST\n",
      "[06:37:45] Trial 47/54: ROC-AUC=0.91929 ✓ NEW BEST\n",
      "[06:37:45] Trial 47/54: ROC-AUC=0.91929 ✓ NEW BEST\n",
      "[07:00:26] Trial 50/54: ROC-AUC=0.91844\n",
      "[07:00:26] Trial 50/54: ROC-AUC=0.91844\n",
      "[07:01:41] Trial 52/54: ROC-AUC=0.91930 ✓ NEW BEST\n",
      "[07:01:41] Trial 52/54: ROC-AUC=0.91930 ✓ NEW BEST\n",
      "[07:02:56] \n",
      "🎯 Best ROC-AUC: 0.919301\n",
      "[07:02:56] Best params: {'max_depth': 8, 'min_child_weight': 20, 'learning_rate': 0.03, 'reg_lambda': 1.0}\n",
      "[07:02:56] \n",
      "✓ Updated params with best values\n",
      "[07:02:56] \n",
      "🎯 Best ROC-AUC: 0.919301\n",
      "[07:02:56] Best params: {'max_depth': 8, 'min_child_weight': 20, 'learning_rate': 0.03, 'reg_lambda': 1.0}\n",
      "[07:02:56] \n",
      "✓ Updated params with best values\n"
     ]
    }
   ],
   "source": [
    "# FAST ALTERNATIVE: Quick manual hyperparameter search (use this instead of Optuna)\n",
    "# This tests a small grid of promising param combinations\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "log(\"Starting fast manual hyperparameter grid search...\")\n",
    "\n",
    "# Define search grid (only most impactful params)\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'min_child_weight': [10, 15, 20],\n",
    "    'learning_rate': [0.02, 0.03],\n",
    "    'reg_lambda': [1.0, 1.5, 2.0],\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params_found = {}\n",
    "\n",
    "# Quick 2-fold CV for speed\n",
    "skf_quick = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "grid_combinations = list(product(*param_grid.values()))\n",
    "log(f\"Testing {len(grid_combinations)} parameter combinations...\")\n",
    "\n",
    "for i, combo in enumerate(grid_combinations):\n",
    "    test_params = dict(zip(param_grid.keys(), combo))\n",
    "    \n",
    "    # Build full params\n",
    "    params_test = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"subsample\": 0.85,\n",
    "        \"colsample_bytree\": 0.85,\n",
    "        \"reg_alpha\": 0.1,\n",
    "        \"gamma\": 0.5,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    "    params_test.update(test_params)\n",
    "    \n",
    "    fold_scores = []\n",
    "    for train_idx, val_idx in skf_quick.split(X_raw, y):\n",
    "        X_train_fold = X_raw.iloc[train_idx].copy()\n",
    "        X_val_fold = X_raw.iloc[val_idx].copy()\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        X_train_proc, X_val_proc, _ = preprocess_fold(\n",
    "            X_train_fold, X_val_fold, X_test_raw,\n",
    "            y_train_fold, numeric_cols, categorical_cols\n",
    "        )\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train_proc, label=y_train_fold)\n",
    "        dval = xgb.DMatrix(X_val_proc, label=y_val_fold)\n",
    "        \n",
    "        booster = xgb.train(\n",
    "            params_test,\n",
    "            dtrain,\n",
    "            num_boost_round=400,\n",
    "            evals=[(dval, \"valid\")],\n",
    "            early_stopping_rounds=30,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        \n",
    "        preds = booster.predict(dval, iteration_range=(0, booster.best_iteration + 1))\n",
    "        score = roc_auc_score(y_val_fold, preds)\n",
    "        fold_scores.append(score)\n",
    "    \n",
    "    avg_score = np.mean(fold_scores)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_found = test_params.copy()\n",
    "        log(f\"Trial {i+1}/{len(grid_combinations)}: ROC-AUC={avg_score:.5f} ✓ NEW BEST\")\n",
    "    elif (i+1) % 5 == 0:\n",
    "        log(f\"Trial {i+1}/{len(grid_combinations)}: ROC-AUC={avg_score:.5f}\")\n",
    "\n",
    "log(f\"\\n🎯 Best ROC-AUC: {best_score:.6f}\")\n",
    "log(f\"Best params: {best_params_found}\")\n",
    "\n",
    "# Update global params\n",
    "params.update(best_params_found)\n",
    "log(\"\\n✓ Updated params with best values\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
