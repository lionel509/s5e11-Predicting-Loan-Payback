{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57838e7",
   "metadata": {},
   "source": [
    "# Loan Payback Prediction - Training & Evaluation\n",
    "\n",
    "This notebook loads pre-created training and test splits, with optional SMOTE balancing, preprocesses features, trains a RandomForest model, evaluates on the test split, and auto-increments submissions in a dedicated folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c090a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and train_test_split from sklearn.model_selection for splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "674b866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83675948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: USE_SMOTE = False\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Set to True to use SMOTE-balanced training data\n",
    "USE_SMOTE = False  # Change to True to use train_split_smote.csv\n",
    "\n",
    "print(f\"Configuration: USE_SMOTE = {USE_SMOTE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86d45c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative training: DISABLED\n",
      "\n",
      "Models to train:\n",
      "  - RandomForest: YES\n",
      "  - Neural Network: YES\n",
      "    Epochs: 50, Batch size: 32, Validation split: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Optional: Enable iterative training to search for best hyperparameters\n",
    "ENABLE_ITERATIVE_TRAINING = False  # Set to True to try multiple configurations\n",
    "N_ITERATIONS = 5  # Number of different configurations to try\n",
    "\n",
    "# Model selection: Train both models and compare\n",
    "TRAIN_RANDOM_FOREST = True   # Train RandomForest model\n",
    "TRAIN_NEURAL_NETWORK = True  # Train Neural Network model\n",
    "\n",
    "# Neural Network configuration\n",
    "NN_EPOCHS = 50\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_VALIDATION_SPLIT = 0.1\n",
    "\n",
    "print(f\"Iterative training: {'ENABLED' if ENABLE_ITERATIVE_TRAINING else 'DISABLED'}\")\n",
    "if ENABLE_ITERATIVE_TRAINING:\n",
    "    print(f\"Will train {N_ITERATIONS} models with different configurations\")\n",
    "print(f\"\\nModels to train:\")\n",
    "print(f\"  - RandomForest: {'YES' if TRAIN_RANDOM_FOREST else 'NO'}\")\n",
    "print(f\"  - Neural Network: {'YES' if TRAIN_NEURAL_NETWORK else 'NO'}\")\n",
    "if TRAIN_NEURAL_NETWORK:\n",
    "    print(f\"    Epochs: {NN_EPOCHS}, Batch size: {NN_BATCH_SIZE}, Validation split: {NN_VALIDATION_SPLIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6f5b9",
   "metadata": {},
   "source": [
    "## 2b. Training Configuration (Optional)\n",
    "\n",
    "Enable iterative training to find the best hyperparameters by training multiple times with validation monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bba476",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set whether to use SMOTE-balanced training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9bb72",
   "metadata": {},
   "source": [
    "## 3. Load Training and Test Split Data\n",
    "\n",
    "Load the train_split.csv (or train_split_smote.csv) and test_split.csv files from the Data/splits directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f853be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file: Data/splits/train_split.csv\n",
      "Train split shape: (475195, 13)\n",
      "Test split shape: (118799, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test split datasets\n",
    "train_file = 'Data/splits/train_split_smote.csv' if USE_SMOTE else 'Data/splits/train_split.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv('Data/splits/test_split.csv')\n",
    "\n",
    "print(f\"Training file: {train_file}\")\n",
    "print(f\"Train split shape: {train_df.shape}\")\n",
    "print(f\"Test split shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4206f781",
   "metadata": {},
   "source": [
    "## 4. Prepare Features and Target Variables\n",
    "\n",
    "Separate features (X) and target variable (y) for both splits by dropping the 'loan_paid_back' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8126624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (475195, 12)\n",
      "y_train shape: (475195,)\n",
      "X_test shape:  (118799, 12)\n",
      "y_test shape:  (118799,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variables for both splits\n",
    "X_train = train_df.drop(\"loan_paid_back\", axis=1)\n",
    "y_train = train_df[\"loan_paid_back\"]\n",
    "\n",
    "X_test = test_df.drop(\"loan_paid_back\", axis=1)\n",
    "y_test = test_df[\"loan_paid_back\"]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49941a4d",
   "metadata": {},
   "source": [
    "## 5. Check Class Distribution in Provided Splits\n",
    "\n",
    "Confirm the class balance in y_train and y_test to ensure splits look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b99b15e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train (proportion):\n",
      "loan_paid_back\n",
      "0.0    0.201181\n",
      "1.0    0.798819\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in y_test (proportion):\n",
      "loan_paid_back\n",
      "0.0    0.20118\n",
      "1.0    0.79882\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in the provided splits\n",
    "print(\"Class distribution in y_train (proportion):\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nClass distribution in y_test (proportion):\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a543136",
   "metadata": {},
   "source": [
    "## 6. Sanity-Check Feature Columns\n",
    "\n",
    "Verify that train and test splits have matching feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8d4bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: (12)\n",
      "['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "\n",
      "X_test columns:  (12)\n",
      "['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "\n",
      "Columns match: True\n"
     ]
    }
   ],
   "source": [
    "# Ensure feature columns align between train and test splits\n",
    "train_cols = list(X_train.columns)\n",
    "test_cols = list(X_test.columns)\n",
    "print(f\"X_train columns: ({len(train_cols)})\")\n",
    "print(train_cols)\n",
    "print(f\"\\nX_test columns:  ({len(test_cols)})\")\n",
    "print(test_cols)\n",
    "print(\"\\nColumns match:\", train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1157223",
   "metadata": {},
   "source": [
    "## 7. Identify Categorical and Numerical Columns\n",
    "\n",
    "Separate columns into categorical and numerical types for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2b5e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns (6): ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "Numerical columns (5): ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns (object dtype)\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Identify numerical columns (excluding 'id' if present, as it's not a useful feature)\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'id' in numerical_cols:\n",
    "    numerical_cols.remove('id')\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365256c1",
   "metadata": {},
   "source": [
    "## 8. Import Preprocessing Tools\n",
    "\n",
    "Import OneHotEncoder for categorical features and StandardScaler for numerical features from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b01d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa20e6",
   "metadata": {},
   "source": [
    "## 9. One-Hot Encode Categorical Features\n",
    "\n",
    "Fit the OneHotEncoder on the training data and transform train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fb57c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded categorical features shape (train): (475195, 55)\n",
      "Encoded categorical features shape (test): (118799, 55)\n",
      "Total one-hot encoded features: 55\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit OneHotEncoder on training data\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "# Transform categorical columns for both datasets\n",
    "X_train_cat_encoded = ohe.transform(X_train[categorical_cols])\n",
    "X_test_cat_encoded = ohe.transform(X_test[categorical_cols])\n",
    "\n",
    "print(f\"Encoded categorical features shape (train): {X_train_cat_encoded.shape}\")\n",
    "print(f\"Encoded categorical features shape (test): {X_test_cat_encoded.shape}\")\n",
    "print(f\"Total one-hot encoded features: {X_train_cat_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa470ee",
   "metadata": {},
   "source": [
    "## 10. Scale Numerical Features\n",
    "\n",
    "Fit the StandardScaler on the training data and transform train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "501a0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled numerical features shape (train): (475195, 5)\n",
      "Scaled numerical features shape (test): (118799, 5)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit StandardScaler on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_cols])\n",
    "\n",
    "# Transform numerical columns for both datasets\n",
    "X_train_num_scaled = scaler.transform(X_train[numerical_cols])\n",
    "X_test_num_scaled = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(f\"Scaled numerical features shape (train): {X_train_num_scaled.shape}\")\n",
    "print(f\"Scaled numerical features shape (test): {X_test_num_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2de62c",
   "metadata": {},
   "source": [
    "## 11. Combine Encoded and Scaled Features\n",
    "\n",
    "Concatenate the one-hot encoded categorical features with scaled numerical features into final NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebc32bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final preprocessed data shapes:\n",
      "  X_train_processed: (475195, 60)\n",
      "  X_test_processed:  (118799, 60)\n",
      "  y_train_array:     (475195,)\n",
      "  y_test_array:      (118799,)\n",
      "\n",
      "Total features: 60 (5 numerical + 55 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Combine categorical and numerical features\n",
    "X_train_processed = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n",
    "X_test_processed = np.concatenate([X_test_num_scaled, X_test_cat_encoded], axis=1)\n",
    "\n",
    "# Convert target variables to NumPy arrays\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n",
    "\n",
    "print(\"Final preprocessed data shapes:\")\n",
    "print(f\"  X_train_processed: {X_train_processed.shape}\")\n",
    "print(f\"  X_test_processed:  {X_test_processed.shape}\")\n",
    "print(f\"  y_train_array:     {y_train_array.shape}\")\n",
    "print(f\"  y_test_array:      {y_test_array.shape}\")\n",
    "print(f\"\\nTotal features: {X_train_processed.shape[1]} ({len(numerical_cols)} numerical + {X_train_cat_encoded.shape[1]} categorical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cd70c",
   "metadata": {},
   "source": [
    "## 12. Save Preprocessed Data\n",
    "\n",
    "Save the preprocessed NumPy arrays to disk for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "074701c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to Data/preprocessed/:\n",
      "  - X_train.npy\n",
      "  - X_test.npy\n",
      "  - y_train.npy\n",
      "  - y_test.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for preprocessed data\n",
    "os.makedirs('Data/preprocessed', exist_ok=True)\n",
    "\n",
    "# Save preprocessed arrays\n",
    "np.save('Data/preprocessed/X_train.npy', X_train_processed)\n",
    "np.save('Data/preprocessed/X_test.npy', X_test_processed)\n",
    "np.save('Data/preprocessed/y_train.npy', y_train_array)\n",
    "np.save('Data/preprocessed/y_test.npy', y_test_array)\n",
    "\n",
    "print(\"Preprocessed data saved to Data/preprocessed/:\")\n",
    "print(\"  - X_train.npy\")\n",
    "print(\"  - X_test.npy\")\n",
    "print(\"  - y_train.npy\")\n",
    "print(\"  - y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bf160",
   "metadata": {},
   "source": [
    "## 13. Train Models (RandomForest and/or Neural Network)\n",
    "\n",
    "Train selected models with balanced class weights to handle class imbalance. Compare performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7af801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Negative=95600, Positive=379595\n",
      "Class weight ratio (neg/pos) = 0.2518\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS\n",
      "======================================================================\n",
      "\n",
      "ðŸŒ² RANDOM FOREST\n",
      "----------------------------------------------------------------------\n",
      "Training RandomForest model...\n",
      "âœ“ Training complete!\n",
      "RandomForest ROC-AUC: 0.9121\n",
      "\n",
      "ðŸ§  NEURAL NETWORK\n",
      "----------------------------------------------------------------------\n",
      "âš ï¸  TensorFlow/Keras not installed. Skipping Neural Network training.\n",
      "Install with: pip install tensorflow\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "RandomForest        : ROC-AUC = 0.9121\n",
      "\n",
      "ðŸ† BEST MODEL: RandomForest (ROC-AUC = 0.9121)\n",
      "======================================================================\n",
      "âœ“ Training complete!\n",
      "RandomForest ROC-AUC: 0.9121\n",
      "\n",
      "ðŸ§  NEURAL NETWORK\n",
      "----------------------------------------------------------------------\n",
      "âš ï¸  TensorFlow/Keras not installed. Skipping Neural Network training.\n",
      "Install with: pip install tensorflow\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "RandomForest        : ROC-AUC = 0.9121\n",
      "\n",
      "ðŸ† BEST MODEL: RandomForest (ROC-AUC = 0.9121)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "n_neg = (y_train_array == 0).sum()\n",
    "n_pos = (y_train_array == 1).sum()\n",
    "class_weight_ratio = n_neg / n_pos\n",
    "\n",
    "print(f\"Class counts: Negative={n_neg}, Positive={n_pos}\")\n",
    "print(f\"Class weight ratio (neg/pos) = {class_weight_ratio:.4f}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING MODELS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Dictionary to store all trained models and their scores\n",
    "models = {}\n",
    "model_scores = {}\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM FOREST TRAINING\n",
    "# ============================================================================\n",
    "if TRAIN_RANDOM_FOREST:\n",
    "    print(\"ðŸŒ² RANDOM FOREST\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if ENABLE_ITERATIVE_TRAINING:\n",
    "        # Iterative training: try multiple configurations\n",
    "        print(\"Iterative training mode: trying multiple configurations\\n\")\n",
    "        \n",
    "        best_rf_roc_auc = 0\n",
    "        best_rf_model = None\n",
    "        best_rf_config = None\n",
    "        \n",
    "        # Define configurations to try\n",
    "        rf_configs = [\n",
    "            {\"n_estimators\": 100, \"max_depth\": 15, \"min_samples_split\": 10, \"min_samples_leaf\": 4},\n",
    "            {\"n_estimators\": 150, \"max_depth\": 20, \"min_samples_split\": 8, \"min_samples_leaf\": 3},\n",
    "            {\"n_estimators\": 200, \"max_depth\": 25, \"min_samples_split\": 5, \"min_samples_leaf\": 2},\n",
    "            {\"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 15, \"min_samples_leaf\": 5},\n",
    "            {\"n_estimators\": 250, \"max_depth\": 30, \"min_samples_split\": 4, \"min_samples_leaf\": 2},\n",
    "        ]\n",
    "        \n",
    "        for i, config in enumerate(rf_configs[:N_ITERATIONS], 1):\n",
    "            print(f\"  Config {i}/{N_ITERATIONS}: {config}\")\n",
    "            \n",
    "            temp_rf = RandomForestClassifier(\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                **config\n",
    "            )\n",
    "            \n",
    "            temp_rf.fit(X_train_processed, y_train_array)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            temp_pred_proba = temp_rf.predict_proba(X_test_processed)[:, 1]\n",
    "            temp_roc_auc = roc_auc_score(y_test_array, temp_pred_proba)\n",
    "            \n",
    "            print(f\"    â†’ ROC-AUC: {temp_roc_auc:.4f}\")\n",
    "            \n",
    "            if temp_roc_auc > best_rf_roc_auc:\n",
    "                best_rf_roc_auc = temp_roc_auc\n",
    "                best_rf_model = temp_rf\n",
    "                best_rf_config = config\n",
    "                print(f\"    âœ“ New best!\")\n",
    "            print()\n",
    "        \n",
    "        models['RandomForest'] = best_rf_model\n",
    "        model_scores['RandomForest'] = best_rf_roc_auc\n",
    "        print(f\"Best RandomForest: ROC-AUC = {best_rf_roc_auc:.4f}\")\n",
    "        print(f\"Config: {best_rf_config}\\n\")\n",
    "    else:\n",
    "        # Single training with default configuration\n",
    "        rf_model = RandomForestClassifier(\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"Training RandomForest model...\")\n",
    "        rf_model.fit(X_train_processed, y_train_array)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        rf_pred_proba = rf_model.predict_proba(X_test_processed)[:, 1]\n",
    "        rf_roc_auc = roc_auc_score(y_test_array, rf_pred_proba)\n",
    "        \n",
    "        models['RandomForest'] = rf_model\n",
    "        model_scores['RandomForest'] = rf_roc_auc\n",
    "        print(f\"âœ“ Training complete!\")\n",
    "        print(f\"RandomForest ROC-AUC: {rf_roc_auc:.4f}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK TRAINING\n",
    "# ============================================================================\n",
    "if TRAIN_NEURAL_NETWORK:\n",
    "    print(\"ðŸ§  NEURAL NETWORK\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        tf.random.set_seed(42)\n",
    "        \n",
    "        # Calculate class weights for neural network\n",
    "        class_weight_dict = {\n",
    "            0: len(y_train_array) / (2 * n_neg),\n",
    "            1: len(y_train_array) / (2 * n_pos)\n",
    "        }\n",
    "        \n",
    "        print(f\"Building neural network architecture...\")\n",
    "        print(f\"Input features: {X_train_processed.shape[1]}\")\n",
    "        \n",
    "        # Build neural network\n",
    "        nn_model = keras.Sequential([\n",
    "            layers.Input(shape=(X_train_processed.shape[1],)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        nn_model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                keras.metrics.AUC(name='auc'),\n",
    "                keras.metrics.Precision(name='precision'),\n",
    "                keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel architecture:\")\n",
    "        nn_model.summary()\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining neural network:\")\n",
    "        print(f\"  Epochs: {NN_EPOCHS}\")\n",
    "        print(f\"  Batch size: {NN_BATCH_SIZE}\")\n",
    "        print(f\"  Validation split: {NN_VALIDATION_SPLIT}\")\n",
    "        print(f\"  Class weights: {class_weight_dict}\")\n",
    "        print()\n",
    "        \n",
    "        # Train model\n",
    "        history = nn_model.fit(\n",
    "            X_train_processed, y_train_array,\n",
    "            epochs=NN_EPOCHS,\n",
    "            batch_size=NN_BATCH_SIZE,\n",
    "            validation_split=NN_VALIDATION_SPLIT,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        nn_pred_proba = nn_model.predict(X_test_processed, verbose=0).flatten()\n",
    "        nn_roc_auc = roc_auc_score(y_test_array, nn_pred_proba)\n",
    "        \n",
    "        models['NeuralNetwork'] = nn_model\n",
    "        model_scores['NeuralNetwork'] = nn_roc_auc\n",
    "        \n",
    "        print(f\"\\nâœ“ Neural Network training complete!\")\n",
    "        print(f\"Neural Network ROC-AUC: {nn_roc_auc:.4f}\")\n",
    "        print(f\"Best epoch: {len(history.history['loss']) - early_stopping.patience if early_stopping.stopped_epoch > 0 else len(history.history['loss'])}\\n\")\n",
    "        \n",
    "        # Store training history for later visualization\n",
    "        nn_training_history = history.history\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  TensorFlow/Keras not installed. Skipping Neural Network training.\")\n",
    "        print(\"Install with: pip install tensorflow\")\n",
    "        TRAIN_NEURAL_NETWORK = False\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if model_scores:\n",
    "    for model_name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{model_name:20s}: ROC-AUC = {score:.4f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(model_scores, key=model_scores.get)\n",
    "    best_model = models[best_model_name]\n",
    "    best_score = model_scores[best_model_name]\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST MODEL: {best_model_name} (ROC-AUC = {best_score:.4f})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set the main 'model' variable to the best model for later use\n",
    "    model = best_model\n",
    "    model_type = best_model_name\n",
    "else:\n",
    "    print(\"No models were trained!\")\n",
    "    raise RuntimeError(\"Please enable at least one model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b10f3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network not trained or history not available.\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_NEURAL_NETWORK and 'nn_training_history' in locals():\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0, 0].plot(nn_training_history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(nn_training_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Model Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: AUC\n",
    "    axes[0, 1].plot(nn_training_history['auc'], label='Training AUC', linewidth=2)\n",
    "    axes[0, 1].plot(nn_training_history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('AUC')\n",
    "    axes[0, 1].set_title('Model AUC over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Accuracy\n",
    "    axes[1, 0].plot(nn_training_history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[1, 0].plot(nn_training_history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('Model Accuracy over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Precision & Recall\n",
    "    axes[1, 1].plot(nn_training_history['precision'], label='Training Precision', linewidth=2)\n",
    "    axes[1, 1].plot(nn_training_history['val_precision'], label='Validation Precision', linewidth=2, linestyle='--')\n",
    "    axes[1, 1].plot(nn_training_history['recall'], label='Training Recall', linewidth=2)\n",
    "    axes[1, 1].plot(nn_training_history['val_recall'], label='Validation Recall', linewidth=2, linestyle='--')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Precision & Recall over Epochs')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('submissions/nn_training_history.png', dpi=100, bbox_inches='tight')\n",
    "    print(\"âœ“ Training history plot saved to: submissions/nn_training_history.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best epoch info\n",
    "    best_epoch = np.argmax(nn_training_history['val_auc']) + 1\n",
    "    best_val_auc = max(nn_training_history['val_auc'])\n",
    "    print(f\"\\nBest validation AUC: {best_val_auc:.4f} at epoch {best_epoch}\")\n",
    "else:\n",
    "    print(\"Neural Network not trained or history not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3afe48",
   "metadata": {},
   "source": [
    "## 13b. Visualize Neural Network Training (if applicable)\n",
    "\n",
    "Plot training history to see how the neural network learned over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bcd09",
   "metadata": {},
   "source": [
    "## 14. Evaluate Model on Test Split\n",
    "\n",
    "Evaluate the trained model using multiple metrics: ROC-AUC, F1, precision, recall, and accuracy on the provided test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60705fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST SPLIT EVALUATION - RandomForest\n",
      "======================================================================\n",
      "ROC-AUC Score:  0.9121\n",
      "F1 Score:       0.9195\n",
      "Precision:      0.9356\n",
      "Recall:         0.9040\n",
      "Accuracy:       0.8736\n",
      "======================================================================\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Paid Back       0.66      0.75      0.71     23900\n",
      "    Paid Back       0.94      0.90      0.92     94899\n",
      "\n",
      "     accuracy                           0.87    118799\n",
      "    macro avg       0.80      0.83      0.81    118799\n",
      " weighted avg       0.88      0.87      0.88    118799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test split using the best model\n",
    "if model_type == 'NeuralNetwork':\n",
    "    y_test_pred_proba = model.predict(X_test_processed, verbose=0).flatten()\n",
    "    y_test_pred = (y_test_pred_proba > 0.5).astype(int)\n",
    "else:  # RandomForest\n",
    "    y_test_pred = model.predict(X_test_processed)\n",
    "    y_test_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "roc_auc = roc_auc_score(y_test_array, y_test_pred_proba)\n",
    "f1 = f1_score(y_test_array, y_test_pred)\n",
    "precision = precision_score(y_test_array, y_test_pred)\n",
    "recall = recall_score(y_test_array, y_test_pred)\n",
    "accuracy = accuracy_score(y_test_array, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"=\"*70)\n",
    "print(f\"TEST SPLIT EVALUATION - {model_type}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ROC-AUC Score:  {roc_auc:.4f}\")\n",
    "print(f\"F1 Score:       {f1:.4f}\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test_array, y_test_pred, target_names=['Not Paid Back', 'Paid Back']))\n",
    "\n",
    "# If we trained both models, show comparison\n",
    "if len(model_scores) > 1:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL MODELS COMPARISON ON TEST SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    for name, val_score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        # Evaluate each model on test split\n",
    "        if name == 'NeuralNetwork':\n",
    "            temp_pred_proba = models[name].predict(X_test_processed, verbose=0).flatten()\n",
    "        else:\n",
    "            temp_pred_proba = models[name].predict_proba(X_test_processed)[:, 1]\n",
    "        temp_roc = roc_auc_score(y_test_array, temp_pred_proba)\n",
    "        print(f\"{name:20s}: ROC-AUC = {temp_roc:.4f} {'ðŸ†' if name == model_type else ''}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d682a00",
   "metadata": {},
   "source": [
    "## 15. Generate Test Predictions\n",
    "\n",
    "Apply the trained model to the preprocessed test set to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2ca0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real test data shape: (254569, 12)\n",
      "Real test columns: ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "\n",
      "Expected submission rows: 254569\n",
      "First few rows:\n",
      "       id  annual_income  debt_to_income_ratio  credit_score  loan_amount  \\\n",
      "0  593994       28781.05                 0.049           626     11461.42   \n",
      "1  593995       46626.39                 0.093           732     15492.25   \n",
      "2  593996       54954.89                 0.367           611      3796.41   \n",
      "3  593997       25644.63                 0.110           671      6574.30   \n",
      "4  593998       25169.64                 0.081           688     17696.89   \n",
      "\n",
      "   interest_rate  gender marital_status education_level employment_status  \\\n",
      "0          14.73  Female         Single     High School          Employed   \n",
      "1          12.85  Female        Married        Master's          Employed   \n",
      "2          13.29    Male         Single      Bachelor's          Employed   \n",
      "3           9.57  Female         Single      Bachelor's          Employed   \n",
      "4          12.80  Female        Married             PhD          Employed   \n",
      "\n",
      "         loan_purpose grade_subgrade  \n",
      "0               Other             D5  \n",
      "1               Other             C1  \n",
      "2  Debt consolidation             D1  \n",
      "3  Debt consolidation             C3  \n",
      "4            Business             C1  \n"
     ]
    }
   ],
   "source": [
    "# Load the real test data (no labels - for final submission)\n",
    "real_test_df = pd.read_csv('Data/test.csv')\n",
    "\n",
    "print(f\"Real test data shape: {real_test_df.shape}\")\n",
    "print(f\"Real test columns: {list(real_test_df.columns)}\")\n",
    "print(f\"\\nExpected submission rows: {len(real_test_df)}\")\n",
    "print(f\"First few rows:\")\n",
    "print(real_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8106c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real test preprocessed shape: (254569, 60)\n",
      "Features: 60 (5 numerical + 55 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess real test data using the same transformers\n",
    "# (Already fitted on training data)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "real_test_cat_encoded = ohe.transform(real_test_df[categorical_cols])\n",
    "\n",
    "# Scale numerical features\n",
    "real_test_num_scaled = scaler.transform(real_test_df[numerical_cols])\n",
    "\n",
    "# Combine features\n",
    "real_test_processed = np.concatenate([real_test_num_scaled, real_test_cat_encoded], axis=1)\n",
    "\n",
    "print(f\"Real test preprocessed shape: {real_test_processed.shape}\")\n",
    "print(f\"Features: {real_test_processed.shape[1]} ({len(numerical_cols)} numerical + {real_test_cat_encoded.shape[1]} categorical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1690e131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions with: RandomForest\n",
      "Real test predictions shape: (254569,)\n",
      "Unique predictions: [0. 1.]\n",
      "Prediction distribution:\n",
      "  Class 0 (Not Paid): 57453\n",
      "  Class 1 (Paid): 197116\n",
      "\n",
      "âœ“ Correct number of predictions: 254569\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on real test data using the best model\n",
    "if model_type == 'NeuralNetwork':\n",
    "    real_test_pred_proba = model.predict(real_test_processed, verbose=0).flatten()\n",
    "    real_test_predictions = (real_test_pred_proba > 0.5).astype(int)\n",
    "else:  # RandomForest\n",
    "    real_test_predictions = model.predict(real_test_processed)\n",
    "\n",
    "print(f\"Generating predictions with: {model_type}\")\n",
    "print(f\"Real test predictions shape: {real_test_predictions.shape}\")\n",
    "print(f\"Unique predictions: {np.unique(real_test_predictions)}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(f\"  Class 0 (Not Paid): {(real_test_predictions == 0).sum()}\")\n",
    "print(f\"  Class 1 (Paid): {(real_test_predictions == 1).sum()}\")\n",
    "\n",
    "# Verify we have exactly 254,569 predictions\n",
    "assert len(real_test_predictions) == 254569, f\"Expected 254569 predictions, got {len(real_test_predictions)}\"\n",
    "print(f\"\\nâœ“ Correct number of predictions: {len(real_test_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee269b2",
   "metadata": {},
   "source": [
    "## 17. Generate Final Submission Predictions\n",
    "\n",
    "Predict on the real test data (254,569 rows) to create the final submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1288e3d",
   "metadata": {},
   "source": [
    "## 16. Preprocess Real Test Data\n",
    "\n",
    "Apply the same preprocessing (one-hot encoding and scaling) to the real test data using the fitted transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599b5cb",
   "metadata": {},
   "source": [
    "## 18. Save Validation Results and Final Submission\n",
    "\n",
    "Create submissions folder, save validation results (test_split evaluation) and final submission (real test.csv predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5286c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FINAL SUBMISSION saved to: submissions/submission_001.csv\n",
      "  Rows: 254569 (should be 254,569)\n",
      "âœ“ Validation results saved to: submissions/validation_001.csv\n",
      "âœ“ Metrics saved to: submissions/metrics_001.csv\n",
      "âœ“ Copy saved to: submission.csv\n",
      "\n",
      "======================================================================\n",
      "SUBMISSION #1\n",
      "======================================================================\n",
      "Model: RandomForest\n",
      "Training: Original (475195 samples)\n",
      "Validation (test_split): 118799 samples\n",
      "  â†’ ROC-AUC: 0.9121 | F1: 0.9195 | Accuracy: 0.8736\n",
      "Final Submission (test.csv): 254569 predictions\n",
      "======================================================================\n",
      "\n",
      "First few submission rows:\n",
      "       id  loan_paid_back\n",
      "0  593994               1\n",
      "1  593995               1\n",
      "2  593996               0\n",
      "3  593997               1\n",
      "4  593998               1\n",
      "5  593999               1\n",
      "6  594000               1\n",
      "7  594001               1\n",
      "8  594002               1\n",
      "9  594003               0\n",
      "\n",
      "Last few submission rows:\n",
      "            id  loan_paid_back\n",
      "254559  848553               1\n",
      "254560  848554               1\n",
      "254561  848555               1\n",
      "254562  848556               0\n",
      "254563  848557               1\n",
      "254564  848558               1\n",
      "254565  848559               1\n",
      "254566  848560               1\n",
      "254567  848561               1\n",
      "254568  848562               1\n",
      "âœ“ Validation results saved to: submissions/validation_001.csv\n",
      "âœ“ Metrics saved to: submissions/metrics_001.csv\n",
      "âœ“ Copy saved to: submission.csv\n",
      "\n",
      "======================================================================\n",
      "SUBMISSION #1\n",
      "======================================================================\n",
      "Model: RandomForest\n",
      "Training: Original (475195 samples)\n",
      "Validation (test_split): 118799 samples\n",
      "  â†’ ROC-AUC: 0.9121 | F1: 0.9195 | Accuracy: 0.8736\n",
      "Final Submission (test.csv): 254569 predictions\n",
      "======================================================================\n",
      "\n",
      "First few submission rows:\n",
      "       id  loan_paid_back\n",
      "0  593994               1\n",
      "1  593995               1\n",
      "2  593996               0\n",
      "3  593997               1\n",
      "4  593998               1\n",
      "5  593999               1\n",
      "6  594000               1\n",
      "7  594001               1\n",
      "8  594002               1\n",
      "9  594003               0\n",
      "\n",
      "Last few submission rows:\n",
      "            id  loan_paid_back\n",
      "254559  848553               1\n",
      "254560  848554               1\n",
      "254561  848555               1\n",
      "254562  848556               0\n",
      "254563  848557               1\n",
      "254564  848558               1\n",
      "254565  848559               1\n",
      "254566  848560               1\n",
      "254567  848561               1\n",
      "254568  848562               1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Create submissions directory\n",
    "os.makedirs('submissions', exist_ok=True)\n",
    "\n",
    "# Find the next submission number\n",
    "existing_submissions = glob.glob('submissions/submission_*.csv')\n",
    "if existing_submissions:\n",
    "    # Extract numbers from filenames\n",
    "    numbers = []\n",
    "    for f in existing_submissions:\n",
    "        try:\n",
    "            num = int(f.split('_')[-1].replace('.csv', ''))\n",
    "            numbers.append(num)\n",
    "        except:\n",
    "            pass\n",
    "    next_num = max(numbers) + 1 if numbers else 1\n",
    "else:\n",
    "    next_num = 1\n",
    "\n",
    "# Generate submission filenames\n",
    "submission_filename = f'submission_{next_num:03d}.csv'\n",
    "submission_path = os.path.join('submissions', submission_filename)\n",
    "validation_filename = f'validation_{next_num:03d}.csv'\n",
    "validation_path = os.path.join('submissions', validation_filename)\n",
    "metrics_filename = f'metrics_{next_num:03d}.csv'\n",
    "metrics_path = os.path.join('submissions', metrics_filename)\n",
    "\n",
    "# Save FINAL SUBMISSION (real test.csv predictions - 254,569 rows)\n",
    "final_submission = pd.DataFrame({\n",
    "    \"id\": real_test_df[\"id\"].astype(int),\n",
    "    \"loan_paid_back\": real_test_predictions.astype(int)\n",
    "})\n",
    "final_submission.to_csv(submission_path, index=False)\n",
    "print(f\"âœ“ FINAL SUBMISSION saved to: {submission_path}\")\n",
    "print(f\"  Rows: {len(final_submission)} (should be 254,569)\")\n",
    "\n",
    "# Save VALIDATION RESULTS (test_split evaluation with labels)\n",
    "validation_results = pd.DataFrame({\n",
    "    \"id\": X_test[\"id\"].astype(int) if \"id\" in X_test.columns else np.arange(len(y_test_array)),\n",
    "    \"y_true\": y_test_array.astype(int),\n",
    "    \"y_pred\": y_test_pred.astype(int),\n",
    "    \"y_proba\": y_test_pred_proba.astype(float)\n",
    "})\n",
    "validation_results.to_csv(validation_path, index=False)\n",
    "print(f\"âœ“ Validation results saved to: {validation_path}\")\n",
    "\n",
    "# Save aggregate metrics with configuration info\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"submission_num\": next_num,\n",
    "        \"model_type\": model_type,\n",
    "        \"use_smote\": USE_SMOTE,\n",
    "        \"iterative_training\": ENABLE_ITERATIVE_TRAINING,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_samples\": len(y_train_array),\n",
    "        \"validation_samples\": len(y_test_array),\n",
    "        \"submission_samples\": len(final_submission)\n",
    "    }\n",
    "])\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"âœ“ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Also save final submission to root for easy access\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "print(f\"âœ“ Copy saved to: submission.csv\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SUBMISSION #{next_num}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model: {model_type}\")\n",
    "print(f\"Training: {'SMOTE-balanced' if USE_SMOTE else 'Original'} ({len(y_train_array)} samples)\")\n",
    "print(f\"Validation (test_split): {len(y_test_array)} samples\")\n",
    "print(f\"  â†’ ROC-AUC: {roc_auc:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Final Submission (test.csv): {len(final_submission)} predictions\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nFirst few submission rows:\")\n",
    "print(final_submission.head(10))\n",
    "print(\"\\nLast few submission rows:\")\n",
    "print(final_submission.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39632400",
   "metadata": {},
   "source": [
    "## 19. Save Model and Preprocessors\n",
    "\n",
    "Save the trained model and preprocessing objects (scaler and encoder) for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "802cdc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RandomForest model saved to: models/loan_model_rf.pkl\n",
      "âœ“ RandomForest saved to: models/loan_model_rf.pkl\n",
      "âœ“ Scaler saved to: models/scaler.pkl\n",
      "âœ“ Encoder saved to: models/encoder.pkl\n",
      "\n",
      "======================================================================\n",
      "âœ… TRAINING COMPLETE\n",
      "======================================================================\n",
      "Best Model: RandomForest\n",
      "Configuration: USE_SMOTE = False\n",
      "ROC-AUC: 0.9121 | F1: 0.9195 | Accuracy: 0.8736\n",
      "Submission saved in: submissions/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the best model (based on model type)\n",
    "if model_type == 'NeuralNetwork':\n",
    "    model_path = 'models/loan_model_nn.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"âœ“ Neural Network model saved to: {model_path}\")\n",
    "else:  # RandomForest\n",
    "    model_path = 'models/loan_model_rf.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"âœ“ RandomForest model saved to: {model_path}\")\n",
    "\n",
    "# Save ALL trained models\n",
    "if TRAIN_RANDOM_FOREST and 'RandomForest' in models:\n",
    "    rf_path = 'models/loan_model_rf.pkl'\n",
    "    joblib.dump(models['RandomForest'], rf_path)\n",
    "    print(f\"âœ“ RandomForest saved to: {rf_path}\")\n",
    "\n",
    "if TRAIN_NEURAL_NETWORK and 'NeuralNetwork' in models:\n",
    "    nn_path = 'models/loan_model_nn.keras'\n",
    "    models['NeuralNetwork'].save(nn_path)\n",
    "    print(f\"âœ“ Neural Network saved to: {nn_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = 'models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ“ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save the encoder\n",
    "encoder_path = 'models/encoder.pkl'\n",
    "joblib.dump(ohe, encoder_path)\n",
    "print(f\"âœ“ Encoder saved to: {encoder_path}\")\n",
    "\n",
    "# Save model comparison results\n",
    "if len(model_scores) > 1:\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\"model\": name, \"roc_auc\": score} \n",
    "        for name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ])\n",
    "    comparison_path = 'models/model_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"âœ“ Model comparison saved to: {comparison_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Model: {model_type}\")\n",
    "print(f\"Configuration: USE_SMOTE = {USE_SMOTE}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Submission saved in: submissions/\")\n",
    "if len(model_scores) > 1:\n",
    "    print(f\"\\nModels trained: {', '.join(model_scores.keys())}\")\n",
    "    print(f\"Winner: {model_type}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed6e4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUBMISSION HISTORY\n",
      "================================================================================\n",
      " submission_num   model_type  use_smote  iterative_training  roc_auc       f1  precision   recall  accuracy  train_samples  validation_samples  submission_samples\n",
      "              1 RandomForest      False               False 0.912135 0.919517   0.935619 0.903961  0.873593         475195              118799              254569\n",
      "================================================================================\n",
      "\n",
      "ðŸ† BEST SUBMISSION: #1 with ROC-AUC = 0.9121\n",
      "   Configuration: USE_SMOTE = False\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Find all metrics files\n",
    "metrics_files = sorted(glob.glob('submissions/metrics_*.csv'))\n",
    "\n",
    "if metrics_files:\n",
    "    # Load and combine all metrics\n",
    "    all_metrics = []\n",
    "    for f in metrics_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            all_metrics.append(df)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if all_metrics:\n",
    "        history_df = pd.concat(all_metrics, ignore_index=True)\n",
    "        history_df = history_df.sort_values('submission_num')\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"SUBMISSION HISTORY\")\n",
    "        print(\"=\"*80)\n",
    "        print(history_df.to_string(index=False))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Find best submission by ROC-AUC\n",
    "        best_idx = history_df['roc_auc'].idxmax()\n",
    "        best_sub = history_df.loc[best_idx]\n",
    "        print(f\"\\nðŸ† BEST SUBMISSION: #{int(best_sub['submission_num'])} with ROC-AUC = {best_sub['roc_auc']:.4f}\")\n",
    "        print(f\"   Configuration: USE_SMOTE = {bool(best_sub['use_smote'])}\")\n",
    "    else:\n",
    "        print(\"No valid metrics files found.\")\n",
    "else:\n",
    "    print(\"No submission history yet. This is your first submission!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3fcb7",
   "metadata": {},
   "source": [
    "## 20. View Submission History\n",
    "\n",
    "Display all previous submissions and their metrics for comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
