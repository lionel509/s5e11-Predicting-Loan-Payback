{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0ed472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG ====\n",
    "RUN_TAG = \"rf_v1\"  # change per experiment, e.g., \"rf_v2_bins\", \"xgb_v1\"\n",
    "TARGET_COL = \"loan_paid_back\"\n",
    "ID_COL = \"id\"  # change if your test id column is named differently\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "RAW_TRAIN = f\"{DATA_DIR}/train.csv\"\n",
    "RAW_TEST = f\"{DATA_DIR}/test.csv\"\n",
    "SAMPLE_SUB = f\"{DATA_DIR}/sample_submission.csv\"\n",
    "\n",
    "import os, json, time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ff73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Clips numeric columns to given quantiles learned from training data only.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower=0.01, upper=0.99):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.qs_ = None\n",
    "        self.columns_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.columns_ = X.columns.tolist()\n",
    "        self.qs_ = {}\n",
    "        for c in self.columns_:\n",
    "            s = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            self.qs_[c] = (np.nanquantile(s, self.lower), np.nanquantile(s, self.upper))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for c in self.columns_:\n",
    "            low, high = self.qs_[c]\n",
    "            s = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            s = s.clip(low, high)\n",
    "            X[c] = s\n",
    "        return X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa360e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\n",
      "Categorical: ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(RAW_TRAIN)\n",
    "test_df  = pd.read_csv(RAW_TEST)\n",
    "\n",
    "# Standardize column names (strip spaces, lowercase)\n",
    "train_df.columns = [c.strip().lower() for c in train_df.columns]\n",
    "test_df.columns  = [c.strip().lower() for c in test_df.columns]\n",
    "\n",
    "assert TARGET_COL in train_df.columns, f\"{TARGET_COL} not found in train\"\n",
    "assert ID_COL in test_df.columns, f\"{ID_COL} not found in test\"\n",
    "\n",
    "# Split X/y\n",
    "X = train_df.drop(columns=[TARGET_COL])\n",
    "y = train_df[TARGET_COL].astype(int)\n",
    "\n",
    "# Identify column types\n",
    "numeric_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "print(\"Numeric:\", numeric_cols)\n",
    "print(\"Categorical:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312adf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw splits to data/processed/rf_v1__20251101-040747\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=SEED\n",
    " )\n",
    "\n",
    "# Create versioned output folders\n",
    "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_ID = f\"{RUN_TAG}__{ts}\"\n",
    "PROCESSED_DIR = Path(f\"{DATA_DIR}/processed/{RUN_ID}\")\n",
    "MODELS_DIR    = Path(f\"models/{RUN_ID}\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the raw split CSVs (for reproducibility)\n",
    "X_train.assign(**{TARGET_COL: y_train}).to_csv(PROCESSED_DIR / \"train_split_raw.csv\", index=False)\n",
    "X_val.assign(**{TARGET_COL: y_val}).to_csv(PROCESSED_DIR / \"val_split_raw.csv\", index=False)\n",
    "test_df.to_csv(PROCESSED_DIR / \"test_raw.csv\", index=False)\n",
    "print(f\"Saved raw splits to {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95566c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric pipeline: median impute -> clip outliers -> scale\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"clipper\", QuantileClipper(lower=0.01, upper=0.99)),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))  # with_mean=False keeps sparse compatibility\n",
    "])\n",
    "\n",
    "# Categorical pipeline: constant impute -> one-hot (ignore unknowns)\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, numeric_cols),\n",
    "        (\"cat\", cat_pipe, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    " )\n",
    "\n",
    "# Full model pipeline: preprocessing + RandomForest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", rf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ec4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.9089\n",
      "Saved artifacts to models/rf_v1__20251101-040747\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Validation evaluation\n",
    "y_val_pred  = pipeline.predict(X_val)\n",
    "y_val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "auc = roc_auc_score(y_val, y_val_proba)\n",
    "cm  = confusion_matrix(y_val, y_val_pred).tolist()\n",
    "\n",
    "metrics = {\n",
    "    \"roc_auc\": float(auc),\n",
    "    \"classification_report\": report,\n",
    "    \"confusion_matrix\": cm,\n",
    "    \"n_train\": int(len(y_train)),\n",
    "    \"n_val\": int(len(y_val)),\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"model\": \"RandomForestClassifier\"\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Persist pipeline (preprocessing + model)\n",
    "dump(pipeline, MODELS_DIR / \"pipeline.joblib\")\n",
    "\n",
    "# Persist column lists (for reference)\n",
    "with open(MODELS_DIR / \"columns.json\", \"w\") as f:\n",
    "    json.dump({\"numeric\": numeric_cols, \"categorical\": categorical_cols}, f, indent=2)\n",
    "\n",
    "print(f\"ROC-AUC: {auc:.4f}\")\n",
    "print(f\"Saved artifacts to {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e742b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after preprocessing\n",
    "ohe = pipeline.named_steps[\"prep\"].named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "num_features = numeric_cols\n",
    "cat_features = list(ohe.get_feature_names_out(categorical_cols))\n",
    "all_features = num_features + cat_features\n",
    "\n",
    "rf_model = pipeline.named_steps[\"model\"]\n",
    "importances = rf_model.feature_importances_\n",
    "feat_imp = pd.DataFrame({\"feature\": all_features, \"importance\": importances}) \\\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "feat_imp.head(50).to_csv(MODELS_DIR / \"feature_importances_top50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcfe99ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed matrices.\n"
     ]
    }
   ],
   "source": [
    "# Save transformed matrices in NPZ for future reuse\n",
    "from scipy import sparse\n",
    "\n",
    "Xtr_t = pipeline.named_steps[\"prep\"].transform(X_train)\n",
    "Xv_t  = pipeline.named_steps[\"prep\"].transform(X_val)\n",
    "Xt_t  = pipeline.named_steps[\"prep\"].transform(test_df)\n",
    "\n",
    "sparse.save_npz(PROCESSED_DIR / \"X_train_proc.npz\", Xtr_t)\n",
    "sparse.save_npz(PROCESSED_DIR / \"X_val_proc.npz\", Xv_t)\n",
    "sparse.save_npz(PROCESSED_DIR / \"X_test_proc.npz\", Xt_t)\n",
    "pd.Series(y_train.values).to_csv(PROCESSED_DIR / \"y_train.csv\", index=False, header=[\"y\"])\n",
    "pd.Series(y_val.values).to_csv(PROCESSED_DIR / \"y_val.csv\", index=False, header=[\"y\"])\n",
    "print(\"Saved processed matrices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f280f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved -> data/processed/rf_v1__20251101-040747/submission.csv\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "assert ID_COL in sample.columns, f\"{ID_COL} not in sample_submission\"\n",
    "\n",
    "test_pred = pipeline.predict(test_df)  # use .predict_proba(...)[:,1] if the competition wants probabilities\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: sample[ID_COL],  # keep ordering identical to sample\n",
    "    TARGET_COL: test_pred\n",
    "})\n",
    "SUB_PATH = PROCESSED_DIR / \"submission.csv\"\n",
    "submission.to_csv(SUB_PATH, index=False)\n",
    "print(f\"Submission saved -> {SUB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57838e7",
   "metadata": {},
   "source": [
    "# Loan Payback Prediction - Training & Evaluation\n",
    "\n",
    "This notebook loads pre-created training and test splits, with optional SMOTE balancing, preprocesses features, trains a RandomForest model, evaluates on the test split, and auto-increments submissions in a dedicated folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c090a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and train_test_split from sklearn.model_selection for splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674b866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83675948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: USE_SMOTE = False\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Set to True to use SMOTE-balanced training data\n",
    "USE_SMOTE = False  # Change to True to use train_split_smote.csv\n",
    "\n",
    "print(f\"Configuration: USE_SMOTE = {USE_SMOTE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d45c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative training: DISABLED\n",
      "\n",
      "Models to train:\n",
      "  - RandomForest: YES\n",
      "  - Neural Network: YES\n",
      "    Epochs: 50, Batch size: 32, Validation split: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Optional: Enable iterative training to search for best hyperparameters\n",
    "ENABLE_ITERATIVE_TRAINING = False  # Set to True to try multiple configurations\n",
    "N_ITERATIONS = 5  # Number of different configurations to try\n",
    "\n",
    "# Model selection: Train both models and compare\n",
    "TRAIN_RANDOM_FOREST = True   # Train RandomForest model\n",
    "TRAIN_NEURAL_NETWORK = True  # Train Neural Network model\n",
    "\n",
    "# Neural Network configuration\n",
    "NN_EPOCHS = 50\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_VALIDATION_SPLIT = 0.1\n",
    "\n",
    "print(f\"Iterative training: {'ENABLED' if ENABLE_ITERATIVE_TRAINING else 'DISABLED'}\")\n",
    "if ENABLE_ITERATIVE_TRAINING:\n",
    "    print(f\"Will train {N_ITERATIONS} models with different configurations\")\n",
    "print(f\"\\nModels to train:\")\n",
    "print(f\"  - RandomForest: {'YES' if TRAIN_RANDOM_FOREST else 'NO'}\")\n",
    "print(f\"  - Neural Network: {'YES' if TRAIN_NEURAL_NETWORK else 'NO'}\")\n",
    "if TRAIN_NEURAL_NETWORK:\n",
    "    print(f\"    Epochs: {NN_EPOCHS}, Batch size: {NN_BATCH_SIZE}, Validation split: {NN_VALIDATION_SPLIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6f5b9",
   "metadata": {},
   "source": [
    "## 2b. Training Configuration (Optional)\n",
    "\n",
    "Enable iterative training to find the best hyperparameters by training multiple times with validation monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bba476",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set whether to use SMOTE-balanced training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9bb72",
   "metadata": {},
   "source": [
    "## 3. Load Training and Test Split Data\n",
    "\n",
    "Load the train_split.csv (or train_split_smote.csv) and test_split.csv files from the Data/splits directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f853be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file: Data/splits/train_split.csv\n",
      "Train split shape: (475195, 13)\n",
      "Test split shape: (118799, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test split datasets\n",
    "train_file = 'Data/splits/train_split_smote.csv' if USE_SMOTE else 'Data/splits/train_split.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv('Data/splits/test_split.csv')\n",
    "\n",
    "print(f\"Training file: {train_file}\")\n",
    "print(f\"Train split shape: {train_df.shape}\")\n",
    "print(f\"Test split shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4206f781",
   "metadata": {},
   "source": [
    "## 4. Prepare Features and Target Variables\n",
    "\n",
    "Separate features (X) and target variable (y) for both splits by dropping the 'loan_paid_back' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8126624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (475195, 12)\n",
      "y_train shape: (475195,)\n",
      "X_test shape:  (118799, 12)\n",
      "y_test shape:  (118799,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variables for both splits\n",
    "X_train = train_df.drop(\"loan_paid_back\", axis=1)\n",
    "y_train = train_df[\"loan_paid_back\"]\n",
    "\n",
    "X_test = test_df.drop(\"loan_paid_back\", axis=1)\n",
    "y_test = test_df[\"loan_paid_back\"]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49941a4d",
   "metadata": {},
   "source": [
    "## 5. Check Class Distribution in Provided Splits\n",
    "\n",
    "Confirm the class balance in y_train and y_test to ensure splits look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b99b15e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train (proportion):\n",
      "loan_paid_back\n",
      "0.0    0.201181\n",
      "1.0    0.798819\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in y_test (proportion):\n",
      "loan_paid_back\n",
      "0.0    0.20118\n",
      "1.0    0.79882\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in the provided splits\n",
    "print(\"Class distribution in y_train (proportion):\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nClass distribution in y_test (proportion):\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a543136",
   "metadata": {},
   "source": [
    "## 6. Sanity-Check Feature Columns\n",
    "\n",
    "Verify that train and test splits have matching feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d4bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: (12)\n",
      "['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "\n",
      "X_test columns:  (12)\n",
      "['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "\n",
      "Columns match: True\n"
     ]
    }
   ],
   "source": [
    "# Ensure feature columns align between train and test splits\n",
    "train_cols = list(X_train.columns)\n",
    "test_cols = list(X_test.columns)\n",
    "print(f\"X_train columns: ({len(train_cols)})\")\n",
    "print(train_cols)\n",
    "print(f\"\\nX_test columns:  ({len(test_cols)})\")\n",
    "print(test_cols)\n",
    "print(\"\\nColumns match:\", train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1157223",
   "metadata": {},
   "source": [
    "## 7. Identify Categorical and Numerical Columns\n",
    "\n",
    "Separate columns into categorical and numerical types for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b5e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns (6): ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "Numerical columns (5): ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns (object dtype)\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Identify numerical columns (excluding 'id' if present, as it's not a useful feature)\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'id' in numerical_cols:\n",
    "    numerical_cols.remove('id')\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365256c1",
   "metadata": {},
   "source": [
    "## 8. Import Preprocessing Tools\n",
    "\n",
    "Import OneHotEncoder for categorical features and StandardScaler for numerical features from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b01d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa20e6",
   "metadata": {},
   "source": [
    "## 9. One-Hot Encode Categorical Features\n",
    "\n",
    "Fit the OneHotEncoder on the training data and transform train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fb57c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded categorical features shape (train): (475195, 55)\n",
      "Encoded categorical features shape (test): (118799, 55)\n",
      "Total one-hot encoded features: 55\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit OneHotEncoder on training data\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "# Transform categorical columns for both datasets\n",
    "X_train_cat_encoded = ohe.transform(X_train[categorical_cols])\n",
    "X_test_cat_encoded = ohe.transform(X_test[categorical_cols])\n",
    "\n",
    "print(f\"Encoded categorical features shape (train): {X_train_cat_encoded.shape}\")\n",
    "print(f\"Encoded categorical features shape (test): {X_test_cat_encoded.shape}\")\n",
    "print(f\"Total one-hot encoded features: {X_train_cat_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa470ee",
   "metadata": {},
   "source": [
    "## 10. Scale Numerical Features\n",
    "\n",
    "Fit the StandardScaler on the training data and transform train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501a0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled numerical features shape (train): (475195, 5)\n",
      "Scaled numerical features shape (test): (118799, 5)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit StandardScaler on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_cols])\n",
    "\n",
    "# Transform numerical columns for both datasets\n",
    "X_train_num_scaled = scaler.transform(X_train[numerical_cols])\n",
    "X_test_num_scaled = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(f\"Scaled numerical features shape (train): {X_train_num_scaled.shape}\")\n",
    "print(f\"Scaled numerical features shape (test): {X_test_num_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2de62c",
   "metadata": {},
   "source": [
    "## 11. Combine Encoded and Scaled Features\n",
    "\n",
    "Concatenate the one-hot encoded categorical features with scaled numerical features into final NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebc32bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final preprocessed data shapes:\n",
      "  X_train_processed: (475195, 60)\n",
      "  X_test_processed:  (118799, 60)\n",
      "  y_train_array:     (475195,)\n",
      "  y_test_array:      (118799,)\n",
      "\n",
      "Total features: 60 (5 numerical + 55 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Combine categorical and numerical features\n",
    "X_train_processed = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n",
    "X_test_processed = np.concatenate([X_test_num_scaled, X_test_cat_encoded], axis=1)\n",
    "\n",
    "# Convert target variables to NumPy arrays\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n",
    "\n",
    "print(\"Final preprocessed data shapes:\")\n",
    "print(f\"  X_train_processed: {X_train_processed.shape}\")\n",
    "print(f\"  X_test_processed:  {X_test_processed.shape}\")\n",
    "print(f\"  y_train_array:     {y_train_array.shape}\")\n",
    "print(f\"  y_test_array:      {y_test_array.shape}\")\n",
    "print(f\"\\nTotal features: {X_train_processed.shape[1]} ({len(numerical_cols)} numerical + {X_train_cat_encoded.shape[1]} categorical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cd70c",
   "metadata": {},
   "source": [
    "## 12. Save Preprocessed Data\n",
    "\n",
    "Save the preprocessed NumPy arrays to disk for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "074701c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to Data/preprocessed/:\n",
      "  - X_train.npy\n",
      "  - X_test.npy\n",
      "  - y_train.npy\n",
      "  - y_test.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for preprocessed data\n",
    "os.makedirs('Data/preprocessed', exist_ok=True)\n",
    "\n",
    "# Save preprocessed arrays\n",
    "np.save('Data/preprocessed/X_train.npy', X_train_processed)\n",
    "np.save('Data/preprocessed/X_test.npy', X_test_processed)\n",
    "np.save('Data/preprocessed/y_train.npy', y_train_array)\n",
    "np.save('Data/preprocessed/y_test.npy', y_test_array)\n",
    "\n",
    "print(\"Preprocessed data saved to Data/preprocessed/:\")\n",
    "print(\"  - X_train.npy\")\n",
    "print(\"  - X_test.npy\")\n",
    "print(\"  - y_train.npy\")\n",
    "print(\"  - y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bf160",
   "metadata": {},
   "source": [
    "## 13. Train Models (RandomForest and/or Neural Network)\n",
    "\n",
    "Train selected models with balanced class weights to handle class imbalance. Compare performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Negative=95600, Positive=379595\n",
      "Class weight ratio (neg/pos) = 0.2518\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS\n",
      "======================================================================\n",
      "\n",
      "ðŸŒ² RANDOM FOREST\n",
      "----------------------------------------------------------------------\n",
      "Training RandomForest model...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "n_neg = (y_train_array == 0).sum()\n",
    "n_pos = (y_train_array == 1).sum()\n",
    "class_weight_ratio = n_neg / n_pos\n",
    "\n",
    "print(f\"Class counts: Negative={n_neg}, Positive={n_pos}\")\n",
    "print(f\"Class weight ratio (neg/pos) = {class_weight_ratio:.4f}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING MODELS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Dictionary to store all trained models and their scores\n",
    "models = {}\n",
    "model_scores = {}\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM FOREST TRAINING\n",
    "# ============================================================================\n",
    "if TRAIN_RANDOM_FOREST:\n",
    "    print(\"ðŸŒ² RANDOM FOREST\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if ENABLE_ITERATIVE_TRAINING:\n",
    "        # Iterative training: try multiple configurations\n",
    "        print(\"Iterative training mode: trying multiple configurations\\n\")\n",
    "        \n",
    "        best_rf_roc_auc = 0\n",
    "        best_rf_model = None\n",
    "        best_rf_config = None\n",
    "        \n",
    "        # Define configurations to try\n",
    "        rf_configs = [\n",
    "            {\"n_estimators\": 100, \"max_depth\": 15, \"min_samples_split\": 10, \"min_samples_leaf\": 4},\n",
    "            {\"n_estimators\": 150, \"max_depth\": 20, \"min_samples_split\": 8, \"min_samples_leaf\": 3},\n",
    "            {\"n_estimators\": 200, \"max_depth\": 25, \"min_samples_split\": 5, \"min_samples_leaf\": 2},\n",
    "            {\"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 15, \"min_samples_leaf\": 5},\n",
    "            {\"n_estimators\": 250, \"max_depth\": 30, \"min_samples_split\": 4, \"min_samples_leaf\": 2},\n",
    "        ]\n",
    "        \n",
    "        for i, config in enumerate(rf_configs[:N_ITERATIONS], 1):\n",
    "            print(f\"  Config {i}/{N_ITERATIONS}: {config}\")\n",
    "            \n",
    "            temp_rf = RandomForestClassifier(\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                **config\n",
    "            )\n",
    "            \n",
    "            temp_rf.fit(X_train_processed, y_train_array)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            temp_pred_proba = temp_rf.predict_proba(X_test_processed)[:, 1]\n",
    "            temp_roc_auc = roc_auc_score(y_test_array, temp_pred_proba)\n",
    "            \n",
    "            print(f\"    â†’ ROC-AUC: {temp_roc_auc:.4f}\")\n",
    "            \n",
    "            if temp_roc_auc > best_rf_roc_auc:\n",
    "                best_rf_roc_auc = temp_roc_auc\n",
    "                best_rf_model = temp_rf\n",
    "                best_rf_config = config\n",
    "                print(f\"    âœ“ New best!\")\n",
    "            print()\n",
    "        \n",
    "        models['RandomForest'] = best_rf_model\n",
    "        model_scores['RandomForest'] = best_rf_roc_auc\n",
    "        print(f\"Best RandomForest: ROC-AUC = {best_rf_roc_auc:.4f}\")\n",
    "        print(f\"Config: {best_rf_config}\\n\")\n",
    "    else:\n",
    "        # Single training with default configuration\n",
    "        rf_model = RandomForestClassifier(\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"Training RandomForest model...\")\n",
    "        rf_model.fit(X_train_processed, y_train_array)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        rf_pred_proba = rf_model.predict_proba(X_test_processed)[:, 1]\n",
    "        rf_roc_auc = roc_auc_score(y_test_array, rf_pred_proba)\n",
    "        \n",
    "        models['RandomForest'] = rf_model\n",
    "        model_scores['RandomForest'] = rf_roc_auc\n",
    "        print(f\"âœ“ Training complete!\")\n",
    "        print(f\"RandomForest ROC-AUC: {rf_roc_auc:.4f}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK TRAINING\n",
    "# ============================================================================\n",
    "if TRAIN_NEURAL_NETWORK:\n",
    "    print(\"ðŸ§  NEURAL NETWORK\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        tf.random.set_seed(42)\n",
    "        \n",
    "        # Calculate class weights for neural network\n",
    "        class_weight_dict = {\n",
    "            0: len(y_train_array) / (2 * n_neg),\n",
    "            1: len(y_train_array) / (2 * n_pos)\n",
    "        }\n",
    "        \n",
    "        print(f\"Building neural network architecture...\")\n",
    "        print(f\"Input features: {X_train_processed.shape[1]}\")\n",
    "        \n",
    "        # Build neural network\n",
    "        nn_model = keras.Sequential([\n",
    "            layers.Input(shape=(X_train_processed.shape[1],)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        nn_model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                keras.metrics.AUC(name='auc'),\n",
    "                keras.metrics.Precision(name='precision'),\n",
    "                keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel architecture:\")\n",
    "        nn_model.summary()\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining neural network:\")\n",
    "        print(f\"  Epochs: {NN_EPOCHS}\")\n",
    "        print(f\"  Batch size: {NN_BATCH_SIZE}\")\n",
    "        print(f\"  Validation split: {NN_VALIDATION_SPLIT}\")\n",
    "        print(f\"  Class weights: {class_weight_dict}\")\n",
    "        print()\n",
    "        \n",
    "        # Train model\n",
    "        history = nn_model.fit(\n",
    "            X_train_processed, y_train_array,\n",
    "            epochs=NN_EPOCHS,\n",
    "            batch_size=NN_BATCH_SIZE,\n",
    "            validation_split=NN_VALIDATION_SPLIT,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        nn_pred_proba = nn_model.predict(X_test_processed, verbose=0).flatten()\n",
    "        nn_roc_auc = roc_auc_score(y_test_array, nn_pred_proba)\n",
    "        \n",
    "        models['NeuralNetwork'] = nn_model\n",
    "        model_scores['NeuralNetwork'] = nn_roc_auc\n",
    "        \n",
    "        print(f\"\\nâœ“ Neural Network training complete!\")\n",
    "        print(f\"Neural Network ROC-AUC: {nn_roc_auc:.4f}\")\n",
    "        print(f\"Best epoch: {len(history.history['loss']) - early_stopping.patience if early_stopping.stopped_epoch > 0 else len(history.history['loss'])}\\n\")\n",
    "        \n",
    "        # Store training history for later visualization\n",
    "        nn_training_history = history.history\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  TensorFlow/Keras not installed. Skipping Neural Network training.\")\n",
    "        print(\"Install with: pip install tensorflow\")\n",
    "        TRAIN_NEURAL_NETWORK = False\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if model_scores:\n",
    "    for model_name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{model_name:20s}: ROC-AUC = {score:.4f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(model_scores, key=model_scores.get)\n",
    "    best_model = models[best_model_name]\n",
    "    best_score = model_scores[best_model_name]\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST MODEL: {best_model_name} (ROC-AUC = {best_score:.4f})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set the main 'model' variable to the best model for later use\n",
    "    model = best_model\n",
    "    model_type = best_model_name\n",
    "else:\n",
    "    print(\"No models were trained!\")\n",
    "    raise RuntimeError(\"Please enable at least one model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_NEURAL_NETWORK and 'nn_training_history' in locals():\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0, 0].plot(nn_training_history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(nn_training_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Model Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: AUC\n",
    "    axes[0, 1].plot(nn_training_history['auc'], label='Training AUC', linewidth=2)\n",
    "    axes[0, 1].plot(nn_training_history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('AUC')\n",
    "    axes[0, 1].set_title('Model AUC over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Accuracy\n",
    "    axes[1, 0].plot(nn_training_history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[1, 0].plot(nn_training_history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('Model Accuracy over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Precision & Recall\n",
    "    axes[1, 1].plot(nn_training_history['precision'], label='Training Precision', linewidth=2)\n",
    "    axes[1, 1].plot(nn_training_history['val_precision'], label='Validation Precision', linewidth=2, linestyle='--')\n",
    "    axes[1, 1].plot(nn_training_history['recall'], label='Training Recall', linewidth=2)\n",
    "    axes[1, 1].plot(nn_training_history['val_recall'], label='Validation Recall', linewidth=2, linestyle='--')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Precision & Recall over Epochs')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('submissions/nn_training_history.png', dpi=100, bbox_inches='tight')\n",
    "    print(\"âœ“ Training history plot saved to: submissions/nn_training_history.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best epoch info\n",
    "    best_epoch = np.argmax(nn_training_history['val_auc']) + 1\n",
    "    best_val_auc = max(nn_training_history['val_auc'])\n",
    "    print(f\"\\nBest validation AUC: {best_val_auc:.4f} at epoch {best_epoch}\")\n",
    "else:\n",
    "    print(\"Neural Network not trained or history not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3afe48",
   "metadata": {},
   "source": [
    "## 13b. Visualize Neural Network Training (if applicable)\n",
    "\n",
    "Plot training history to see how the neural network learned over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bcd09",
   "metadata": {},
   "source": [
    "## 14. Evaluate Model on Test Split\n",
    "\n",
    "Evaluate the trained model using multiple metrics: ROC-AUC, F1, precision, recall, and accuracy on the provided test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60705fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test split using the best model\n",
    "if model_type == 'NeuralNetwork':\n",
    "    y_test_pred_proba = model.predict(X_test_processed, verbose=0).flatten()\n",
    "    y_test_pred = (y_test_pred_proba > 0.5).astype(int)\n",
    "else:  # RandomForest\n",
    "    y_test_pred = model.predict(X_test_processed)\n",
    "    y_test_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "roc_auc = roc_auc_score(y_test_array, y_test_pred_proba)\n",
    "f1 = f1_score(y_test_array, y_test_pred)\n",
    "precision = precision_score(y_test_array, y_test_pred)\n",
    "recall = recall_score(y_test_array, y_test_pred)\n",
    "accuracy = accuracy_score(y_test_array, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"=\"*70)\n",
    "print(f\"TEST SPLIT EVALUATION - {model_type}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ROC-AUC Score:  {roc_auc:.4f}\")\n",
    "print(f\"F1 Score:       {f1:.4f}\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test_array, y_test_pred, target_names=['Not Paid Back', 'Paid Back']))\n",
    "\n",
    "# If we trained both models, show comparison\n",
    "if len(model_scores) > 1:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL MODELS COMPARISON ON TEST SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    for name, val_score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        # Evaluate each model on test split\n",
    "        if name == 'NeuralNetwork':\n",
    "            temp_pred_proba = models[name].predict(X_test_processed, verbose=0).flatten()\n",
    "        else:\n",
    "            temp_pred_proba = models[name].predict_proba(X_test_processed)[:, 1]\n",
    "        temp_roc = roc_auc_score(y_test_array, temp_pred_proba)\n",
    "        print(f\"{name:20s}: ROC-AUC = {temp_roc:.4f} {'ðŸ†' if name == model_type else ''}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d682a00",
   "metadata": {},
   "source": [
    "## 15. Generate Test Predictions\n",
    "\n",
    "Apply the trained model to the preprocessed test set to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real test data (no labels - for final submission)\n",
    "real_test_df = pd.read_csv('Data/test.csv')\n",
    "\n",
    "print(f\"Real test data shape: {real_test_df.shape}\")\n",
    "print(f\"Real test columns: {list(real_test_df.columns)}\")\n",
    "print(f\"\\nExpected submission rows: {len(real_test_df)}\")\n",
    "print(f\"First few rows:\")\n",
    "print(real_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8106c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess real test data using the same transformers\n",
    "# (Already fitted on training data)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "real_test_cat_encoded = ohe.transform(real_test_df[categorical_cols])\n",
    "\n",
    "# Scale numerical features\n",
    "real_test_num_scaled = scaler.transform(real_test_df[numerical_cols])\n",
    "\n",
    "# Combine features\n",
    "real_test_processed = np.concatenate([real_test_num_scaled, real_test_cat_encoded], axis=1)\n",
    "\n",
    "print(f\"Real test preprocessed shape: {real_test_processed.shape}\")\n",
    "print(f\"Features: {real_test_processed.shape[1]} ({len(numerical_cols)} numerical + {real_test_cat_encoded.shape[1]} categorical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on real test data using the best model\n",
    "if model_type == 'NeuralNetwork':\n",
    "    real_test_pred_proba = model.predict(real_test_processed, verbose=0).flatten()\n",
    "    real_test_predictions = (real_test_pred_proba > 0.5).astype(int)\n",
    "else:  # RandomForest\n",
    "    real_test_predictions = model.predict(real_test_processed)\n",
    "\n",
    "print(f\"Generating predictions with: {model_type}\")\n",
    "print(f\"Real test predictions shape: {real_test_predictions.shape}\")\n",
    "print(f\"Unique predictions: {np.unique(real_test_predictions)}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(f\"  Class 0 (Not Paid): {(real_test_predictions == 0).sum()}\")\n",
    "print(f\"  Class 1 (Paid): {(real_test_predictions == 1).sum()}\")\n",
    "\n",
    "# Verify we have exactly 254,569 predictions\n",
    "assert len(real_test_predictions) == 254569, f\"Expected 254569 predictions, got {len(real_test_predictions)}\"\n",
    "print(f\"\\nâœ“ Correct number of predictions: {len(real_test_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee269b2",
   "metadata": {},
   "source": [
    "## 17. Generate Final Submission Predictions\n",
    "\n",
    "Predict on the real test data (254,569 rows) to create the final submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1288e3d",
   "metadata": {},
   "source": [
    "## 16. Preprocess Real Test Data\n",
    "\n",
    "Apply the same preprocessing (one-hot encoding and scaling) to the real test data using the fitted transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599b5cb",
   "metadata": {},
   "source": [
    "## 18. Save Validation Results and Final Submission\n",
    "\n",
    "Create submissions folder, save validation results (test_split evaluation) and final submission (real test.csv predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Create submissions directory\n",
    "os.makedirs('submissions', exist_ok=True)\n",
    "\n",
    "# Find the next submission number\n",
    "existing_submissions = glob.glob('submissions/submission_*.csv')\n",
    "if existing_submissions:\n",
    "    # Extract numbers from filenames\n",
    "    numbers = []\n",
    "    for f in existing_submissions:\n",
    "        try:\n",
    "            num = int(f.split('_')[-1].replace('.csv', ''))\n",
    "            numbers.append(num)\n",
    "        except:\n",
    "            pass\n",
    "    next_num = max(numbers) + 1 if numbers else 1\n",
    "else:\n",
    "    next_num = 1\n",
    "\n",
    "# Generate submission filenames\n",
    "submission_filename = f'submission_{next_num:03d}.csv'\n",
    "submission_path = os.path.join('submissions', submission_filename)\n",
    "validation_filename = f'validation_{next_num:03d}.csv'\n",
    "validation_path = os.path.join('submissions', validation_filename)\n",
    "metrics_filename = f'metrics_{next_num:03d}.csv'\n",
    "metrics_path = os.path.join('submissions', metrics_filename)\n",
    "\n",
    "# Save FINAL SUBMISSION (real test.csv predictions - 254,569 rows)\n",
    "final_submission = pd.DataFrame({\n",
    "    \"id\": real_test_df[\"id\"].astype(int),\n",
    "    \"loan_paid_back\": real_test_predictions.astype(int)\n",
    "})\n",
    "final_submission.to_csv(submission_path, index=False)\n",
    "print(f\"âœ“ FINAL SUBMISSION saved to: {submission_path}\")\n",
    "print(f\"  Rows: {len(final_submission)} (should be 254,569)\")\n",
    "\n",
    "# Save VALIDATION RESULTS (test_split evaluation with labels)\n",
    "validation_results = pd.DataFrame({\n",
    "    \"id\": X_test[\"id\"].astype(int) if \"id\" in X_test.columns else np.arange(len(y_test_array)),\n",
    "    \"y_true\": y_test_array.astype(int),\n",
    "    \"y_pred\": y_test_pred.astype(int),\n",
    "    \"y_proba\": y_test_pred_proba.astype(float)\n",
    "})\n",
    "validation_results.to_csv(validation_path, index=False)\n",
    "print(f\"âœ“ Validation results saved to: {validation_path}\")\n",
    "\n",
    "# Save aggregate metrics with configuration info\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"submission_num\": next_num,\n",
    "        \"model_type\": model_type,\n",
    "        \"use_smote\": USE_SMOTE,\n",
    "        \"iterative_training\": ENABLE_ITERATIVE_TRAINING,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_samples\": len(y_train_array),\n",
    "        \"validation_samples\": len(y_test_array),\n",
    "        \"submission_samples\": len(final_submission)\n",
    "    }\n",
    "])\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"âœ“ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Also save final submission to root for easy access\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "print(f\"âœ“ Copy saved to: submission.csv\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SUBMISSION #{next_num}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model: {model_type}\")\n",
    "print(f\"Training: {'SMOTE-balanced' if USE_SMOTE else 'Original'} ({len(y_train_array)} samples)\")\n",
    "print(f\"Validation (test_split): {len(y_test_array)} samples\")\n",
    "print(f\"  â†’ ROC-AUC: {roc_auc:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Final Submission (test.csv): {len(final_submission)} predictions\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nFirst few submission rows:\")\n",
    "print(final_submission.head(10))\n",
    "print(\"\\nLast few submission rows:\")\n",
    "print(final_submission.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39632400",
   "metadata": {},
   "source": [
    "## 19. Save Model and Preprocessors\n",
    "\n",
    "Save the trained model and preprocessing objects (scaler and encoder) for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the best model (based on model type)\n",
    "if model_type == 'NeuralNetwork':\n",
    "    model_path = 'models/loan_model_nn.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"âœ“ Neural Network model saved to: {model_path}\")\n",
    "else:  # RandomForest\n",
    "    model_path = 'models/loan_model_rf.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"âœ“ RandomForest model saved to: {model_path}\")\n",
    "\n",
    "# Save ALL trained models\n",
    "if TRAIN_RANDOM_FOREST and 'RandomForest' in models:\n",
    "    rf_path = 'models/loan_model_rf.pkl'\n",
    "    joblib.dump(models['RandomForest'], rf_path)\n",
    "    print(f\"âœ“ RandomForest saved to: {rf_path}\")\n",
    "\n",
    "if TRAIN_NEURAL_NETWORK and 'NeuralNetwork' in models:\n",
    "    nn_path = 'models/loan_model_nn.keras'\n",
    "    models['NeuralNetwork'].save(nn_path)\n",
    "    print(f\"âœ“ Neural Network saved to: {nn_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = 'models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ“ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save the encoder\n",
    "encoder_path = 'models/encoder.pkl'\n",
    "joblib.dump(ohe, encoder_path)\n",
    "print(f\"âœ“ Encoder saved to: {encoder_path}\")\n",
    "\n",
    "# Save model comparison results\n",
    "if len(model_scores) > 1:\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\"model\": name, \"roc_auc\": score} \n",
    "        for name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ])\n",
    "    comparison_path = 'models/model_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"âœ“ Model comparison saved to: {comparison_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Model: {model_type}\")\n",
    "print(f\"Configuration: USE_SMOTE = {USE_SMOTE}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Submission saved in: submissions/\")\n",
    "if len(model_scores) > 1:\n",
    "    print(f\"\\nModels trained: {', '.join(model_scores.keys())}\")\n",
    "    print(f\"Winner: {model_type}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Find all metrics files\n",
    "metrics_files = sorted(glob.glob('submissions/metrics_*.csv'))\n",
    "\n",
    "if metrics_files:\n",
    "    # Load and combine all metrics\n",
    "    all_metrics = []\n",
    "    for f in metrics_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            all_metrics.append(df)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if all_metrics:\n",
    "        history_df = pd.concat(all_metrics, ignore_index=True)\n",
    "        history_df = history_df.sort_values('submission_num')\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"SUBMISSION HISTORY\")\n",
    "        print(\"=\"*80)\n",
    "        print(history_df.to_string(index=False))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Find best submission by ROC-AUC\n",
    "        best_idx = history_df['roc_auc'].idxmax()\n",
    "        best_sub = history_df.loc[best_idx]\n",
    "        print(f\"\\nðŸ† BEST SUBMISSION: #{int(best_sub['submission_num'])} with ROC-AUC = {best_sub['roc_auc']:.4f}\")\n",
    "        print(f\"   Configuration: USE_SMOTE = {bool(best_sub['use_smote'])}\")\n",
    "    else:\n",
    "        print(\"No valid metrics files found.\")\n",
    "else:\n",
    "    print(\"No submission history yet. This is your first submission!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3fcb7",
   "metadata": {},
   "source": [
    "## 20. View Submission History\n",
    "\n",
    "Display all previous submissions and their metrics for comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
