{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57838e7",
   "metadata": {},
   "source": [
    "# Load and Split Loan Data\n",
    "\n",
    "This notebook loads the training and test datasets, then performs a stratified train-validation split on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c090a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and train_test_split from sklearn.model_selection for splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674b866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9bb72",
   "metadata": {},
   "source": [
    "## 2. Load Training and Test Data\n",
    "\n",
    "Load the train.csv and test.csv files from the Data directory using pandas read_csv() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f853be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (593994, 13)\n",
      "Test data shape: (254569, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test datasets\n",
    "train_df = pd.read_csv('Data/train.csv')\n",
    "test_df = pd.read_csv('Data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4206f781",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target Variable\n",
    "\n",
    "Separate the features (X) from the target variable (y) by dropping the 'loan_paid_back' column from the training dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8126624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (593994, 12)\n",
      "Target shape: (593994,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "X = train_df.drop(\"loan_paid_back\", axis=1)\n",
    "y = train_df[\"loan_paid_back\"]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49941a4d",
   "metadata": {},
   "source": [
    "## 4. Perform Stratified Train-Validation Split\n",
    "\n",
    "Use train_test_split with test_size=0.2, stratify=y, and random_state=42 to split the data into 80% training and 20% validation sets while maintaining class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99b15e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (475195, 12)\n",
      "X_val shape: (118799, 12)\n",
      "y_train shape: (475195,)\n",
      "y_val shape: (118799,)\n",
      "\n",
      "Class distribution in y_train:\n",
      "loan_paid_back\n",
      "0.0    0.201181\n",
      "1.0    0.798819\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in y_val:\n",
      "loan_paid_back\n",
      "0.0    0.20118\n",
      "1.0    0.79882\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"\\nClass distribution in y_train:\\n{y_train.value_counts(normalize=True).sort_index()}\")\n",
    "print(f\"\\nClass distribution in y_val:\\n{y_val.value_counts(normalize=True).sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a543136",
   "metadata": {},
   "source": [
    "## 5. Prepare Test Features\n",
    "\n",
    "Extract the features from the test dataset (test.csv doesn't have the target column 'loan_paid_back')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d4bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (254569, 12)\n",
      "Test data columns: ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n"
     ]
    }
   ],
   "source": [
    "# Test data doesn't have the target column, so use it as-is for features\n",
    "X_test = test_df.copy()\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Test data columns: {list(X_test.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1157223",
   "metadata": {},
   "source": [
    "## 6. Identify Categorical and Numerical Columns\n",
    "\n",
    "Separate columns into categorical and numerical types for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b5e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns (6): ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "Numerical columns (5): ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns (object dtype)\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Identify numerical columns (excluding 'id' if present, as it's not a useful feature)\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'id' in numerical_cols:\n",
    "    numerical_cols.remove('id')\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365256c1",
   "metadata": {},
   "source": [
    "## 7. Import Preprocessing Tools\n",
    "\n",
    "Import OneHotEncoder for categorical features and StandardScaler for numerical features from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b01d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa20e6",
   "metadata": {},
   "source": [
    "## 8. One-Hot Encode Categorical Features\n",
    "\n",
    "Fit the OneHotEncoder on the training data and transform train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb57c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded categorical features shape (train): (475195, 55)\n",
      "Encoded categorical features shape (val): (118799, 55)\n",
      "Encoded categorical features shape (test): (254569, 55)\n",
      "Total one-hot encoded features: 55\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit OneHotEncoder on training data\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "# Transform categorical columns for all datasets\n",
    "X_train_cat_encoded = ohe.transform(X_train[categorical_cols])\n",
    "X_val_cat_encoded = ohe.transform(X_val[categorical_cols])\n",
    "X_test_cat_encoded = ohe.transform(X_test[categorical_cols])\n",
    "\n",
    "print(f\"Encoded categorical features shape (train): {X_train_cat_encoded.shape}\")\n",
    "print(f\"Encoded categorical features shape (val): {X_val_cat_encoded.shape}\")\n",
    "print(f\"Encoded categorical features shape (test): {X_test_cat_encoded.shape}\")\n",
    "print(f\"Total one-hot encoded features: {X_train_cat_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa470ee",
   "metadata": {},
   "source": [
    "## 9. Scale Numerical Features\n",
    "\n",
    "Fit the StandardScaler on the training data and transform train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501a0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled numerical features shape (train): (475195, 5)\n",
      "Scaled numerical features shape (val): (118799, 5)\n",
      "Scaled numerical features shape (test): (254569, 5)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit StandardScaler on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_cols])\n",
    "\n",
    "# Transform numerical columns for all datasets\n",
    "X_train_num_scaled = scaler.transform(X_train[numerical_cols])\n",
    "X_val_num_scaled = scaler.transform(X_val[numerical_cols])\n",
    "X_test_num_scaled = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(f\"Scaled numerical features shape (train): {X_train_num_scaled.shape}\")\n",
    "print(f\"Scaled numerical features shape (val): {X_val_num_scaled.shape}\")\n",
    "print(f\"Scaled numerical features shape (test): {X_test_num_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2de62c",
   "metadata": {},
   "source": [
    "## 10. Combine Encoded and Scaled Features\n",
    "\n",
    "Concatenate the one-hot encoded categorical features with scaled numerical features into final NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc32bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final preprocessed data shapes:\n",
      "  X_train_processed: (475195, 60)\n",
      "  X_val_processed: (118799, 60)\n",
      "  X_test_processed: (254569, 60)\n",
      "  y_train_array: (475195,)\n",
      "  y_val_array: (118799,)\n",
      "\n",
      "Total features: 60 (5 numerical + 55 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Combine categorical and numerical features\n",
    "X_train_processed = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n",
    "X_val_processed = np.concatenate([X_val_num_scaled, X_val_cat_encoded], axis=1)\n",
    "X_test_processed = np.concatenate([X_test_num_scaled, X_test_cat_encoded], axis=1)\n",
    "\n",
    "# Convert target variables to NumPy arrays\n",
    "y_train_array = y_train.values\n",
    "y_val_array = y_val.values\n",
    "\n",
    "print(\"Final preprocessed data shapes:\")\n",
    "print(f\"  X_train_processed: {X_train_processed.shape}\")\n",
    "print(f\"  X_val_processed: {X_val_processed.shape}\")\n",
    "print(f\"  X_test_processed: {X_test_processed.shape}\")\n",
    "print(f\"  y_train_array: {y_train_array.shape}\")\n",
    "print(f\"  y_val_array: {y_val_array.shape}\")\n",
    "print(f\"\\nTotal features: {X_train_processed.shape[1]} ({len(numerical_cols)} numerical + {X_train_cat_encoded.shape[1]} categorical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cd70c",
   "metadata": {},
   "source": [
    "## 11. Save Preprocessed Data\n",
    "\n",
    "Save the preprocessed NumPy arrays to disk for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074701c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to Data/preprocessed/:\n",
      "  - X_train.npy\n",
      "  - X_val.npy\n",
      "  - X_test.npy\n",
      "  - y_train.npy\n",
      "  - y_val.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for preprocessed data\n",
    "os.makedirs('Data/preprocessed', exist_ok=True)\n",
    "\n",
    "# Save preprocessed arrays\n",
    "np.save('Data/preprocessed/X_train.npy', X_train_processed)\n",
    "np.save('Data/preprocessed/X_val.npy', X_val_processed)\n",
    "np.save('Data/preprocessed/X_test.npy', X_test_processed)\n",
    "np.save('Data/preprocessed/y_train.npy', y_train_array)\n",
    "np.save('Data/preprocessed/y_val.npy', y_val_array)\n",
    "\n",
    "print(\"Preprocessed data saved to Data/preprocessed/:\")\n",
    "print(\"  - X_train.npy\")\n",
    "print(\"  - X_val.npy\")\n",
    "print(\"  - X_test.npy\")\n",
    "print(\"  - y_train.npy\")\n",
    "print(\"  - y_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bf160",
   "metadata": {},
   "source": [
    "## 12. Train RandomForest Model\n",
    "\n",
    "Train a RandomForest classifier with balanced class weights to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7af801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Negative=95600, Positive=379595\n",
      "Class weight ratio (neg/pos) = 0.2518\n",
      "\n",
      "Training RandomForest model...\n",
      "✓ Model training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "n_neg = (y_train_array == 0).sum()\n",
    "n_pos = (y_train_array == 1).sum()\n",
    "class_weight_ratio = n_neg / n_pos\n",
    "\n",
    "print(f\"Class counts: Negative={n_neg}, Positive={n_pos}\")\n",
    "print(f\"Class weight ratio (neg/pos) = {class_weight_ratio:.4f}\")\n",
    "\n",
    "# Initialize RandomForest classifier with balanced class weights\n",
    "model = RandomForestClassifier(\n",
    "    class_weight='balanced',  # Automatically adjusts weights inversely proportional to class frequencies\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining RandomForest model...\")\n",
    "model.fit(X_train_processed, y_train_array)\n",
    "print(\"✓ Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bcd09",
   "metadata": {},
   "source": [
    "## 13. Evaluate Model on Validation Set\n",
    "\n",
    "Evaluate the trained model using multiple metrics: ROC-AUC, F1, precision, recall, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60705fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION SET METRICS\n",
      "============================================================\n",
      "ROC-AUC Score:  0.9121\n",
      "F1 Score:       0.9195\n",
      "Precision:      0.9356\n",
      "Recall:         0.9040\n",
      "Accuracy:       0.8736\n",
      "============================================================\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Paid Back       0.66      0.75      0.71     23900\n",
      "    Paid Back       0.94      0.90      0.92     94899\n",
      "\n",
      "     accuracy                           0.87    118799\n",
      "    macro avg       0.80      0.83      0.81    118799\n",
      " weighted avg       0.88      0.87      0.88    118799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on validation set\n",
    "y_val_pred = model.predict(X_val_processed)\n",
    "y_val_pred_proba = model.predict_proba(X_val_processed)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "roc_auc = roc_auc_score(y_val_array, y_val_pred_proba)\n",
    "f1 = f1_score(y_val_array, y_val_pred)\n",
    "precision = precision_score(y_val_array, y_val_pred)\n",
    "recall = recall_score(y_val_array, y_val_pred)\n",
    "accuracy = accuracy_score(y_val_array, y_val_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION SET METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ROC-AUC Score:  {roc_auc:.4f}\")\n",
    "print(f\"F1 Score:       {f1:.4f}\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_val_array, y_val_pred, target_names=['Not Paid Back', 'Paid Back']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d682a00",
   "metadata": {},
   "source": [
    "## 14. Generate Test Predictions\n",
    "\n",
    "Apply the trained model to the preprocessed test set to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ca0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: (254569,)\n",
      "Unique predictions: [0. 1.]\n",
      "Prediction distribution:\n",
      "  Class 0 (Not Paid): 57453\n",
      "  Class 1 (Paid): 197116\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on test set\n",
    "test_predictions = model.predict(X_test_processed)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Unique predictions: {np.unique(test_predictions)}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(f\"  Class 0 (Not Paid): {(test_predictions == 0).sum()}\")\n",
    "print(f\"  Class 1 (Paid): {(test_predictions == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599b5cb",
   "metadata": {},
   "source": [
    "## 15. Create Submission File\n",
    "\n",
    "Create a submission DataFrame matching the format of sample_submission.csv with id and loan_paid_back columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5286c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample submission columns: ['id', 'loan_paid_back']\n",
      "Our submission columns: ['id', 'loan_paid_back']\n",
      "\n",
      "Sample submission shape: (254569, 2)\n",
      "Our submission shape: (254569, 2)\n",
      "\n",
      "First few rows of submission:\n",
      "       id  loan_paid_back\n",
      "0  593994               1\n",
      "1  593995               1\n",
      "2  593996               0\n",
      "3  593997               1\n",
      "4  593998               1\n",
      "5  593999               1\n",
      "6  594000               1\n",
      "7  594001               1\n",
      "8  594002               1\n",
      "9  594003               0\n",
      "\n",
      "Last few rows of submission:\n",
      "            id  loan_paid_back\n",
      "254559  848553               1\n",
      "254560  848554               1\n",
      "254561  848555               1\n",
      "254562  848556               0\n",
      "254563  848557               1\n",
      "254564  848558               1\n",
      "254565  848559               1\n",
      "254566  848560               1\n",
      "254567  848561               1\n",
      "254568  848562               1\n"
     ]
    }
   ],
   "source": [
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"loan_paid_back\": test_predictions\n",
    "})\n",
    "\n",
    "# Verify format matches sample_submission.csv\n",
    "sample_sub = pd.read_csv('Data/sample_submission.csv')\n",
    "print(\"Sample submission columns:\", list(sample_sub.columns))\n",
    "print(\"Our submission columns:\", list(submission.columns))\n",
    "print(f\"\\nSample submission shape: {sample_sub.shape}\")\n",
    "print(f\"Our submission shape: {submission.shape}\")\n",
    "\n",
    "# Ensure data types match\n",
    "submission['id'] = submission['id'].astype(int)\n",
    "submission['loan_paid_back'] = submission['loan_paid_back'].astype(int)\n",
    "\n",
    "print(\"\\nFirst few rows of submission:\")\n",
    "print(submission.head(10))\n",
    "print(\"\\nLast few rows of submission:\")\n",
    "print(submission.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39632400",
   "metadata": {},
   "source": [
    "## 16. Save Model and Submission\n",
    "\n",
    "Save the trained model, preprocessing objects (scaler and encoder), and the submission CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802cdc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to: models/loan_model.pkl\n",
      "✓ Scaler saved to: models/scaler.pkl\n",
      "✓ Encoder saved to: models/encoder.pkl\n",
      "✓ Submission file saved to: submission.csv\n",
      "\n",
      "============================================================\n",
      "✅ Model trained and submission file generated successfully\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'models/loan_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = 'models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save the encoder\n",
    "encoder_path = 'models/encoder.pkl'\n",
    "joblib.dump(ohe, encoder_path)\n",
    "print(f\"✓ Encoder saved to: {encoder_path}\")\n",
    "\n",
    "# Save submission file\n",
    "submission_path = 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"✓ Submission file saved to: {submission_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Model trained and submission file generated successfully\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
